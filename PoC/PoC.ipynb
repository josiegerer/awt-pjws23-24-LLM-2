{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Llama2 Model weights\n",
    "First we need to load the Model weights of the Llama2 model. In order to access the models you might have to be registered at HuggingFace-Hub as requirement.\n",
    "Therefore, we can install the GGUF model weights from the HuggingFace-Hub from the user <a href=https://huggingface.co/TheBloke/Llama-2-7B-GGUF>TheBloke Llama 2 7B</a> or <a href=https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF> TheBloke Llama2 7B Chat Version </a>. The following model weights are considered and can be changed throughout the PoC code:\n",
    "- llama-2-7b-chat.Q4_K_M.gguf (MAX RAM required: 6.58 GB; SPACE required: 3.9 GB)\n",
    "- llama-2-7b-chat.Q5_K_S.gguf (MAX RAM required: 7.15 GB; SPACE required: 4.5 GB)\n",
    "- llama-2-7b-chat.Q5_K_M.gguf (MAX RAM required: 7.28 GB; SPACE required: 4.9 GB)\n",
    "\n",
    "We are using llama-2-7b-chat.Q4_K_M.gguf for our use case.\n",
    "\n",
    "## Alternative download:\n",
    "- create folder and `cd` into it\n",
    "- run: `wget https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf` or `wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf?download` for the chat version\n",
    "\n",
    "This might take a while.\n",
    "Sidenote: 13B models would be nicer but the cheapest recommended model requires 11.47 GB, which is quite much given our hardware capabilities\n",
    "\n",
    "# Get the requirements for Llama2 usage\n",
    "\n",
    "Once the models are downloaded we can now use the llama.cpp library to make use of the downloaded model weights and use it together with LangChain. Therefore we have to first install the libraries by running:\n",
    "\n",
    "```\n",
    "pip install langchain llama-cpp-python\n",
    "```\n",
    "\n",
    "Now everything should be setup and you should be able to use the quantized Llama2 model + LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem handling:\n",
    "- Using Anaconda on Windows, I constantly run into the following error message:\n",
    "```\n",
    "ERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects\n",
    "```\n",
    "\n",
    "Following the instructions from <a href=\"https://stackoverflow.com/questions/73969269/error-could-not-build-wheels-for-hnswlib-which-is-required-to-install-pyprojec\">Varada</a> solved my issues.\n",
    "1. Download and run the VSC builder\n",
    "2. Under \"Individual Components\" check the corresponding boxes and run the program further\n",
    "3. It hopefully should be fixed by now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install prerequisits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain llama-cpp-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell in a suitable directory (example: ../llama_weights)\n",
    "# !wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf?download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Initial Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "# DEFAULT_SYSTEM_PROMPT = \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"You are an english teacher for very formal language. Emphasize any mistake without correcting it.\"\n",
    "\n",
    "SYSTEM_PROMPT = B_SYS + DEFAULT_SYSTEM_PROMPT +E_SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(instruction):\n",
    "    return B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "\n",
    "# chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(\"/Users/josi/Llama2_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path = \"/Users/josi/Llama2_weights/llama-2-7b-chat.Q5_K_M.gguf\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2048,\n",
    "    top_p=1,\n",
    "    # callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = get_prompt(\"Correct the following sentence: Hau is the wether today?\")\n",
    "\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = B_INST +SYSTEM_PROMPT + \"{user_message}\" + E_INST\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.run(\"Hai tere!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out LangChain's ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, verbose=True, memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Conversation Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "[INST] <<SYS>>\n",
    "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "<</SYS>>\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI Assistant:[/INST]\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "conversation = ConversationChain(\n",
    "    prompt=PROMPT,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory(ai_prefix=\"AI Assistant\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = conversation.predict(input=\"Hi there!\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = conversation.predict(input=\"How is the weather?\")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = conversation.predict(input=\"Do you like spagghetti?\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the conversation the whole history of the conversation between the human and AI assistant can be stored with LangChain's `RedisChatMessageHistory`. Then the messages can be pulled from the DB in order to analyse the conversations from the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse_chat = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Grammar Helping Assistant\n",
    "\n",
    "** Maybe using standard LLMChain is the better approach for this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "[INST] <<SYS>>\n",
    "The following is a Grammar lesson between an AI and the human. The AI emphasizes mistakes made by the human without given the solution of the problem. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "<</SYS>>\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI Assistant:[/INST]\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "conversation = ConversationChain(\n",
    "    prompt=PROMPT,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory(ai_prefix=\"AI Assistant\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = conversation.predict(input=\"Hai tere!\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = conversation.predict(input=\"Wat was the first mistake?\")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALternative without using the conversational framework, but just the single prompt itself\n",
    "prompt_template = B_INST +SYSTEM_PROMPT + \"{user_message}\" + E_INST\n",
    "llm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "llm_chain.run(\"Hai tere!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Translator from English to German\n",
    "\n",
    "Using LLMChain is sufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "<<SYS>>\n",
    "You are a translator that translates text from German to French. Only return the translated text, nothing else.\n",
    "<</SYS>>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = B_INST +SYSTEM_PROMPT + \"{user_message}\" + E_INST\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "llm_chain.run(\"Translate the following text into french: Ich mag es Basketball zu spielen.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
