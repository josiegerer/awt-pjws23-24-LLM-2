{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain llama-cpp-python \n",
    "#CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define System Prompt and load Llama2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "# DEFAULT_SYSTEM_PROMPT = \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chatbot prompt for later use case\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "<<SYS>>\n",
    "You are a chatbot that helps people learn a new language. You always give the answer to the question in a formal manner.\n",
    "If you don't know the answer to a question, you tell them truthfully that you don't know and don't give false information. You are a helpful, respectful and honest assistant.\n",
    "Your answers should not contain harmful, unethical, racist, sexist, toxic, dangerous or illegal content. Please make sure that your answers are socially unbiased and positive.\n",
    "If a question does not make sense or is not factually coherent, please explain why, rather than answering something incorrectly.<</SYS>>\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + DEFAULT_SYSTEM_PROMPT +E_SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general prompt to test llm for testing questions\n",
    "SYSTEM_PROMPT_2 = \"\"\"\n",
    "<<SYS>>\n",
    "You are alwys giving the answer to the question in a short and very simple manner and you answer in the language style that is asked from you. You leave out unnecessary words and if you have several options you would like to present only name one. If the answer is only one word you don't put a dot and the end. If you are asked to translate something you translate it.\n",
    "If you don't know the answer to a question, you tell them truthfully that you don't know and don't give false information. You are a helpful, respectful and honest assistant.\n",
    "Your answers should not contain harmful, unethical, racist, sexist, toxic, dangerous or illegal content. Please make sure that your answers are socially unbiased and positive.\n",
    "If a question does not make sense or is not factually coherent, please explain why, rather than answering something incorrectly.<</SYS>>\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + SYSTEM_PROMPT_2 +E_SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(instruction):\n",
    "    return B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "\n",
    "os.listdir(\"/Users/josi/Llama2_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LlamaCpp(\n",
    "    #model_path=\"/Users/josi/Llama2_weights/llama-2-7b.Q4_K_M.gguf\",\n",
    "    model_path = \"/Users/josi/Llama2_weights/llama-2-7b-chat.Q5_K_M.gguf\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2048,\n",
    "    top_p=1,\n",
    "    # callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionaries with test questions (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test questions for different categories\n",
    "informal_questions = {\n",
    "    \"Write an informal greeting.\": \"Hey\",\n",
    "    \"Write an informal response to the question: 'How's it going?'\": \"Hi, going good, hbu?\",\n",
    "    \"Write an informal message inviting someone to dinner tonight\": \"Hey, wanna join us for dinner tonight?\",\n",
    "    \"Compose an informal email closing.\": \"Cheers, [Your Name]\",\n",
    "}\n",
    "\n",
    "formal_questions = {\n",
    "    \"Write a formal greeting.\": \"Hello\",\n",
    "    \"Write a formal response to the question: 'How is it going?'\": \"Hello, I am doing well. How about you?\",\n",
    "    \"Write a formal message inviting someone to dinner tonight\": \"Hello, would you like to join us for dinner tonight?\",\n",
    "    \"Compose a professional email closing.\": \"Sincerely, [Your Name]\",\n",
    "}\n",
    "\n",
    "academic_questions = {\n",
    "    \"Describe the french revolution in two sentences using academic language.\": \"The historical event of the French Revolution was marked by widespread social upheaval...\",\n",
    "    \"Define gravity in two sentences using academic language.\": \"Gravity is a force that attracts two objects with mass\",\n",
    "}\n",
    "\n",
    "translation_questions = {\n",
    "    \"Translate 'hello' to Spanish.\": \"Hola\",\n",
    "    \"Translate 'good morning' to French.\": \"Bonjour\",\n",
    "    \"Translate 'Thank you' to German.\": \"Danke\",\n",
    "    \"How do you say 'hello' in Japanese?\": \"Konnichiwa (こんにちは)\",\n",
    "    \"Provide the Italian translation for 'book'.\": \"Libro\",\n",
    "}\n",
    "\n",
    "general_knowledge_questions = {\n",
    "    \"What is the capital of France?\": \"The capital of France is Paris\",\n",
    "    \"What is the square root of 25?\": \"The square root of 25 is 5\",\n",
    "    \"What is the circumference of the earth?\":\"The circumference of the earth is approximately 40,075 kilometers\",\n",
    "    \"question\": \"Explain the concept of photosynthesis.\", \"answer\": \"Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods with the help of chlorophyll.\",\n",
    "}\n",
    "\n",
    "textual_entailment = {\n",
    "    \"question\": \"Given the statement 'The sun rises in the east,' is the statement 'The earth is flat' likely true or false?\", \"answer\": \"False\",\n",
    "    \"question\": \"If 'A implies B' and 'B is false,' what can you conclude about A?\", \"answer\": \"Nothing definitive can be concluded about A.\",\n",
    "}\n",
    "\n",
    "ambiguity_handling = {\n",
    "    \"question\": \"Provide two different interpretations of the phrase 'bank account.'\", \"answer\": \"1. An account with a financial institution. 2. The land alongside a river.\",\n",
    "    \"question\": \"What could the word 'bat' mean in the context of baseball and in the context of an animal?\", \"answer\": \"In baseball, a 'bat' is a piece of equipment used to hit the ball. In the context of an animal, a 'bat' is a flying mammal.\",\n",
    "}\n",
    "\n",
    "reasoning_problem_solving = {\n",
    "    \"question\": \"If a car travels at 60 miles per hour, how long will it take to cover 120 miles?\", \"answer\": \"2 hours\",\n",
    "    \"question\": \"Solve the following math problem: 3x + 5 = 20.\", \"answer\": \"x = 5\",\n",
    "}\n",
    "\n",
    "analogical_reasoning = {\n",
    "    \"question\": \"If 'cat' is to 'kitten,' what is 'dog' to?\", \"answer\": \"'Dog' is to 'puppy'\",\n",
    "    \"question\": \"Identify the relationship between 'pen' and 'ink' and apply the same relationship to 'keyboard.'\", \"answer\": \"'Keyboard' is to 'electricity'\",\n",
    "}\n",
    "\n",
    "commonsense_reasoning = {\n",
    "    \"question\": \"What might be a common reaction to receiving a gift?\", \"answer\": \"A common reaction to receiving a gift is expressing gratitude and appreciation.\",\n",
    "    \"question\": \"Predict a possible consequence of leaving food out in the sun for too long.\", \"answer\": \"Leaving food out in the sun for too long may lead to spoilage and bacterial growth.\",\n",
    "}\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "ner = {\n",
    "    \"question\": \"Identify the named entities in the sentence 'Elon Musk is the CEO of SpaceX.'\", \"answer\": \"Named entities: Elon Musk, SpaceX.\",\n",
    "    \"question\": \"Extract the dates mentioned in the text 'The conference is scheduled for January 25-27, 2023.'\", \"answer\": \"Dates: January 25-27, 2023.\",\n",
    "}\n",
    "\n",
    "# Combine all test questions into a single dictionary\n",
    "test_questions = {**informal_questions, **formal_questions, **academic_questions, **translation_questions, **general_knowledge_questions,**textual_entailment,**ambiguity_handling,**reasoning_problem_solving,**analogical_reasoning,**commonsense_reasoning,**ner}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model based on Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answers = []\n",
    "def evaluate_model(model, questions):\n",
    "    correct_answers = 0\n",
    "    total_questions = len(questions)\n",
    "\n",
    "    for question, expected_answer in questions.items():\n",
    "        # Get the model's answer for the current question\n",
    "        prompt = get_prompt(question)\n",
    "        model_answer = model(prompt)\n",
    "\n",
    "        # Remove trailing dots from the model's answer and Compare the model's answer to the expected answer (case-insensitive, whitespace-insensitive)\n",
    "        model_answer = model_answer.rstrip('.')\n",
    "        if model_answer.strip().lower() == expected_answer.strip().lower():\n",
    "            correct_answers += 1\n",
    "\n",
    "        # Check if any of the expected words are present in the model's answer\n",
    "        #if any(word.lower() in model_answer.lower() for word in expected_answer.split()):\n",
    "        #    correct_answers += 1\n",
    "\n",
    "        # Print the results for each question\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected Answer: {expected_answer}\")\n",
    "        print(f\"Model Answer: {model_answer}\")\n",
    "        print(\"------------\")\n",
    "\n",
    "        # Store the model's answer and the corresponding question in the array\n",
    "        model_answers.append({\n",
    "            \"question\": question,\n",
    "            \"expected_answer\": expected_answer,\n",
    "            \"model_answer\": model_answer,\n",
    "        })\n",
    "\n",
    "    # Print overall evaluation results\n",
    "    print(f\"Total Questions: {total_questions}\")\n",
    "    print(f\"Correct Answers: {correct_answers}\")\n",
    "    print(f\"Accuracy: {correct_answers / total_questions * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(llm, test_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model based on Rouge (Recall-Oriented Understudy for Gissing Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ROUGE (Recall-Oriented Understudy for Gissing Evaluation): a set of metrics used for evaluating the quality of summaries. It compares the generated summary with one or more reference summaries and calculates precision, recall, and F1-score \n",
    "\n",
    "#%pip install rouge\n",
    "from rouge import Rouge\n",
    "\n",
    "def evaluate_model(model, questions):\n",
    "    try:\n",
    "        correct_answers = 0\n",
    "        total_questions = len(questions)\n",
    "        model_answers = []\n",
    "        rouge_scores = {\"rouge-1\": {\"precision\": 0, \"recall\": 0, \"f1-score\": 0}}\n",
    "\n",
    "        for question, expected_answer in questions.items():\n",
    "            # Get the model's answer for the current question\n",
    "            prompt = get_prompt(question)\n",
    "            model_answer = model(prompt)\n",
    "\n",
    "            # format answer to match properly \n",
    "            model_answer = model_answer.rstrip('.')\n",
    "            #and calculate accuracy (which is currently defined as approximate match where only any word has to match the correct answer.)\n",
    "            if any(word.lower() in model_answer.lower() for word in expected_answer.split()):\n",
    "                correct_answers += 1\n",
    "\n",
    "            # Concatenate model's answer and expected answer into strings for Rouge evaluation\n",
    "            candidate_summary = \" \".join(model_answer.split())\n",
    "            reference_summary = \" \".join(expected_answer.split())\n",
    "\n",
    "            # Update Rouge scores\n",
    "            rouge = Rouge()\n",
    "            scores = rouge.get_scores(candidate_summary, reference_summary)\n",
    "\n",
    "            # Check if Rouge calculation was successful - WIP\n",
    "            if scores:\n",
    "                scores = scores[0]\n",
    "                for metric in [\"precision\", \"recall\", \"f1-score\"]:\n",
    "                    rouge_scores[f\"rouge-1\"][metric] += scores.get(f\"rouge-1\", {}).get(metric, 0)\n",
    "\n",
    "            # store model answers in array\n",
    "            model_answers.append({\n",
    "                \"question\": question,\n",
    "                \"expected_answer\": expected_answer,\n",
    "                \"model_answer\": model_answer,\n",
    "            })\n",
    "\n",
    "            #print each model answer with score\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Expected Answer: {expected_answer}\")\n",
    "            print(f\"Model Answer: {model_answer}\")\n",
    "            print(\"------------\")\n",
    "\n",
    "\n",
    "        # Calculate average Rouge scores\n",
    "        for metric in [\"precision\", \"recall\", \"f1-score\"]:\n",
    "            rouge_scores[f\"rouge-1\"][metric] /= total_questions\n",
    "\n",
    "        # Print overall evaluation results\n",
    "        print(f\"Total Questions: {total_questions}\")\n",
    "        print(f\"Correct Answers: {correct_answers}\")\n",
    "        print(f\"Accuracy: {correct_answers / total_questions * 100:.2f}%\")\n",
    "        print(\"Rouge Scores:\")\n",
    "        print(rouge_scores)\n",
    "\n",
    "        return model_answers, rouge_scores\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_answers, rouge_scores = evaluate_model(llm, test_questions)\n",
    "\n",
    "\n",
    "###current output: \n",
    "#Total Questions: 19\n",
    "#Correct Answers: 16\n",
    "#Accuracy: 84.21%\n",
    "#Rouge Scores:\n",
    "#{'rouge-1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Correctness Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bitsandbytes\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Performance Evaluation - Perplexity: measure that quantifies how well the model predicts a sample of text. Lower perplexity values indicate better performance \n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, LlamaTokenizer\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "model_name = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n",
    "max_memory = f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  device_map='auto',\n",
    "  load_in_4bit=True,\n",
    "  max_memory=max_memory,\n",
    "  do_sample=True,\n",
    "  torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.max_length\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "device = \"cuda\"\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
