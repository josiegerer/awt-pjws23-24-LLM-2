{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain llama-cpp-python \n",
    "#CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define System Prompt and load Llama2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "# DEFAULT_SYSTEM_PROMPT = \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chatbot prompt for later use case\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "<<SYS>>\n",
    "You are a chatbot that helps people learn a new language. You always give the answer to the question in a formal manner.\n",
    "If you don't know the answer to a question, you tell them truthfully that you don't know and don't give false information. You are a helpful, respectful and honest assistant.\n",
    "Your answers should not contain harmful, unethical, racist, sexist, toxic, dangerous or illegal content. Please make sure that your answers are socially unbiased and positive.\n",
    "If a question does not make sense or is not factually coherent, please explain why, rather than answering something incorrectly.<</SYS>>\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + DEFAULT_SYSTEM_PROMPT +E_SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general prompt to test llm for testing questions\n",
    "SYSTEM_PROMPT_2 = \"\"\"\n",
    "<<SYS>>\n",
    "You are alwys giving the answer to the question in a short and very simple manner and you answer in the language style that is asked from you. You leave out unnecessary words and if you have several options you would like to present only name one. If the answer is only one word you don't put a dot and the end. If you are asked to translate something you translate it.\n",
    "If you don't know the answer to a question, you tell them truthfully that you don't know and don't give false information. You are a helpful, respectful and honest assistant.\n",
    "Your answers should not contain harmful, unethical, racist, sexist, toxic, dangerous or illegal content. Please make sure that your answers are socially unbiased and positive.\n",
    "If a question does not make sense or is not factually coherent, please explain why, rather than answering something incorrectly.<</SYS>>\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + SYSTEM_PROMPT_2 +E_SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(instruction):\n",
    "    return B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "\n",
    "os.listdir(\"/Users/josi/Llama2_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LlamaCpp(\n",
    "    #model_path=\"/Users/josi/Llama2_weights/llama-2-7b.Q4_K_M.gguf\",\n",
    "    model_path = \"/Users/josi/Llama2_weights/llama-2-7b-chat.Q5_K_M.gguf\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2048,\n",
    "    top_p=1,\n",
    "    # callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionaries with test questions (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test questions for different categories\n",
    "informal_questions = {\n",
    "    \"Write an informal greeting.\": \"Hey\",\n",
    "    \"Write an informal response to the question: 'How's it going?'\": \"Hi, going good, hbu?\",\n",
    "    \"Write an informal message inviting someone to dinner tonight\": \"Hey, wanna join us for dinner tonight?\",\n",
    "    \"Compose an informal email closing.\": \"Cheers, [Your Name]\",\n",
    "}\n",
    "\n",
    "formal_questions = {\n",
    "    \"Write a formal greeting.\": \"Hello\",\n",
    "    \"Write a formal response to the question: 'How is it going?'\": \"Hello, I am doing well. How about you?\",\n",
    "    \"Write a formal message inviting someone to dinner tonight\": \"Hello, would you like to join us for dinner tonight?\",\n",
    "    \"Compose a professional email closing.\": \"Sincerely, [Your Name]\",\n",
    "}\n",
    "\n",
    "academic_questions = {\n",
    "    \"Describe the french revolution in two sentences using academic language.\": \"The historical event of the French Revolution was marked by widespread social upheaval...\",\n",
    "    \"Define gravity in two sentences using academic language.\": \"Gravity is a force that attracts two objects with mass\",\n",
    "}\n",
    "\n",
    "translation_questions = {\n",
    "    \"Translate 'hello' to Spanish.\": \"Hola\",\n",
    "    \"Translate 'good morning' to French.\": \"Bonjour\",\n",
    "    \"Translate 'Thank you' to German.\": \"Danke\",\n",
    "    \"How do you say 'hello' in Japanese?\": \"Konnichiwa (こんにちは)\",\n",
    "    \"Provide the Italian translation for 'book'.\": \"Libro\",\n",
    "}\n",
    "\n",
    "general_knowledge_questions = {\n",
    "    \"What is the capital of France?\": \"The capital of France is Paris\",\n",
    "    \"What is the square root of 25?\": \"The square root of 25 is 5\",\n",
    "    \"What is the circumference of the earth\":\"The circumference of the earth is 10\",\n",
    "}\n",
    "\n",
    "# Combine all test questions into a single dictionary\n",
    "test_questions = {**informal_questions, **formal_questions, **academic_questions, **translation_questions, **general_knowledge_questions}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model based on Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answers = []\n",
    "def evaluate_model(model, questions):\n",
    "    correct_answers = 0\n",
    "    total_questions = len(questions)\n",
    "\n",
    "    for question, expected_answer in questions.items():\n",
    "        # Get the model's answer for the current question\n",
    "        prompt = get_prompt(question)\n",
    "        model_answer = model(prompt)\n",
    "\n",
    "        # Remove trailing dots from the model's answer and Compare the model's answer to the expected answer (case-insensitive, whitespace-insensitive)\n",
    "        model_answer = model_answer.rstrip('.')\n",
    "        if model_answer.strip().lower() == expected_answer.strip().lower():\n",
    "            correct_answers += 1\n",
    "\n",
    "        # Check if any of the expected words are present in the model's answer\n",
    "        #if any(word.lower() in model_answer.lower() for word in expected_answer.split()):\n",
    "        #    correct_answers += 1\n",
    "\n",
    "        # Print the results for each question\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected Answer: {expected_answer}\")\n",
    "        print(f\"Model Answer: {model_answer}\")\n",
    "        print(\"------------\")\n",
    "\n",
    "        # Store the model's answer and the corresponding question in the array\n",
    "        model_answers.append({\n",
    "            \"question\": question,\n",
    "            \"expected_answer\": expected_answer,\n",
    "            \"model_answer\": model_answer,\n",
    "        })\n",
    "\n",
    "    # Print overall evaluation results\n",
    "    print(f\"Total Questions: {total_questions}\")\n",
    "    print(f\"Correct Answers: {correct_answers}\")\n",
    "    print(f\"Accuracy: {correct_answers / total_questions * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(llm, test_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model based on Rouge (Recall-Oriented Understudy for Gissing Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ROUGE (Recall-Oriented Understudy for Gissing Evaluation): a set of metrics used for evaluating the quality of summaries. It compares the generated summary with one or more reference summaries and calculates precision, recall, and F1-score \n",
    "\n",
    "#%pip install rouge\n",
    "from rouge import Rouge\n",
    "\n",
    "def evaluate_model(model, questions):\n",
    "    try:\n",
    "        correct_answers = 0\n",
    "        total_questions = len(questions)\n",
    "        model_answers = []\n",
    "        rouge_scores = {\"rouge-1\": {\"precision\": 0, \"recall\": 0, \"f1-score\": 0}}\n",
    "\n",
    "        for question, expected_answer in questions.items():\n",
    "            # Get the model's answer for the current question\n",
    "            prompt = get_prompt(question)\n",
    "            model_answer = model(prompt)\n",
    "\n",
    "            # format answer to match properly \n",
    "            model_answer = model_answer.rstrip('.')\n",
    "            #and calculate accuracy (which is currently defined as approximate match where only any word has to match the correct answer.)\n",
    "            if any(word.lower() in model_answer.lower() for word in expected_answer.split()):\n",
    "                correct_answers += 1\n",
    "\n",
    "            # Concatenate model's answer and expected answer into strings for Rouge evaluation\n",
    "            candidate_summary = \" \".join(model_answer.split())\n",
    "            reference_summary = \" \".join(expected_answer.split())\n",
    "\n",
    "            # Update Rouge scores\n",
    "            rouge = Rouge()\n",
    "            scores = rouge.get_scores(candidate_summary, reference_summary)\n",
    "\n",
    "            # Check if Rouge calculation was successful - WIP\n",
    "            if scores:\n",
    "                scores = scores[0]\n",
    "                for metric in [\"precision\", \"recall\", \"f1-score\"]:\n",
    "                    rouge_scores[f\"rouge-1\"][metric] += scores.get(f\"rouge-1\", {}).get(metric, 0)\n",
    "\n",
    "            # store model answers in array\n",
    "            model_answers.append({\n",
    "                \"question\": question,\n",
    "                \"expected_answer\": expected_answer,\n",
    "                \"model_answer\": model_answer,\n",
    "            })\n",
    "\n",
    "            #print each model answer with score\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Expected Answer: {expected_answer}\")\n",
    "            print(f\"Model Answer: {model_answer}\")\n",
    "            print(\"------------\")\n",
    "\n",
    "\n",
    "        # Calculate average Rouge scores\n",
    "        for metric in [\"precision\", \"recall\", \"f1-score\"]:\n",
    "            rouge_scores[f\"rouge-1\"][metric] /= total_questions\n",
    "\n",
    "        # Print overall evaluation results\n",
    "        print(f\"Total Questions: {total_questions}\")\n",
    "        print(f\"Correct Answers: {correct_answers}\")\n",
    "        print(f\"Accuracy: {correct_answers / total_questions * 100:.2f}%\")\n",
    "        print(\"Rouge Scores:\")\n",
    "        print(rouge_scores)\n",
    "\n",
    "        return model_answers, rouge_scores\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_answers, rouge_scores = evaluate_model(llm, test_questions)\n",
    "\n",
    "\n",
    "###current output: \n",
    "#Total Questions: 19\n",
    "#Correct Answers: 16\n",
    "#Accuracy: 84.21%\n",
    "#Rouge Scores:\n",
    "#{'rouge-1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Performance Evaluation - Perplexity: measure that quantifies how well the model predicts a sample of text. Lower perplexity values indicate better performance \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
