{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain llama-cpp-python \n",
    "#CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "# DEFAULT_SYSTEM_PROMPT = \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "<<SYS>>\n",
    "You are a chatbot that helps people learn a new language. You always give the answer to the question in a formal manner.\n",
    "If you don't know the answer to a question, you tell them truthfully that you don't know and don't give false information. You are a helpful, respectful and honest assistant.\n",
    "Your answers should not contain harmful, unethical, racist, sexist, toxic, dangerous or illegal content. Please make sure that your answers are socially unbiased and positive.\n",
    "If a question does not make sense or is not factually coherent, please explain why, rather than answering something incorrectly.<</SYS>>\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + DEFAULT_SYSTEM_PROMPT +E_SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_2 = \"\"\"\n",
    "<<SYS>>\n",
    "You are a chatbot that always gives the answer to the question in a formal, short and very simple manner. You leave out unnecessary words and if the answer is only one word you don't put a dot and the end. If you are asked to translate something you translate it.\n",
    "If you don't know the answer to a question, you tell them truthfully that you don't know and don't give false information. You are a helpful, respectful and honest assistant.\n",
    "Your answers should not contain harmful, unethical, racist, sexist, toxic, dangerous or illegal content. Please make sure that your answers are socially unbiased and positive.\n",
    "If a question does not make sense or is not factually coherent, please explain why, rather than answering something incorrectly.<</SYS>>\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + SYSTEM_PROMPT_2 +E_SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(instruction):\n",
    "    return B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "\n",
    "os.listdir(\"/Users/josi/Llama2_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LlamaCpp(\n",
    "    #model_path=\"/Users/josi/Llama2_weights/llama-2-7b.Q4_K_M.gguf?download=true\",\n",
    "    model_path = \"/Users/josi/Llama2_weights/llama-2-7b-chat.Q5_K_M.gguf\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2048,\n",
    "    top_p=1,\n",
    "    # callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answers = []\n",
    "def evaluate_model(model, questions):\n",
    "    correct_answers = 0\n",
    "    total_questions = len(questions)\n",
    "\n",
    "    for question, expected_answer in questions.items():\n",
    "        # Get the model's answer for the current question\n",
    "        prompt = get_prompt(question)\n",
    "        model_answer = model(prompt)\n",
    "\n",
    "        # Remove trailing dots from the model's answer and Compare the model's answer to the expected answer (case-insensitive, whitespace-insensitive)\n",
    "        model_answer = model_answer.rstrip('.')\n",
    "        if model_answer.strip().lower() == expected_answer.strip().lower():\n",
    "            correct_answers += 1\n",
    "\n",
    "        # Check if any of the expected words are present in the model's answer\n",
    "        #if any(word.lower() in model_answer.lower() for word in expected_answer.split()):\n",
    "        #    correct_answers += 1\n",
    "\n",
    "        # Print the results for each question\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected Answer: {expected_answer}\")\n",
    "        print(f\"Model Answer: {model_answer}\")\n",
    "        print(\"------------\")\n",
    "\n",
    "        # Store the model's answer and the corresponding question in the array\n",
    "        model_answers.append({\n",
    "            \"question\": question,\n",
    "            \"expected_answer\": expected_answer,\n",
    "            \"model_answer\": model_answer,\n",
    "        })\n",
    "\n",
    "    # Print overall evaluation results\n",
    "    print(f\"Total Questions: {total_questions}\")\n",
    "    print(f\"Correct Answers: {correct_answers}\")\n",
    "    print(f\"Accuracy: {correct_answers / total_questions * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# Dictionary with questions\n",
    "test_questions = {\n",
    "    \"What is the capital of France?\": \"The capital of France is Paris\",\n",
    "    \"Translate 'hello' to Spanish.\": \"Hola\",\n",
    "    \"Write an informal greeting.\": \"Hey there!\",\n",
    "    \"Explain the concept of gravity.\": \"Gravity is a force that attracts two objects with mass\",\n",
    "    \"What is the square root of 25?\": \"The square root of 25 is 5\",\n",
    "    \"Translate 'good morning' to French.\": \"Bonjour\",\n",
    "    \"Translate 'Thank you' to German.\": \"Danke\",\n",
    "    \"How do you say 'hello' in Japanese?\": \"Konnichiwa (こんにちは)\",\n",
    "    \"Provide the Italian translation for 'book'.\": \"Libro\",\n",
    "    \"Write an informal response to the question: 'How's it going?'\": \"Hey! It's going well, thanks for asking! How about you?\",\n",
    "    \"Explain the concept of artificial intelligence in an academic style.\": \"Artificial Intelligence (AI) refers to the development of algorithms and computational systems that enable machines to exhibit intelligent behavior, learn from experience, and perform tasks traditionally requiring human cognitive abilities. It encompasses a broad range of techniques, including machine learning, natural language processing, and expert systems. In an academic context, AI research focuses on understanding, modeling, and implementing intelligent agents capable of reasoning, problem-solving, and adapting to complex environments. The goal is to create systems that emulate human-like intelligence, allowing machines to autonomously analyze data, make informed decisions, and continuously improve their performance across diverse domains\",\n",
    "    \"Write a casual message inviting someone to an event.\": \"Hey, want to join us for an event this weekend?\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(llm, test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample Test Suite\n",
    "test_suite = {\n",
    "    \"Test1\": \"Question 1\",\n",
    "    \"Test2\": \"Question 2\",\n",
    "    \"Test3\": \"Question 3\",\n",
    "    \"Test4\": \"Question 4\",\n",
    "    \"Test5\": \"Question 5\",\n",
    "}\n",
    "\n",
    "# Sample Predictions from the Language Model\n",
    "predictions = {\n",
    "    \"Test1\": \"Answer 1\",\n",
    "    \"Test2\": \"Answer 2\",\n",
    "    \"Test3\": \"Answer 3\",\n",
    "    \"Test4\": \"Answer 4\",\n",
    "    \"Test5\": \"Answer 5\",\n",
    "}\n",
    "\n",
    "# Function to Calculate the Accuracy of the Model\n",
    "def accuracy(test_suite, predictions):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for test in test_suite:\n",
    "        total += 1\n",
    "        if test_suite[test] == predictions[test]:\n",
    "            correct += 1\n",
    "    return correct / total\n",
    "\n",
    "# Evaluate the Model's Performance\n",
    "print(\"Accuracy: \", accuracy(test_suite, predictions))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
