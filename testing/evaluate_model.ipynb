{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain llama-cpp-python \n",
    "#CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define System Prompt and load Llama2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFAULT PROMPT\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + DEFAULT_SYSTEM_PROMPT + E_SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chatbot prompt for later use case\n",
    "SYSTEM_PROMPT_USECASE = \"\"\"\n",
    "<<SYS>>\n",
    "You are a chatbot that helps people learn a new language on a platform. You always give the answer to the question in an academic language style.\n",
    "If you don't know the answer to a question, you tell them truthfully that you don't know and don't give false information. You are a helpful, respectful and honest assistant.\n",
    "Your answers should not contain harmful, unethical, racist, sexist, toxic, dangerous or illegal content. Please make sure that your answers are socially unbiased and positive if nature#.\n",
    "If a question does not make sense or is not factually coherent, please explain why, rather than answering something incorrectly.<</SYS>>\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + SYSTEM_PROMPT_USECASE +E_SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general prompt to test llm for testing questions\n",
    "SYSTEM_PROMPT_EVAL = \"\"\"\n",
    "<<SYS>>\n",
    "You will be presented with a question and four answer options where only one is correct. You will respond with the letter that represents the correct answer option.\n",
    "Do not add any answer options, only choose the correct one from the for options presented because one of them is correct.\n",
    "If you don't know the answer, be honest and state that you don't know.\n",
    "<</SYS>>\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + SYSTEM_PROMPT_EVAL +E_SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(instruction):\n",
    "    return B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "\n",
    "os.listdir(\"/Users/josi/Llama2_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path= \"/Users/josi/Llama2_weights/llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    temperature=0,\n",
    "    max_tokens=48,\n",
    "    top_p=1,\n",
    "    #callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionaries with test questions (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##QUESTIONS WITH ANSWER OPTIONS\n",
    "questions_data = [\n",
    "    {\n",
    "        \"question\": \"Choose the correct option that represents an informal greeting.\",\n",
    "        \"choices\": {\"a\": \"Hi\", \"b\": \"Good Day\", \"c\": \"Greetings\", \"d\": \"Dear Mr./Mrs.\"},\n",
    "        \"correct\": \"a: Hi\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct option that represents an informal response to the question: 'How's it going?'\",\n",
    "        \"choices\": {\"a\": \"Hi, good hbu?\", \"b\": \"Greetings, everything is fine, and you?\", \"c\": \"Good day, I'm well, thank you. How are you?\", \"d\": \"Hello, everything's fine\"},\n",
    "        \"correct\": \"a: Hi, good hbu?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct option that represents an informal email closing.\",\n",
    "        \"choices\": {\"a\": \"Cheers\", \"b\": \"Sincerely\", \"c\": \"Best regards\", \"d\": \"I am awaiting your response\"},\n",
    "        \"correct\": \"a: Cheers\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct option that represents a formal greeting.\",\n",
    "        \"choices\": {\"a\": \"Good Day\", \"b\": \"Hi\", \"c\": \"Hey\", \"d\": \"Yo\"},\n",
    "        \"correct\": \"a: Good Day\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct option that represents a formal response to the question: 'How are you doing?\",\n",
    "        \"choices\": {\"a\": \"Hello, I am doing well. How about you?\", \"b\": \"Hi, good you?\", \"c\": \"Not too bad, you?\", \"d\": \"good, you?\"},\n",
    "        \"correct\": \"a: Hello, I am doing well. How about you?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct option that represents a professional email closing.\",\n",
    "        \"choices\": {\"a\": \"Sincerely\", \"b\": \"Cheers\", \"c\": \"See you!\", \"d\": \"Until next time.\"},\n",
    "        \"correct\": \"a: Sincerely\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct translation for 'hello' in Spanish.\",\n",
    "        \"choices\": {\"a\":\"Hola\", \"b\": \"Hello\", \"c\":\"Hallo\", \"d\": \"Bonjour\"},\n",
    "        \"correct\": \"a: Hola\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct translation for 'Thank you' in German.\",\n",
    "        \"choices\": {\"a\": \"Danke\", \"b\": \"Thanks\", \"c\": \"Gracias\", \"d\": \"Auf Wiedersehen\"},\n",
    "        \"correct\": \"a: Danke\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct translation for 'book' in italian.\",\n",
    "        \"choices\": {\"a\": \"Libro\", \"b\": \"Buch\", \"c\": \"Plant\", \"d\": \"Libre\"},\n",
    "        \"correct\": \"a: Libro\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct answer that is the capital of France?\",\n",
    "        \"choices\": {\"a\": \"Paris\", \"b\": \"Berlin\", \"c\": \"Madrid\", \"d\": \"New York\"},\n",
    "        \"correct\": \"a: Paris\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct answer for the following math problem: 3x + 5 = 20.\",\n",
    "        \"choices\": {\"a\": \"x = 5\", \"b\": \"x = 15\", \"c\": \"x = 10\", \"d\": \"x = 2\"},\n",
    "        \"correct\": \"a: x = 5\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct answer for the following statement: If 'cat' is to 'kitten,' what is 'dog' to?\",\n",
    "        \"choices\": {\"a\": \"'Dog' is to 'puppy'\", \"b\": \"'Dog' is to 'kitten'\", \"c\": \"'Dog' is to 'cub'\", \"d\": \"'Dog' is to 'kitty'\"},\n",
    "        \"correct\": \"a: 'Dog' is to 'puppy'\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct answer for the following statement: Identify the named Person in the sentence 'Elon Musk is the CEO of a company that produces cars.'\",\n",
    "        \"choices\": {\"a\": \"Elon Musk\", \"b\": \"CEO\", \"c\": \"company\", \"d\": \"cars\"},\n",
    "        \"correct\": \"a: Elon Musk\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct answer for the following statement: Extract the date mentioned in the text 'The conference is scheduled for January 25, 2023.'\",\n",
    "        \"choices\": {\"a\": \"January 25, 2023\", \"b\": \"May 25-27, 2023.\", \"c\": \"January 27, 2023.\", \"d\": \"January 2, 2025\"},\n",
    "        \"correct\": \"a: January 25, 2023\"\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "for question in questions_data:\n",
    "    print(\"Question:\", question[\"question\"])\n",
    "    print(\"Answer Options:\")\n",
    "    for option, answer in question[\"choices\"].items():\n",
    "        print(f\"{option}: {answer}\")\n",
    "    print(\"Correct Answer:\", f\"{question['correct']}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model based on Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answers = []\n",
    "\n",
    "def evaluate_model(model, questions_data):\n",
    "    correct_answers = 0\n",
    "    total_questions = len(questions_data)\n",
    "\n",
    "    for question_data in questions_data:\n",
    "        question = question_data[\"question\"]\n",
    "        choices = question_data[\"choices\"]\n",
    "        correct_option = question_data[\"correct\"]\n",
    "\n",
    "        # Get model's answer for current question\n",
    "        prompt = f\"{question}\\nAnswer Options:\\n\"\n",
    "        for option, answer in choices.items():\n",
    "            prompt += f\"{option}: {answer}\\n\"\n",
    "        prompt = get_prompt(prompt)\n",
    "        #print(prompt)\n",
    "        model_answer = model(prompt)\n",
    "\n",
    "        # Check if selected option in model's answer matches the correct option\n",
    "        if model_answer.strip() == correct_option.strip():\n",
    "            correct_answers += 1\n",
    "\n",
    "        # Print results for each question\n",
    "        print(f\"Question: {question}\")\n",
    "        print(\"Answer Options:\")\n",
    "        for option, answer in choices.items():\n",
    "            print(f\"{option}: {answer}\")\n",
    "        print(f\"Correct Answer: {correct_option}\")\n",
    "        print(f\"Model Answer: {model_answer}\")\n",
    "        print(\"------------\")\n",
    "\n",
    "        # Store the model's answer and the corresponding question in the array\n",
    "        model_answers.append({\n",
    "            \"question\": question,\n",
    "            \"choices\": choices,\n",
    "            \"correct_option\": correct_option,\n",
    "            \"selected_option\": model_answers,\n",
    "        })\n",
    "\n",
    "    # print overall evaluation results\n",
    "    print(f\"Total Questions: {total_questions}\")\n",
    "    print(f\"Correct Answers: {correct_answers}\")\n",
    "    print(f\"Accuracy: {correct_answers / total_questions * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run method to get accuracy score\n",
    "evaluate_model(llm, questions_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model based on Rouge (Recall-Oriented Understudy for Gissing Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of metrics used for evaluating the quality of summaries. It compares the generated summary with one or more reference summaries and calculates precision, recall, and F1-score \n",
    "\n",
    "- ROUGE-N (quantifies the overlap of N-grams, contiguous sequences of N items (typically words or characters), between the system-generated summary and the reference summary) \n",
    "- ROUGE-1 (unigram overlap): This metric measures the overlap of unigrams (single words) between the generated summary and the reference summary. It focuses on the recall of unigrams.\n",
    "- ROUGE-2 (bigram overlap): Similar to ROUGE-1, but it measures the overlap of bigrams (pairs of consecutive words) instead of unigrams. It evaluates the recall of bigrams.\n",
    "- ROUGE-L (longest common subsequence): Instead of measuring word overlap, ROUGE-L focuses on the longest common subsequence (LCS) between the generated and reference summaries. It evaluates the recall of the longest common subsequence\n",
    "\n",
    "- high precision value suggests that the words or phrases churned out by the machine translation or submodel are primarily accurate\n",
    "- high recall value, ideally close to 1, implies that the content of the machine-generated output aligns closely with the human-made reference. It signifies the model's proficiency in capturing relevant information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = \"the #### transcript is a written version of each day 's cnn student news program use this transcript to he    lp students with reading comprehension and vocabulary use the weekly newsquiz to test your knowledge of storie s you     saw on cnn student news\"\n",
    "\n",
    "reference = \"this page includes the show transcript use the transcript to help students with reading comprehension and     vocabulary at the bottom of the page , comment for a chance to be mentioned on cnn student news . you must be a teac    her or a student age # # or older to request a mention on the cnn student news roll call . the weekly newsquiz tests     students ' knowledge of even ts in the news\"\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also check if rouge-score library better\n",
    "\n",
    "#%pip install rouge\n",
    "from rouge import Rouge\n",
    "\n",
    "##Rouge() hat rouge-1, rouge-2 and rouge-l as option \n",
    "\n",
    "def evaluate_model(model, questions):\n",
    "    try:\n",
    "        correct_answers = 0\n",
    "        total_questions = len(questions)\n",
    "        model_answers = []\n",
    "        rouge_scores = {\"rouge-1\": {\"recall\": 0, \"precision\": 0, \"f1-score\": 0}} \n",
    "\n",
    "        for question_data in questions_data:\n",
    "            question = question_data[\"question\"]\n",
    "            choices = question_data[\"choices\"]\n",
    "            correct_option = question_data[\"correct\"]\n",
    "\n",
    "            # Get model's answer for current question\n",
    "            prompt = f\"{question}\\nAnswer Options:\\n\"\n",
    "            for option, answer in choices.items():\n",
    "                prompt += f\"{option}: {answer}\\n\"\n",
    "            prompt = get_prompt(prompt)\n",
    "            model_answer = model(prompt)\n",
    "\n",
    "            # Check if selected option in model's answer matches the correct option\n",
    "            model_answer = model_answer.strip()\n",
    "            correct_option = correct_option.strip()\n",
    "\n",
    "            if model_answer == correct_option:\n",
    "                correct_answers += 1\n",
    "\n",
    "            # Get Rouge Score\n",
    "            rouge = Rouge()\n",
    "            scores = rouge.get_scores(correct_option, model_answer) \n",
    "            print(scores)\n",
    "            # Check if Rouge calculation was successful - WIP -> see test below \n",
    "            if scores:\n",
    "                scores = scores[0]\n",
    "                for metric in [\"recall\", \"precision\", \"f1-score\"]:\n",
    "                    rouge_scores[f\"rouge-1\"][metric] += scores.get(f\"rouge-1\", {}).get(metric, 0)\n",
    "            \n",
    "\n",
    "            # Print results for each question\n",
    "            print(f\"Question: {question}\")\n",
    "            print(\"Answer Options:\")\n",
    "            for option, answer in choices.items():\n",
    "                print(f\"{option}: {answer}\")\n",
    "            print(f\"Correct Answer: {correct_option}\")\n",
    "            print(f\"Model Answer: {model_answer}\")\n",
    "            print(\"------------\")\n",
    "\n",
    "            # Store the model's answer and the corresponding question in the array\n",
    "            model_answers.append({\n",
    "                \"question\": question,\n",
    "                \"choices\": choices,\n",
    "                \"correct_option\": correct_option,\n",
    "                \"selected_option\": model_answers,\n",
    "            })\n",
    "\n",
    "\n",
    "        # Calculate average Rouge scores\n",
    "        for metric in [\"recall\", \"precision\", \"f1-score\"]:\n",
    "            avg_rouge = rouge_scores[f\"rouge-1\"][metric] / total_questions\n",
    "\n",
    "        # Print overall evaluation results\n",
    "        print(f\"Total Questions: {total_questions}\")\n",
    "        print(f\"Correct Answers: {correct_answers}\")\n",
    "        print(f\"Accuracy: {correct_answers / total_questions * 100:.2f}%\")\n",
    "        print(\"Average Rouge Score: \", avg_rouge)\n",
    "\n",
    "        return model_answers, rouge_scores\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answers, rouge_scores = evaluate_model(llm, questions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##test rouge library - https://pypi.org/project/rouge/ \n",
    "## -> not clear which rouge metric meant: theres rouge-1/2/l - only referencing paper\n",
    "##Note: \"f\" stands for f1_score, \"p\" stands for precision, \"r\" stands for recall.\n",
    "\n",
    "\n",
    "##1 sentence\n",
    "hypothesis = \"I am doing well. How about you?\"\n",
    "reference = \"Hello, I am doing well. How about you?\"\n",
    "#reference = \"Good day! I'm doing well, thank you for asking. How about you?\"\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##multiple sentences\n",
    "import json\n",
    "\n",
    "# Load some sentences\n",
    "with open('./tests/data.json') as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "hyps, refs = map(list, zip(*[[d['hyp'], d['ref']] for d in data]))\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hyps, refs)\n",
    "# or\n",
    "scores = rouge.get_scores(hyps, refs, avg=True)\n",
    "\n",
    "\n",
    "##two files\n",
    "from rouge import FilesRouge\n",
    "\n",
    "files_rouge = FilesRouge()\n",
    "scores = files_rouge.get_scores(hyp_path, ref_path)\n",
    "# or\n",
    "scores = files_rouge.get_scores(hyp_path, ref_path, avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bitsandbytes\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation - Perplexity: measure that quantifies how well the model predicts a sample of text. Lower perplexity values indicate better performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, LlamaTokenizer\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "model_name = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n",
    "max_memory = f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  device_map='auto',\n",
    "  load_in_4bit=True,\n",
    "  max_memory=max_memory,\n",
    "  do_sample=True,\n",
    "  torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.max_length\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "device = \"cuda\"\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
