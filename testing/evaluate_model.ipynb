{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.1.6-py3-none-any.whl (811 kB)\n",
      "\u001b[K     |████████████████████████████████| 811 kB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.39.tar.gz (10.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.8 MB 13.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pydantic<3,>=1\n",
      "  Downloading pydantic-2.6.1-py3-none-any.whl (394 kB)\n",
      "\u001b[K     |████████████████████████████████| 394 kB 15.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Collecting langsmith<0.1,>=0.0.83\n",
      "  Downloading langsmith-0.0.90-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 6.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from langchain) (2.27.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from langchain) (6.0)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.9.3-cp39-cp39-macosx_10_9_x86_64.whl (398 kB)\n",
      "\u001b[K     |████████████████████████████████| 398 kB 12.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from langchain) (4.0.1)\n",
      "Collecting langchain-community<0.1,>=0.0.18\n",
      "  Downloading langchain_community-0.0.19-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 13.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from langchain) (1.4.32)\n",
      "Collecting langchain-core<0.2,>=0.1.22\n",
      "  Downloading langchain_core-0.1.22-py3-none-any.whl (239 kB)\n",
      "\u001b[K     |████████████████████████████████| 239 kB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from langchain) (1.21.5)\n",
      "Collecting diskcache>=5.6.1\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 6.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions>=4.5.0\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from llama-cpp-python) (2.11.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (5.2.0)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 11.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.0.1)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting langsmith<0.1,>=0.0.83\n",
      "  Downloading langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 6.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting packaging<24.0,>=23.2\n",
      "  Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.22->langchain) (3.5.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (3.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.2.0)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting pydantic-core==2.16.2\n",
      "  Downloading pydantic_core-2.16.2-cp39-cp39-macosx_10_12_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 13.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/josi/opt/anaconda3/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.39-cp39-cp39-macosx_10_16_x86_64.whl size=2598128 sha256=ab611f29fd53c8d28d70e9f7b746e2dd3dfabd84d2f9b4467ebcaa0ec397c023\n",
      "  Stored in directory: /Users/josi/Library/Caches/pip/wheels/3d/71/b5/b02cf8be6bcfa7486a238e9bab01f5d9e010ed46633d8c3a5a\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: typing-extensions, pydantic-core, annotated-types, pydantic, packaging, jsonpointer, typing-inspect, tenacity, marshmallow, langsmith, jsonpatch, langchain-core, dataclasses-json, aiohttp, langchain-community, diskcache, llama-cpp-python, langchain\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.1.1\n",
      "    Uninstalling typing-extensions-4.1.1:\n",
      "      Successfully uninstalled typing-extensions-4.1.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.0.1\n",
      "    Uninstalling tenacity-8.0.1:\n",
      "      Successfully uninstalled tenacity-8.0.1\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.8.1\n",
      "    Uninstalling aiohttp-3.8.1:\n",
      "      Successfully uninstalled aiohttp-3.8.1\n",
      "Successfully installed aiohttp-3.9.3 annotated-types-0.6.0 dataclasses-json-0.6.4 diskcache-5.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.6 langchain-community-0.0.19 langchain-core-0.1.22 langsmith-0.0.87 llama-cpp-python-0.2.39 marshmallow-3.20.2 packaging-23.2 pydantic-2.6.1 pydantic-core-2.16.2 tenacity-8.2.3 typing-extensions-4.9.0 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install langchain llama-cpp-python \n",
    "#CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define System Prompt and load Llama2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFAULT PROMPT\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + DEFAULT_SYSTEM_PROMPT + E_SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chatbot prompt for later use case\n",
    "SYSTEM_PROMPT_USECASE = \"\"\"\n",
    "<<SYS>>\n",
    "You are a chatbot that helps people learn a new language on a platform. You always give the answer to the question in an academic language style.\n",
    "If you don't know the answer to a question, you tell them truthfully that you don't know and don't give false information. You are a helpful, respectful and honest assistant.\n",
    "Your answers should not contain harmful, unethical, racist, sexist, toxic, dangerous or illegal content. Please make sure that your answers are socially unbiased and positive if nature#.\n",
    "If a question does not make sense or is not factually coherent, please explain why, rather than answering something incorrectly.<</SYS>>\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + SYSTEM_PROMPT_USECASE +E_SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general prompt to test llm for testing questions\n",
    "SYSTEM_PROMPT_EVAL_ACCURACY = \"\"\"\n",
    "<<SYS>>\n",
    "You will be presented with a question and several answer options where only one is correct. You will respond with the letter that represents the correct answer option.\n",
    "Do not add any answer options, only choose the correct one from the for options presented because one of them is correct.\n",
    "If you don't know the answer, be honest and state that you don't know.\n",
    "<</SYS>>\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + SYSTEM_PROMPT_EVAL_ACCURACY +E_SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'llama-2-7b.Q4_K_M.gguf', 'llama-2-7b-chat.Q4_K_M.gguf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prompt(instruction):\n",
    "    return B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "\n",
    "os.listdir(\"/Users/josi/Llama2_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/josi/Llama2_weights/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.15.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:        blk.15.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:             blk.15.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:             blk.15.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.16.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:        blk.16.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.16.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.16.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.17.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.17.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.17.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.17.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.18.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:        blk.18.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.18.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.18.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.19.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.19.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.19.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.19.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:              blk.2.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.2.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:         blk.2.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.2.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:              blk.2.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.20.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.20.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.20.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.20.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.20.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.20.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.21.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.21.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.21.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.21.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.21.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.21.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.22.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.22.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.22.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.22.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.22.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.22.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.23.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.23.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.23.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.23.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.23.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.23.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:            blk.3.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:              blk.3.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:              blk.3.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:         blk.3.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:              blk.3.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:              blk.3.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:            blk.4.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:              blk.4.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:              blk.4.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:         blk.4.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:              blk.4.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:              blk.4.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:            blk.5.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:              blk.5.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:              blk.5.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:         blk.5.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:              blk.5.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:              blk.5.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:            blk.6.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:              blk.6.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:              blk.6.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:         blk.6.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:              blk.6.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:              blk.6.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.7.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:              blk.7.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:              blk.7.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:         blk.7.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:              blk.7.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:              blk.7.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:            blk.8.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:              blk.8.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:              blk.8.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:         blk.8.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:              blk.8.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.8.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:            blk.9.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:              blk.9.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:              blk.9.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:         blk.9.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:              blk.9.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:              blk.9.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:        blk.24.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:        blk.25.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.26.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:        blk.27.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:        blk.28.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:        blk.29.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:        blk.30.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  18:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MB\n",
      "llm_load_tensors: mem required  = 3891.35 MB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "llama_build_graph: non-view tensors processed: 740/740\n",
      "llama_new_context_with_model: compute buffer total size = 2.66 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path= \"/Users/josi/Llama2_weights/llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    temperature=0,\n",
    "    max_tokens=48,\n",
    "    top_p=1,\n",
    "    #callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionaries with test questions (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Questions with Answer options for ACCURACY\n",
    "questions_accuracy = [\n",
    "\n",
    "    #Informal Style\n",
    "    {\n",
    "        \"question\": \"Choose the correct option that represents an Informal greeting.\",\n",
    "        \"choices\": {\"a\": \"Hi\", \"b\": \"Good Day\", \"c\": \"Greetings\", \"d\": \"Dear Mr./Mrs.\"},\n",
    "        \"correct\": \"a: Hi\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct option that represents an Informal response to the question: 'How's it going?'\",\n",
    "        \"choices\": {\"a\": \"Hi, good hbu?\", \"b\": \"Greetings, everything is fine, and you?\", \"c\": \"Good day, I'm well, thank you. How are you?\", \"d\": \"Hello, everything's fine\"},\n",
    "        \"correct\": \"a: Hi, good hbu?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct option that represents an informal email closing.\",\n",
    "        \"choices\": {\"a\": \"Cheers\", \"b\": \"Sincerely\", \"c\": \"Best regards\", \"d\": \"I am awaiting your response\"},\n",
    "        \"correct\": \"a: Cheers\"\n",
    "    },\n",
    "\n",
    "    #Formal Style\n",
    "    {\n",
    "        \"question\": \"Choose the correct option that represents a formal greeting.\",\n",
    "        \"choices\": {\"a\": \"Good Day\", \"b\": \"Hi\", \"c\": \"Hey\", \"d\": \"Yo\"},\n",
    "        \"correct\": \"a: Good Day\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct option that represents a formal response to the question: 'How are you doing?\",\n",
    "        \"choices\": {\"a\": \"Hello, I am doing well. How about you?\", \"b\": \"Hi, good you?\", \"c\": \"Not too bad, you?\", \"d\": \"good, you?\"},\n",
    "        \"correct\": \"a: Hello, I am doing well. How about you?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct option that represents a professional email closing.\",\n",
    "        \"choices\": {\"a\": \"Sincerely\", \"b\": \"Cheers\", \"c\": \"See you!\", \"d\": \"Until next time.\"},\n",
    "        \"correct\": \"a: Sincerely\"\n",
    "    },\n",
    "\n",
    "    #academic Style\n",
    "    {\n",
    "        \"question\": \"Choose the correct option that represents an academic paper introduction.\",\n",
    "        \"choices\": {\"a\": \"Today we present our topic XYZ and we will also focus on ABC.\", \"b\": \"In this paper XYZ is presented with a focus on ABC.\", \"c\": \"Hi! In our paper we write about XYZ and ABC.\", \"d\": \"I show you something about XYZ in this paper.\"},\n",
    "        \"correct\": \"b: In this paper XYZ is presented with a focus on ABC.\"\n",
    "    },\n",
    "    \n",
    "\n",
    "\n",
    "    #Detect correct language style\n",
    "    {\n",
    "        \"question\": \"The improvements canʼt be introduced due to funding restrictions.\",\n",
    "        \"choices\": {\"a\": \"Formal Language Style\", \"b\": \"Informal Language Style\", \"c\": \"Academic Language Style\"},\n",
    "        \"correct\": \"b: Informal Language Style\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \" It was raining cats and dogs.\",\n",
    "        \"choices\": {\"a\": \"Formal Language Style\", \"b\": \"Informal Language Style\", \"c\": \"Academic Language Style\"},\n",
    "        \"correct\": \"b: Informal Language Style\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Improvements cannot be introduced due to funding restrictions.\",\n",
    "        \"choices\": {\"a\": \"Formal Language Style\", \"b\": \"Informal Language Style\", \"c\": \"Academic Language Style\"},\n",
    "        \"correct\": \"a: Formal Language Style\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"During the interview, students were asked about their experiences.\",\n",
    "        \"choices\": {\"a\": \"Formal Language Style\", \"b\": \"Informal Language Style\", \"c\": \"Academic Language Style\"},\n",
    "        \"correct\": \"a: Formal Language Style\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"New results in this area are produced by the research group.\",\n",
    "        \"choices\": {\"a\": \"Formal Language Style\", \"b\": \"Informal Language Style\", \"c\": \"Academic Language Style\"},\n",
    "        \"correct\": \"c: Academic Language Style\"\n",
    "    },\n",
    "\n",
    "    #Translation\n",
    "    {\n",
    "        \"question\": \"Choose the correct translation for 'hello' in Spanish.\",\n",
    "        \"choices\": {\"a\":\"Hola\", \"b\": \"Hello\", \"c\":\"Hallo\", \"d\": \"Bonjour\"},\n",
    "        \"correct\": \"a: Hola\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct translation for 'Thank you' in German.\",\n",
    "        \"choices\": {\"a\": \"Danke\", \"b\": \"Thanks\", \"c\": \"Gracias\", \"d\": \"Auf Wiedersehen\"},\n",
    "        \"correct\": \"a: Danke\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct translation for 'book' in italian.\",\n",
    "        \"choices\": {\"a\": \"Libro\", \"b\": \"Buch\", \"c\": \"Plant\", \"d\": \"Libre\"},\n",
    "        \"correct\": \"a: Libro\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct answer that is the capital of France?\",\n",
    "        \"choices\": {\"a\": \"Paris\", \"b\": \"Berlin\", \"c\": \"Madrid\", \"d\": \"New York\"},\n",
    "        \"correct\": \"a: Paris\"\n",
    "    },\n",
    "\n",
    "    #General\n",
    "    {\n",
    "        \"question\": \"Choose the correct answer for the following math problem: 3x + 5 = 20.\",\n",
    "        \"choices\": {\"a\": \"x = 5\", \"b\": \"x = 15\", \"c\": \"x = 10\", \"d\": \"x = 2\"},\n",
    "        \"correct\": \"a: x = 5\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct answer for the following statement: If 'cat' is to 'kitten,' what is 'dog' to?\",\n",
    "        \"choices\": {\"a\": \"'Dog' is to 'puppy'\", \"b\": \"'Dog' is to 'kitten'\", \"c\": \"'Dog' is to 'cub'\", \"d\": \"'Dog' is to 'kitty'\"},\n",
    "        \"correct\": \"a: 'Dog' is to 'puppy'\"\n",
    "    },\n",
    "\n",
    "    #NER\n",
    "    {\n",
    "        \"question\": \"Choose the correct answer for the following statement: Identify the named Person in the sentence 'Elon Musk is the CEO of a company that produces cars.'\",\n",
    "        \"choices\": {\"a\": \"Elon Musk\", \"b\": \"CEO\", \"c\": \"company\", \"d\": \"cars\"},\n",
    "        \"correct\": \"a: Elon Musk\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct answer for the following statement: Extract the date mentioned in the text 'The conference is scheduled for January 25, 2023.'\",\n",
    "        \"choices\": {\"a\": \"January 25, 2023\", \"b\": \"May 25-27, 2023.\", \"c\": \"January 27, 2023.\", \"d\": \"January 2, 2025\"},\n",
    "        \"correct\": \"a: January 25, 2023\"\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "for question in questions_accuracy:\n",
    "    print(\"Question:\", question[\"question\"])\n",
    "    print(\"Answer Options:\")\n",
    "    for option, answer in question[\"choices\"].items():\n",
    "        print(f\"{option}: {answer}\")\n",
    "    print(\"Correct Answer:\", f\"{question['correct']}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model based on Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answers = []\n",
    "\n",
    "def evaluate_model_accuracy(model, questions_accuracy):\n",
    "    try:\n",
    "        correct_answers = 0\n",
    "        total_questions = len(questions_accuracy)\n",
    "\n",
    "        for question_data in questions_accuracy:\n",
    "            question = question_data[\"question\"]\n",
    "            choices = question_data[\"choices\"]\n",
    "            correct_option = question_data[\"correct\"]\n",
    "\n",
    "            # Get model's answer for current question\n",
    "            prompt = f\"{question}\\nAnswer Options:\\n\"\n",
    "            for option, answer in choices.items():\n",
    "                prompt += f\"{option}: {answer}\\n\"\n",
    "            prompt = get_prompt(prompt)\n",
    "            #print(prompt)\n",
    "            model_answer = model(prompt)\n",
    "\n",
    "            # Check if selected option in model's answer matches the correct option\n",
    "            if model_answer.strip() == correct_option.strip():\n",
    "                correct_answers += 1\n",
    "\n",
    "            # Print results for each question\n",
    "            print(f\"Question: {question}\")\n",
    "            print(\"Answer Options:\")\n",
    "            for option, answer in choices.items():\n",
    "                print(f\"{option}: {answer}\")\n",
    "            print(f\"Correct Answer: {correct_option}\")\n",
    "            print(f\"Model Answer: {model_answer}\")\n",
    "            print(\"------------\")\n",
    "\n",
    "            # Store the model's answer and the corresponding question in the array\n",
    "            model_answers.append({\n",
    "                \"question\": question,\n",
    "                \"choices\": choices,\n",
    "                \"correct_option\": correct_option,\n",
    "                \"selected_option\": model_answers,\n",
    "            })\n",
    "\n",
    "        # print overall evaluation results\n",
    "        print(f\"Total Questions: {total_questions}\")\n",
    "        print(f\"Correct Answers: {correct_answers}\")\n",
    "        print(f\"Accuracy: {correct_answers / total_questions * 100:.2f}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "#run method to get accuracy score\n",
    "evaluate_model_accuracy(llm, questions_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model based on Rouge (Recall-Oriented Understudy for Gissing Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of metrics used for evaluating the quality of summaries. It compares the generated summary with one or more reference summaries and calculates precision, recall, and F1-score \n",
    "\n",
    "- ROUGE-N quantifies the overlap of N-grams, contiguous sequences of N items (typically words or characters), between the system-generated summary and the reference summary) \n",
    "    - ROUGE-1 (unigram overlap): This metric measures the overlap of unigrams (single words) between the generated summary and the reference summary. It focuses on the recall of unigrams.\n",
    "    - ROUGE-2 (bigram overlap): Similar to ROUGE-1, but it measures the overlap of bigrams (pairs of consecutive words) instead of unigrams. It evaluates the recall of bigrams.\n",
    "- ROUGE-L (longest common subsequence): Instead of measuring word overlap, ROUGE-L focuses on the longest common subsequence (LCS) between the generated and reference summaries. It evaluates the recall of the longest common subsequence\n",
    "\n",
    "- high precision value suggests that the words or phrases churned out by the machine translation or submodel are primarily accurate\n",
    "- high recall value, ideally close to 1, implies that the content of the machine-generated output aligns closely with the human-made reference. It signifies the model's proficiency in capturing relevant information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general prompt to test llm for testing questions\n",
    "SYSTEM_PROMPT_EVAL_ROUGE = \"\"\"\n",
    "<<SYS>>\n",
    "You will be presented with a question where you are expected to respond only with the correct answer in a short and simple manner. Use one sentence maximum to answer the question. \n",
    "Only respond with the correct answer itself and leave out fill words like \"sure\" or \"certainly\".\n",
    "If you don't know the answer, be honest and state that you don't know. Don't give any false information.\n",
    "<</SYS>>\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + SYSTEM_PROMPT_EVAL_ROUGE + E_SYS\n",
    "\n",
    "##CURRENT OUTPUT (extraction of language style)\n",
    "#Average Recall: 0.871\n",
    "#Average Precision: 0.871\n",
    "#Average F1-score: 0.871\n",
    "\n",
    "##CURRENT OUTPUT (without extraction of language style)\n",
    "#Average Recall: 0.112\n",
    "#Average Precision: 0.843\n",
    "#Average F1-score: 0.177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt used in llm_service GRAMMAR AGENT (but without language style defined)\n",
    "SYSTEM_PROMPT_EVAL_ROUGE_USECASE_GRAMMAR = \"\"\"\n",
    "<<SYS>>\n",
    "You are a friendly language teacher for the language english. You nicely correct and analyse the grammar and semantical mistakes of the sentences provided by the student. You do not halucinate or interpret the text of the user. If a message does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer, please don't share false information. If words in the provided message are grammatically correct still analyse the semantic meaning of the sentence and correct it if needed. Think about the correction and analysis step by step.\n",
    "<</SYS>>\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + SYSTEM_PROMPT_EVAL_ROUGE_USECASE_GRAMMAR + E_SYS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt used in llm_service CONVERSATION AGENT (but without language style defined)\n",
    "SYSTEM_PROMPT_EVAL_ROUGE_USECASE_CONVO = \"\"\"\n",
    "<<SYS>>\n",
    "The following is a friendly conversation between a human and an AI. The AI answers precisely only talks in english. No word is other than english. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "<</SYS>>\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + SYSTEM_PROMPT_EVAL_ROUGE_USECASE_CONVO + E_SYS\n",
    "\n",
    "##CURRENT OUTPUT (extraction of language style)\n",
    "#Average Recall: 0.\n",
    "#Average Precision: 0.\n",
    "#Average F1-score: 0.\n",
    "\n",
    "##CURRENT OUTPUT (without extraction of language style)\n",
    "#Average Recall: 0.\n",
    "#Average Precision: 0.\n",
    "#Average F1-score: 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test questions with different categories for ROUGE\n",
    "textual_entailment = {\n",
    "    \"If 'A implies B' and 'B is false,' what can you conclude about A?\":\"Nothing definitive can be concluded about A.\",\n",
    "}\n",
    "\n",
    "reasoning_problem_solving = {\n",
    "    \"If a car travels at 60 miles per hour, how long will it take to cover 120 miles?\": \"2 hours\",\n",
    "}\n",
    "\n",
    "analogical_reasoning = {\n",
    "    \"If 'cat' is to 'kitten,' what is 'dog' to?\":\"'Dog' is to 'puppy'\",\n",
    "}\n",
    "\n",
    "ner = {\n",
    "    \"Identify the named person in the sentence 'Elon Musk is the CEO of SpaceX.'\": \"Elon Musk\",\n",
    "    \"Extract the date mentioned in the text 'The conference is scheduled for January 25, 2023.'\": \"January 25, 2023.\",\n",
    "}\n",
    "\n",
    "identification ={\n",
    "    \"How many objects are named in the following sequence: The dog was running towards the tree with a ball in his mouth.\": \"One object is named, a dog.\",\n",
    "    \"Given the statement, answer the following question. Statement: The sky is blue and the building is grey. Question: What color does the sky have?\": \"Blue\"\n",
    "}\n",
    "\n",
    "languageStyles = {\n",
    "\n",
    "  # Informal Style\n",
    "  \"What language style is used in the following Statement: Can you believe this weather? Do you think it'll rain later?\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: Hey, wanna grab a bite to eat later?\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: What's up with that new movie everyone's talking about? Have you seen the trailer?\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: Do you know if Sarah's coming to the party tonight? I heard she might not make it.\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: So, what's the deal with that new song everyone's listing to?\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: Did you catch the game last night?\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: Are you free tomorrow afternoon? Wanna hang out and grab some coffee?\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: What do you reckon we should do this weekend? Any fun ideas?\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: Did you see that meme I sent you earlier? It had me cracking up!\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: I donʼt believe that the results are accurate.\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: Hi, Helen! How’s it going?\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: Thanks. See you later.\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: No, I wasn’t at Steve’s party.\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: Sure, let’s go get a drink.\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: What are you up to this evening?\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: Oh, that’s a shame.\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: That didn’t make sense to me.\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: Seriously, you’ve got to be kidding me!?\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: I donʼt believe that the results are accurate.\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: He’ll have to do another five tests before he can stop the experiment.\" : \"Informal\",\n",
    "  \"What language style is described by the following statement: The style is more casual and spontaneous and is used to communicate with friends and family either in writing or conversations.\" : \"Informal\",\n",
    "    \"What language style is used in the following e-mail: Hi Beth, Just a quick message to thank you for dinner last night. Absolutely delicious, as always, and I really enjoyed meeting your friend Alice. \\\n",
    "    She’s a laugh, isn’t she? I’m hoping to3 get tickets for the film festival next week, so I’ll be in touch to see if there’s anything you fancy seeing. Give my regards to Conrad when he gets back from Poland, and once again, thanks for last night. Love Sophie\" : \"Informal\",\n",
    "\n",
    "  # Formal Style\n",
    "  \"What language style is used in the following Statement: Improvements cannot be introduced due to funding restrictions.\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: Could you please provide an update on the progress of the project by the end of the day?\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: I would like to inquire about the availability of the conference room for our meeting next week.\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: Per our previous discussion, I am following up on the status of the pending tasks.\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: Kindly confirm the receipt of this email and let me know if any further action is required.\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: In accordance with company policy, employees are required to complete the mandatory training module.\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: Please be advised that the deadline for submitting the report has been extended to next Friday.\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: I am writing to formally request an extension on the deadline for the project proposal.\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: Per your request, attached is the revised version of the contract for your review.\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: I apologize for any inconvenience caused and appreciate your understanding in this matter.\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: Good morning, Mrs. Smith, how are you doing?\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: Only food purchased here may be eaten on the premises.\" : \"Formal\",\n",
    "  \"What language style is described by the following statement: The style is used when writing or speaking for professional purpose and does not use colloquialism, contractions or first-person pronouns.\" : \"Formal\",\n",
    "  \"What language style is used in the following e-mail: Dear Sir or Madam, I am writing in response to3 your advertisement about job opportunities for graduates. I have just completed a degree in Economics at Durham University, and \\\n",
    "    I would be grateful if you could send me further details6 of the graduate training schemes you mention. I am available for interview at any time. I look forward to hearing from you. Yours faithfully\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: Please respect our no radio playing policy.\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: This could be an effective plan.\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: There were two different methods of research.\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: The project will be completed next year.\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: Five more tests will be necessary before the experiment can be concluded.\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: It is possible to consider the results from a different viewpoint.\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: It has been proved that the arguments so far are without foundation.\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: There are a several reasons why the questionnaire should be revised.\" : \"Formal\",\n",
    "  \"What language style is used in the following Statement: I look forward to meeting you next week.\" : \"Formal\",\n",
    "\n",
    "  # academic Style\n",
    "  \"What language style is described by the following statement: The style is used for higher school purposes like thesis or research papers and does not use first-person pronouns and is not personal but factual\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: New results in this area are produced by the research group.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: How do socioeconomic factors influence voting behavior in democratic societies?\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: What is the significance of the findings reported in the latest research paper on climate change?\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: How do cognitive biases affect decision-making processes in individuals and organizations?\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: The study investigates the correlation between socioeconomic status and educational attainment in urban communities.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: The analysis explores the impact of climate change on agricultural productivity in developing countries.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: The research paper examines the role of cultural factors in shaping consumer behavior in global markets.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: The literature review synthesizes existing studies on the effects of social media use on mental health outcomes.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: The theoretical framework draws upon sociological theories to analyze patterns of social inequality in urban neighborhoods.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: The analysis examines demographic trends in urbanization.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: The findings highlight disparities in access to healthcare services.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: The literature review synthesizes recent advancements in machine learning algorithms.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: The analysis underscores the importance of data privacy in digital healthcare systems.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: These findings indicate that the model is valid.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: The following section will show how the model is validated by the findings.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: As Halliday(1973) shows, language is intrinsically social.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: Many researchers have utilised X to measure Y.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: A number of techniques have been developed to...\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: Different authors have measured X in a variety of ways.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: Several methods currently exist for the measurement of X.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: There are three main types of study design used to identify X.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: In most recent studies, X has been measured in four different ways.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: In recent years, two different approaches have attempted to account for X.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: A variety of methods are used to assess X. Each has its advantages and drawbacks.\" : \"Academic\",\n",
    "  \"What language style is used in the following Statement: Two of the most common methods for estimating X are the use of Y and the measurement of Z.\" : \"Academic\",\n",
    "}\n",
    "\n",
    "\n",
    "# Combine all test questions into a single dictionary\n",
    "#questions_rouge = {**textual_entailment,**reasoning_problem_solving,**analogical_reasoning,**ner,**identification,**languageStyles}\n",
    "questions_rouge = {**languageStyles}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-1': {'r': 0.4489795918367347, 'p': 0.6285714285714286, 'f': 0.5238095189484127}, 'rouge-2': {'r': 0.21428571428571427, 'p': 0.375, 'f': 0.2727272680991736}, 'rouge-l': {'r': 0.40816326530612246, 'p': 0.5714285714285714, 'f': 0.4761904713293651}}]\n"
     ]
    }
   ],
   "source": [
    "#%pip install rouge ##- test rouge library - https://pypi.org/project/rouge/ \n",
    "from rouge import Rouge\n",
    "\n",
    "##example calc for rouge\n",
    "hypothesis = \"the #### transcript is a written version of each day 's cnn student news program use this transcript to help students with reading comprehension and vocabulary use the weekly newsquiz to test your knowledge of storie s you     saw on cnn student news\"\n",
    "\n",
    "reference = \"this page includes the show transcript use the transcript to help students with reading comprehension and vocabulary at the bottom of the page , comment for a chance to be mentioned on cnn student news . you must be a teac    her or a student age # # or older to request a mention on the cnn student news roll call . the weekly newsquiz tests     students ' knowledge of even ts in the news\"\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function that extracts only the language style named \n",
    "def extract_language_style(input_string):\n",
    "\n",
    "    # Check if the input string is empty\n",
    "    if not input_string:\n",
    "        return \"\"\n",
    "    \n",
    "    words = input_string.split()\n",
    "    words = [word.strip(\",.!?\").lower() for word in words]\n",
    "    allowed_words = {\"informal\", \"formal\", \"academic\"}\n",
    "\n",
    "    for word in words:\n",
    "        if word in allowed_words:\n",
    "            return word\n",
    "    return \"False\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: Can you believe this weather? Do you think it'll rain later?\n",
      "Expected Answer: informal\n",
      "Model Answer: informal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       9.00 ms /    29 runs   (    0.31 ms per token,  3223.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27028.13 ms /   126 tokens (  214.51 ms per token,     4.66 tokens per second)\n",
      "llama_print_timings:        eval time =    4252.14 ms /    28 runs   (  151.86 ms per token,     6.58 tokens per second)\n",
      "llama_print_timings:       total time =   31444.68 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: Hey, wanna grab a bite to eat later?\n",
      "Expected Answer: informal\n",
      "Model Answer: informal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       8.33 ms /    27 runs   (    0.31 ms per token,  3239.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2586.55 ms /    21 tokens (  123.17 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:        eval time =    3817.01 ms /    26 runs   (  146.81 ms per token,     6.81 tokens per second)\n",
      "llama_print_timings:       total time =    6504.99 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: What's up with that new movie everyone's talking about? Have you seen the trailer?\n",
      "Expected Answer: informal\n",
      "Model Answer: informal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      15.60 ms /    48 runs   (    0.32 ms per token,  3077.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3724.18 ms /    30 tokens (  124.14 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:        eval time =   10112.91 ms /    47 runs   (  215.17 ms per token,     4.65 tokens per second)\n",
      "llama_print_timings:       total time =   14048.67 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: Do you know if Sarah's coming to the party tonight? I heard she might not make it.\n",
      "Expected Answer: informal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      13.13 ms /    48 runs   (    0.27 ms per token,  3655.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3389.48 ms /    31 tokens (  109.34 ms per token,     9.15 tokens per second)\n",
      "llama_print_timings:        eval time =    9093.31 ms /    47 runs   (  193.47 ms per token,     5.17 tokens per second)\n",
      "llama_print_timings:       total time =   12671.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       9.38 ms /    31 runs   (    0.30 ms per token,  3304.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3012.75 ms /    26 tokens (  115.88 ms per token,     8.63 tokens per second)\n",
      "llama_print_timings:        eval time =    4512.89 ms /    30 runs   (  150.43 ms per token,     6.65 tokens per second)\n",
      "llama_print_timings:       total time =    7634.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: So, what's the deal with that new song everyone's listing to?\n",
      "Expected Answer: informal\n",
      "Model Answer: informal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n",
      "Question: What language style is used in the following Statement: Did you catch the game last night?\n",
      "Expected Answer: informal\n",
      "Model Answer: informal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       6.70 ms /    22 runs   (    0.30 ms per token,  3285.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1776.82 ms /    16 tokens (  111.05 ms per token,     9.00 tokens per second)\n",
      "llama_print_timings:        eval time =    3459.64 ms /    22 runs   (  157.26 ms per token,     6.36 tokens per second)\n",
      "llama_print_timings:       total time =    5314.96 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: Are you free tomorrow afternoon? Wanna hang out and grab some coffee?\n",
      "Expected Answer: informal\n",
      "Model Answer: informal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      13.61 ms /    48 runs   (    0.28 ms per token,  3526.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3407.74 ms /    24 tokens (  141.99 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:        eval time =    7248.55 ms /    48 runs   (  151.01 ms per token,     6.62 tokens per second)\n",
      "llama_print_timings:       total time =   10831.59 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: What do you reckon we should do this weekend? Any fun ideas?\n",
      "Expected Answer: informal\n",
      "Model Answer: informal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      14.01 ms /    48 runs   (    0.29 ms per token,  3427.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2634.38 ms /    24 tokens (  109.77 ms per token,     9.11 tokens per second)\n",
      "llama_print_timings:        eval time =    7246.11 ms /    48 runs   (  150.96 ms per token,     6.62 tokens per second)\n",
      "llama_print_timings:       total time =   10054.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: Did you see that meme I sent you earlier? It had me cracking up!\n",
      "Expected Answer: informal\n",
      "Model Answer: informal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      14.47 ms /    48 runs   (    0.30 ms per token,  3316.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3008.47 ms /    27 tokens (  111.42 ms per token,     8.97 tokens per second)\n",
      "llama_print_timings:        eval time =    6974.70 ms /    47 runs   (  148.40 ms per token,     6.74 tokens per second)\n",
      "llama_print_timings:       total time =   10155.52 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: I donʼt believe that the results are accurate.\n",
      "Expected Answer: informal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       7.96 ms /    26 runs   (    0.31 ms per token,  3264.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2253.00 ms /    20 tokens (  112.65 ms per token,     8.88 tokens per second)\n",
      "llama_print_timings:        eval time =    4258.59 ms /    25 runs   (  170.34 ms per token,     5.87 tokens per second)\n",
      "llama_print_timings:       total time =    6605.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: Hi, Helen! How’s it going?\n",
      "Expected Answer: informal\n",
      "Model Answer: informal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      19.01 ms /    24 runs   (    0.79 ms per token,  1262.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2646.88 ms /    19 tokens (  139.31 ms per token,     7.18 tokens per second)\n",
      "llama_print_timings:        eval time =    3980.20 ms /    23 runs   (  173.05 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =    6739.10 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: Thanks. See you later.\n",
      "Expected Answer: informal\n",
      "Model Answer: informal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       5.81 ms /    20 runs   (    0.29 ms per token,  3441.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1987.39 ms /    15 tokens (  132.49 ms per token,     7.55 tokens per second)\n",
      "llama_print_timings:        eval time =    3285.69 ms /    19 runs   (  172.93 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =    5343.34 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: No, I wasn’t at Steve’s party.\n",
      "Expected Answer: informal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       8.05 ms /    27 runs   (    0.30 ms per token,  3354.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2513.11 ms /    21 tokens (  119.67 ms per token,     8.36 tokens per second)\n",
      "llama_print_timings:        eval time =    4361.90 ms /    26 runs   (  167.77 ms per token,     5.96 tokens per second)\n",
      "llama_print_timings:       total time =    6975.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: Sure, let’s go get a drink.\n",
      "Expected Answer: informal\n",
      "Model Answer: informal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       7.92 ms /    25 runs   (    0.32 ms per token,  3154.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2407.26 ms /    19 tokens (  126.70 ms per token,     7.89 tokens per second)\n",
      "llama_print_timings:        eval time =    5276.28 ms /    24 runs   (  219.84 ms per token,     4.55 tokens per second)\n",
      "llama_print_timings:       total time =    7836.82 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: What are you up to this evening?\n",
      "Expected Answer: informal\n",
      "Model Answer: informal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       7.60 ms /    22 runs   (    0.35 ms per token,  2893.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1913.91 ms /    16 tokens (  119.62 ms per token,     8.36 tokens per second)\n",
      "llama_print_timings:        eval time =    4182.93 ms /    22 runs   (  190.13 ms per token,     5.26 tokens per second)\n",
      "llama_print_timings:       total time =    6185.77 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: Oh, that’s a shame.\n",
      "Expected Answer: informal\n",
      "Model Answer: informal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       7.03 ms /    22 runs   (    0.32 ms per token,  3129.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2186.24 ms /    16 tokens (  136.64 ms per token,     7.32 tokens per second)\n",
      "llama_print_timings:        eval time =    3907.38 ms /    22 runs   (  177.61 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:       total time =    6182.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: That didn’t make sense to me.\n",
      "Expected Answer: informal\n",
      "Model Answer: informal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      14.77 ms /    48 runs   (    0.31 ms per token,  3249.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2066.72 ms /    18 tokens (  114.82 ms per token,     8.71 tokens per second)\n",
      "llama_print_timings:        eval time =    8043.30 ms /    47 runs   (  171.13 ms per token,     5.84 tokens per second)\n",
      "llama_print_timings:       total time =   10293.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      13.54 ms /    48 runs   (    0.28 ms per token,  3544.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3550.34 ms /    24 tokens (  147.93 ms per token,     6.76 tokens per second)\n",
      "llama_print_timings:        eval time =    7460.36 ms /    47 runs   (  158.73 ms per token,     6.30 tokens per second)\n",
      "llama_print_timings:       total time =   11187.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: Seriously, you’ve got to be kidding me!?\n",
      "Expected Answer: informal\n",
      "Model Answer: informal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n",
      "Question: What language style is used in the following Statement: He’ll have to do another five tests before he can stop the experiment.\n",
      "Expected Answer: informal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       9.15 ms /    31 runs   (    0.30 ms per token,  3388.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2750.47 ms /    24 tokens (  114.60 ms per token,     8.73 tokens per second)\n",
      "llama_print_timings:        eval time =    5188.72 ms /    31 runs   (  167.38 ms per token,     5.97 tokens per second)\n",
      "llama_print_timings:       total time =    8057.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      11.93 ms /    48 runs   (    0.25 ms per token,  4022.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4467.18 ms /    40 tokens (  111.68 ms per token,     8.95 tokens per second)\n",
      "llama_print_timings:        eval time =    7804.33 ms /    48 runs   (  162.59 ms per token,     6.15 tokens per second)\n",
      "llama_print_timings:       total time =   12455.22 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is described by the following statement: The style is more casual and spontaneous and is used to communicate with friends and family either in writing or conversations.\n",
      "Expected Answer: informal\n",
      "Model Answer: informal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n",
      "Question: What language style is used in the following e-mail: Hi Beth, Just a quick message to thank you for dinner last night. Absolutely delicious, as always, and I really enjoyed meeting your friend Alice.     She’s a laugh, isn’t she? I’m hoping to3 get tickets for the film festival next week, so I’ll be in touch to see if there’s anything you fancy seeing. Give my regards to Conrad when he gets back from Poland, and once again, thanks for last night. Love Sophie\n",
      "Expected Answer: informal\n",
      "Model Answer: informal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      12.05 ms /    48 runs   (    0.25 ms per token,  3983.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13582.81 ms /   120 tokens (  113.19 ms per token,     8.83 tokens per second)\n",
      "llama_print_timings:        eval time =    7662.92 ms /    48 runs   (  159.64 ms per token,     6.26 tokens per second)\n",
      "llama_print_timings:       total time =   21454.02 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: Improvements cannot be introduced due to funding restrictions.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       7.93 ms /    27 runs   (    0.29 ms per token,  3404.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2678.18 ms /    24 tokens (  111.59 ms per token,     8.96 tokens per second)\n",
      "llama_print_timings:        eval time =    3994.54 ms /    26 runs   (  153.64 ms per token,     6.51 tokens per second)\n",
      "llama_print_timings:       total time =    6770.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: Could you please provide an update on the progress of the project by the end of the day?\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      10.99 ms /    34 runs   (    0.32 ms per token,  3093.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3609.81 ms /    28 tokens (  128.92 ms per token,     7.76 tokens per second)\n",
      "llama_print_timings:        eval time =    9082.68 ms /    33 runs   (  275.23 ms per token,     3.63 tokens per second)\n",
      "llama_print_timings:       total time =   12843.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      13.75 ms /    48 runs   (    0.29 ms per token,  3489.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4966.71 ms /    29 tokens (  171.27 ms per token,     5.84 tokens per second)\n",
      "llama_print_timings:        eval time =    9784.92 ms /    47 runs   (  208.19 ms per token,     4.80 tokens per second)\n",
      "llama_print_timings:       total time =   14961.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: I would like to inquire about the availability of the conference room for our meeting next week.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n",
      "Question: What language style is used in the following Statement: Per our previous discussion, I am following up on the status of the pending tasks.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      14.65 ms /    48 runs   (    0.31 ms per token,  3276.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5020.83 ms /    26 tokens (  193.11 ms per token,     5.18 tokens per second)\n",
      "llama_print_timings:        eval time =    9932.20 ms /    47 runs   (  211.32 ms per token,     4.73 tokens per second)\n",
      "llama_print_timings:       total time =   15143.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: Kindly confirm the receipt of this email and let me know if any further action is required.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      13.33 ms /    48 runs   (    0.28 ms per token,  3602.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4597.45 ms /    29 tokens (  158.53 ms per token,     6.31 tokens per second)\n",
      "llama_print_timings:        eval time =    8704.07 ms /    47 runs   (  185.19 ms per token,     5.40 tokens per second)\n",
      "llama_print_timings:       total time =   13497.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: In accordance with company policy, employees are required to complete the mandatory training module.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      13.07 ms /    48 runs   (    0.27 ms per token,  3673.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4120.70 ms /    27 tokens (  152.62 ms per token,     6.55 tokens per second)\n",
      "llama_print_timings:        eval time =    9849.77 ms /    47 runs   (  209.57 ms per token,     4.77 tokens per second)\n",
      "llama_print_timings:       total time =   14171.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: Please be advised that the deadline for submitting the report has been extended to next Friday.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      10.70 ms /    35 runs   (    0.31 ms per token,  3270.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4744.30 ms /    29 tokens (  163.60 ms per token,     6.11 tokens per second)\n",
      "llama_print_timings:        eval time =    8593.99 ms /    34 runs   (  252.76 ms per token,     3.96 tokens per second)\n",
      "llama_print_timings:       total time =   13489.45 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: I am writing to formally request an extension on the deadline for the project proposal.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      12.86 ms /    48 runs   (    0.27 ms per token,  3733.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3717.16 ms /    26 tokens (  142.97 ms per token,     6.99 tokens per second)\n",
      "llama_print_timings:        eval time =    7644.14 ms /    47 runs   (  162.64 ms per token,     6.15 tokens per second)\n",
      "llama_print_timings:       total time =   11539.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      15.25 ms /    48 runs   (    0.32 ms per token,  3147.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3576.54 ms /    26 tokens (  137.56 ms per token,     7.27 tokens per second)\n",
      "llama_print_timings:        eval time =   10434.92 ms /    47 runs   (  222.02 ms per token,     4.50 tokens per second)\n",
      "llama_print_timings:       total time =   14223.90 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: Per your request, attached is the revised version of the contract for your review.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      11.03 ms /    32 runs   (    0.34 ms per token,  2902.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3895.78 ms /    26 tokens (  149.84 ms per token,     6.67 tokens per second)\n",
      "llama_print_timings:        eval time =    7034.60 ms /    31 runs   (  226.92 ms per token,     4.41 tokens per second)\n",
      "llama_print_timings:       total time =   11076.04 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: I apologize for any inconvenience caused and appreciate your understanding in this matter.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n",
      "Question: What language style is used in the following Statement: Good morning, Mrs. Smith, how are you doing?\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       8.18 ms /    27 runs   (    0.30 ms per token,  3301.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3533.18 ms /    21 tokens (  168.25 ms per token,     5.94 tokens per second)\n",
      "llama_print_timings:        eval time =    5750.89 ms /    26 runs   (  221.19 ms per token,     4.52 tokens per second)\n",
      "llama_print_timings:       total time =    9410.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: Only food purchased here may be eaten on the premises.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       8.98 ms /    28 runs   (    0.32 ms per token,  3117.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3379.78 ms /    22 tokens (  153.63 ms per token,     6.51 tokens per second)\n",
      "llama_print_timings:        eval time =    5097.14 ms /    27 runs   (  188.78 ms per token,     5.30 tokens per second)\n",
      "llama_print_timings:       total time =    8598.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is described by the following statement: The style is used when writing or speaking for professional purpose and does not use colloquialism, contractions or first-person pronouns.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       4.95 ms /    19 runs   (    0.26 ms per token,  3839.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7856.45 ms /    46 tokens (  170.79 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:        eval time =    3667.67 ms /    18 runs   (  203.76 ms per token,     4.91 tokens per second)\n",
      "llama_print_timings:       total time =   11616.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following e-mail: Dear Sir or Madam, I am writing in response to3 your advertisement about job opportunities for graduates. I have just completed a degree in Economics at Durham University, and     I would be grateful if you could send me further details6 of the graduate training schemes you mention. I am available for interview at any time. I look forward to hearing from you. Yours faithfully\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      13.01 ms /    48 runs   (    0.27 ms per token,  3690.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17047.54 ms /   101 tokens (  168.79 ms per token,     5.92 tokens per second)\n",
      "llama_print_timings:        eval time =    7400.99 ms /    47 runs   (  157.47 ms per token,     6.35 tokens per second)\n",
      "llama_print_timings:       total time =   24655.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: Please respect our no radio playing policy.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       6.62 ms /    23 runs   (    0.29 ms per token,  3476.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2407.81 ms /    20 tokens (  120.39 ms per token,     8.31 tokens per second)\n",
      "llama_print_timings:        eval time =    3371.36 ms /    22 runs   (  153.24 ms per token,     6.53 tokens per second)\n",
      "llama_print_timings:       total time =    5860.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: This could be an effective plan.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       6.40 ms /    22 runs   (    0.29 ms per token,  3438.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1913.71 ms /    16 tokens (  119.61 ms per token,     8.36 tokens per second)\n",
      "llama_print_timings:        eval time =    3099.15 ms /    21 runs   (  147.58 ms per token,     6.78 tokens per second)\n",
      "llama_print_timings:       total time =    5088.87 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: There were two different methods of research.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       6.61 ms /    23 runs   (    0.29 ms per token,  3481.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1858.62 ms /    16 tokens (  116.16 ms per token,     8.61 tokens per second)\n",
      "llama_print_timings:        eval time =    3433.39 ms /    23 runs   (  149.28 ms per token,     6.70 tokens per second)\n",
      "llama_print_timings:       total time =    5381.74 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       6.80 ms /    23 runs   (    0.30 ms per token,  3383.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1779.19 ms /    16 tokens (  111.20 ms per token,     8.99 tokens per second)\n",
      "llama_print_timings:        eval time =    3493.87 ms /    23 runs   (  151.91 ms per token,     6.58 tokens per second)\n",
      "llama_print_timings:       total time =    5357.83 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: The project will be completed next year.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n",
      "Question: What language style is used in the following Statement: Five more tests will be necessary before the experiment can be concluded.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       8.36 ms /    29 runs   (    0.29 ms per token,  3470.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2456.66 ms /    22 tokens (  111.67 ms per token,     8.96 tokens per second)\n",
      "llama_print_timings:        eval time =    4174.62 ms /    28 runs   (  149.09 ms per token,     6.71 tokens per second)\n",
      "llama_print_timings:       total time =    6735.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: It is possible to consider the results from a different viewpoint.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       8.11 ms /    28 runs   (    0.29 ms per token,  3451.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2418.89 ms /    22 tokens (  109.95 ms per token,     9.10 tokens per second)\n",
      "llama_print_timings:        eval time =    3993.81 ms /    27 runs   (  147.92 ms per token,     6.76 tokens per second)\n",
      "llama_print_timings:       total time =    6510.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: It has been proved that the arguments so far are without foundation.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       8.07 ms /    28 runs   (    0.29 ms per token,  3468.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2348.81 ms /    21 tokens (  111.85 ms per token,     8.94 tokens per second)\n",
      "llama_print_timings:        eval time =    4074.44 ms /    27 runs   (  150.91 ms per token,     6.63 tokens per second)\n",
      "llama_print_timings:       total time =    6525.54 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: There are a several reasons why the questionnaire should be revised.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       8.83 ms /    29 runs   (    0.30 ms per token,  3282.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2716.58 ms /    23 tokens (  118.11 ms per token,     8.47 tokens per second)\n",
      "llama_print_timings:        eval time =    4161.69 ms /    28 runs   (  148.63 ms per token,     6.73 tokens per second)\n",
      "llama_print_timings:       total time =    6981.85 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: I look forward to meeting you next week.\n",
      "Expected Answer: formal\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       6.92 ms /    24 runs   (    0.29 ms per token,  3466.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2042.87 ms /    18 tokens (  113.49 ms per token,     8.81 tokens per second)\n",
      "llama_print_timings:        eval time =    3499.70 ms /    23 runs   (  152.16 ms per token,     6.57 tokens per second)\n",
      "llama_print_timings:       total time =    5629.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is described by the following statement: The style is used for higher school purposes like thesis or research papers and does not use first-person pronouns and is not personal but factual\n",
      "Expected Answer: academic\n",
      "Model Answer: academic\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      11.48 ms /    48 runs   (    0.24 ms per token,  4181.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5053.21 ms /    46 tokens (  109.85 ms per token,     9.10 tokens per second)\n",
      "llama_print_timings:        eval time =    6987.50 ms /    47 runs   (  148.67 ms per token,     6.73 tokens per second)\n",
      "llama_print_timings:       total time =   12218.93 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: New results in this area are produced by the research group.\n",
      "Expected Answer: academic\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       7.74 ms /    27 runs   (    0.29 ms per token,  3486.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3210.69 ms /    28 tokens (  114.67 ms per token,     8.72 tokens per second)\n",
      "llama_print_timings:        eval time =    4199.59 ms /    26 runs   (  161.52 ms per token,     6.19 tokens per second)\n",
      "llama_print_timings:       total time =    7513.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: How do socioeconomic factors influence voting behavior in democratic societies?\n",
      "Expected Answer: academic\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       9.78 ms /    33 runs   (    0.30 ms per token,  3374.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3033.34 ms /    27 tokens (  112.35 ms per token,     8.90 tokens per second)\n",
      "llama_print_timings:        eval time =    4902.09 ms /    32 runs   (  153.19 ms per token,     6.53 tokens per second)\n",
      "llama_print_timings:       total time =    8058.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: What is the significance of the findings reported in the latest research paper on climate change?\n",
      "Expected Answer: academic\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       6.12 ms /    21 runs   (    0.29 ms per token,  3430.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3171.95 ms /    27 tokens (  117.48 ms per token,     8.51 tokens per second)\n",
      "llama_print_timings:        eval time =    2917.02 ms /    20 runs   (  145.85 ms per token,     6.86 tokens per second)\n",
      "llama_print_timings:       total time =    6170.90 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: How do cognitive biases affect decision-making processes in individuals and organizations?\n",
      "Expected Answer: academic\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      12.20 ms /    48 runs   (    0.25 ms per token,  3933.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2652.09 ms /    24 tokens (  110.50 ms per token,     9.05 tokens per second)\n",
      "llama_print_timings:        eval time =    7231.78 ms /    48 runs   (  150.66 ms per token,     6.64 tokens per second)\n",
      "llama_print_timings:       total time =   10058.63 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: The study investigates the correlation between socioeconomic status and educational attainment in urban communities.\n",
      "Expected Answer: academic\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      12.72 ms /    48 runs   (    0.27 ms per token,  3773.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3440.41 ms /    31 tokens (  110.98 ms per token,     9.01 tokens per second)\n",
      "llama_print_timings:        eval time =    8058.45 ms /    47 runs   (  171.46 ms per token,     5.83 tokens per second)\n",
      "llama_print_timings:       total time =   11682.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: The analysis explores the impact of climate change on agricultural productivity in developing countries.\n",
      "Expected Answer: academic\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      12.79 ms /    48 runs   (    0.27 ms per token,  3752.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3007.04 ms /    26 tokens (  115.66 ms per token,     8.65 tokens per second)\n",
      "llama_print_timings:        eval time =    7181.14 ms /    47 runs   (  152.79 ms per token,     6.54 tokens per second)\n",
      "llama_print_timings:       total time =   10363.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      13.90 ms /    48 runs   (    0.29 ms per token,  3453.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3724.29 ms /    28 tokens (  133.01 ms per token,     7.52 tokens per second)\n",
      "llama_print_timings:        eval time =   10193.74 ms /    47 runs   (  216.89 ms per token,     4.61 tokens per second)\n",
      "llama_print_timings:       total time =   14165.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: The research paper examines the role of cultural factors in shaping consumer behavior in global markets.\n",
      "Expected Answer: academic\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n",
      "Question: What language style is used in the following Statement: The literature review synthesizes existing studies on the effects of social media use on mental health outcomes.\n",
      "Expected Answer: academic\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      10.03 ms /    35 runs   (    0.29 ms per token,  3488.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5403.48 ms /    29 tokens (  186.33 ms per token,     5.37 tokens per second)\n",
      "llama_print_timings:        eval time =    7364.95 ms /    34 runs   (  216.62 ms per token,     4.62 tokens per second)\n",
      "llama_print_timings:       total time =   12916.25 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: The theoretical framework draws upon sociological theories to analyze patterns of social inequality in urban neighborhoods.\n",
      "Expected Answer: academic\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      14.18 ms /    48 runs   (    0.30 ms per token,  3385.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4234.72 ms /    28 tokens (  151.24 ms per token,     6.61 tokens per second)\n",
      "llama_print_timings:        eval time =   11044.03 ms /    47 runs   (  234.98 ms per token,     4.26 tokens per second)\n",
      "llama_print_timings:       total time =   15516.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       8.77 ms /    27 runs   (    0.32 ms per token,  3078.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2333.04 ms /    20 tokens (  116.65 ms per token,     8.57 tokens per second)\n",
      "llama_print_timings:        eval time =    4644.18 ms /    26 runs   (  178.62 ms per token,     5.60 tokens per second)\n",
      "llama_print_timings:       total time =    7090.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: The analysis examines demographic trends in urbanization.\n",
      "Expected Answer: academic\n",
      "Model Answer: formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m#run method\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m model_answers, rouge_scores \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_rouge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestions_rouge\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m, in \u001b[0;36mevaluate_model_rouge\u001b[0;34m(model, questions)\u001b[0m\n\u001b[1;32m     15\u001b[0m prompt \u001b[38;5;241m=\u001b[39m get_prompt(prompt)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#print(prompt)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m model_answer \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#remove leading & trailing spaces, extract language style from model_answer if answer is unnecessarily long\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model_answer \u001b[38;5;241m=\u001b[39m model_answer\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/langchain/llms/base.py:870\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    868\u001b[0m     )\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 870\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    880\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/langchain/llms/base.py:650\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    636\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    637\u001b[0m         )\n\u001b[1;32m    638\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    639\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    640\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    648\u001b[0m         )\n\u001b[1;32m    649\u001b[0m     ]\n\u001b[0;32m--> 650\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/langchain/llms/base.py:538\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    537\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    539\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/langchain/llms/base.py:525\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    517\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    522\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 525\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    529\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    533\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    534\u001b[0m         )\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/langchain/llms/base.py:1047\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1046\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1049\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1050\u001b[0m     )\n\u001b[1;32m   1051\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/langchain/llms/llamacpp.py:291\u001b[0m, in \u001b[0;36mLlamaCpp._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# If streaming is enabled, we use the stream\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# method that yields as they are generated\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# and return the combined strings from the first choices's text:\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     combined_text_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[1;32m    292\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m    293\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    294\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    296\u001b[0m     ):\n\u001b[1;32m    297\u001b[0m         combined_text_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_text_output\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/langchain/llms/llamacpp.py:344\u001b[0m, in \u001b[0;36mLlamaCpp._stream\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_parameters(stop), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    343\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(prompt\u001b[38;5;241m=\u001b[39mprompt, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m    345\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m part[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    346\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m GenerationChunk(\n\u001b[1;32m    347\u001b[0m         text\u001b[38;5;241m=\u001b[39mpart[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    348\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs},\n\u001b[1;32m    349\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/llama_cpp/llama.py:1395\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001b[0m\n\u001b[1;32m   1393\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1394\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   1396\u001b[0m     prompt_tokens,\n\u001b[1;32m   1397\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1398\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   1399\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m   1400\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m   1401\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1402\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1403\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1404\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1405\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1406\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1407\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1408\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m   1409\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1410\u001b[0m ):\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_eos:\n\u001b[1;32m   1412\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/llama_cpp/llama.py:1199\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     grammar\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1200\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m   1201\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1202\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1212\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1213\u001b[0m     )\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stopping_criteria \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m stopping_criteria(\n\u001b[1;32m   1215\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m   1216\u001b[0m     ):\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/llama_cpp/llama.py:1030\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m   1026\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m   1028\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m   1029\u001b[0m )\n\u001b[0;32m-> 1030\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/llama_cpp/llama.py:466\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/llama_cpp/llama_cpp.py:1136\u001b[0m, in \u001b[0;36mllama_decode\u001b[0;34m(ctx, batch)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllama_decode\u001b[39m(ctx: llama_context_p, batch: llama_batch) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m-> 1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#maybe check if rouge-score library better\n",
    "\n",
    "#ROUGE WITH extraction of language style\n",
    "def evaluate_model_rouge(model, questions):\n",
    "    try:\n",
    "        total_questions = len(questions)\n",
    "        model_answers = []\n",
    "        rouge_scores_list_r1 = []\n",
    "        rouge_scores_list_r2 = []\n",
    "        rouge_scores_list_rl= []\n",
    "        rouge_scores = {\"rouge\": {\"recall\": 0, \"precision\": 0, \"f1-score\": 0}} \n",
    "\n",
    "        for question, expected_answer in questions_rouge.items():\n",
    "            prompt = f\"{question}\\nAnswer Options:\\n\"\n",
    "            prompt = get_prompt(prompt)\n",
    "            #print(prompt)\n",
    "            model_answer = model(prompt)\n",
    "\n",
    "            #remove leading & trailing spaces, extract language style from model_answer if answer is unnecessarily long\n",
    "            model_answer = model_answer.strip()\n",
    "            expected_answer = expected_answer.strip().lower()\n",
    "\n",
    "            model_answer = extract_language_style(model_answer)\n",
    "\n",
    "            # Get Rouge Score - Rouge() hat rouge-1, rouge-2 and rouge-l as option \n",
    "            rouge = Rouge()\n",
    "            scores = rouge.get_scores(expected_answer, model_answer) \n",
    "\n",
    "            #save rouge-1, rouge-2 and rouge-l values for each question in list\n",
    "            scores_list_r1 = rouge.get_scores(expected_answer, model_answer)[0]\n",
    "            rouge_scores_list_r1.append({\n",
    "                \"r\": scores_list_r1.get('rouge-1', {}).get('r', 0),\n",
    "                \"p\": scores_list_r1.get('rouge-1', {}).get('p', 0),\n",
    "                \"f\": scores_list_r1.get('rouge-1', {}).get('f', 0),\n",
    "            })\n",
    "            scores_list_r2 = rouge.get_scores(expected_answer, model_answer)[0]\n",
    "            rouge_scores_list_r2.append({\n",
    "                \"r\": scores_list_r2.get('rouge-2', {}).get('r', 0),\n",
    "                \"p\": scores_list_r2.get('rouge-2', {}).get('p', 0),\n",
    "                \"f\": scores_list_r2.get('rouge-2', {}).get('f', 0),\n",
    "            })\n",
    "            scores_list_rl = rouge.get_scores(expected_answer, model_answer)[0]\n",
    "            rouge_scores_list_rl.append({\n",
    "                \"r\": scores_list_rl.get('rouge-l', {}).get('r', 0),\n",
    "                \"p\": scores_list_rl.get('rouge-l', {}).get('p', 0),\n",
    "                \"f\": scores_list_rl.get('rouge-l', {}).get('f', 0),\n",
    "            })\n",
    "            \n",
    "            # Print the results for each question\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Expected Answer: {expected_answer}\")\n",
    "            print(f\"Model Answer: {model_answer}\")\n",
    "            print(f\"ROUGE Scores: {scores}\")\n",
    "            print(\"------------\")\n",
    "\n",
    "            # Store the model's answer and the corresponding question in the array\n",
    "            model_answers.append({\n",
    "                \"question\": question,\n",
    "                \"expected_answer\": expected_answer,\n",
    "                \"model_answer\": model_answer,\n",
    "            })\n",
    "\n",
    "        # Calculate the average 'r', 'p', and 'f' values for rouge-1, rouge-2 and rouge-l\n",
    "        average_r_r1 = round(sum(entry[\"r\"] for entry in rouge_scores_list_r1) / total_questions,3) \n",
    "        average_p_r1 = round(sum(entry[\"p\"] for entry in rouge_scores_list_r1) / total_questions,3)\n",
    "        average_f_r1 = round(sum(entry[\"f\"] for entry in rouge_scores_list_r1) / total_questions,3)\n",
    "\n",
    "        average_r_r2 = round(sum(entry[\"r\"] for entry in rouge_scores_list_r2) / total_questions,3) \n",
    "        average_p_r2 = round(sum(entry[\"p\"] for entry in rouge_scores_list_r2) / total_questions,3)\n",
    "        average_f_r2 = round(sum(entry[\"f\"] for entry in rouge_scores_list_r2) / total_questions,3)\n",
    "        \n",
    "        average_r_rl = round(sum(entry[\"r\"] for entry in rouge_scores_list_rl) / total_questions,3) \n",
    "        average_p_rl = round(sum(entry[\"p\"] for entry in rouge_scores_list_rl) / total_questions,3)\n",
    "        average_f_rl = round(sum(entry[\"f\"] for entry in rouge_scores_list_rl) / total_questions,3)\n",
    "\n",
    "        print(f\"Questions total: {total_questions}\")\n",
    "        print(f\"Average Recall (ROUGE-1): {average_r_r1}\")\n",
    "        print(f\"Average Precision (ROUGE-1): {average_p_r1}\")\n",
    "        print(f\"Average F1-score (ROUGE-1): {average_f_r1}\")\n",
    "        print(f\"Average Recall (ROUGE-2): {average_r_r2}\")\n",
    "        print(f\"Average Precision (ROUGE-2): {average_p_r2}\")\n",
    "        print(f\"Average F1-score (ROUGE-2): {average_f_r2}\")\n",
    "        print(f\"Average Recall (ROUGE-L): {average_r_rl}\")\n",
    "        print(f\"Average Precision (ROUGE-L): {average_p_rl}\")\n",
    "        print(f\"Average F1-score (ROUGE-L): {average_f_rl}\")\n",
    "\n",
    "        return model_answers, rouge_scores\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "#run method\n",
    "model_answers, rouge_scores = evaluate_model_rouge(llm, questions_rouge)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: Can you believe this weather? Do you think it'll rain later?\n",
      "Expected Answer: informal\n",
      "Model Answer: The language style used in the statement \"Can you believe this weather? Do you think it'll rain later?\" is informal, casual, and conversational. It is a common expression used in everyday conversation among friends or\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      15.46 ms /    48 runs   (    0.32 ms per token,  3104.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3398.16 ms /    24 tokens (  141.59 ms per token,     7.06 tokens per second)\n",
      "llama_print_timings:        eval time =    9576.40 ms /    47 runs   (  203.75 ms per token,     4.91 tokens per second)\n",
      "llama_print_timings:       total time =   13168.87 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: Hey, wanna grab a bite to eat later?\n",
      "Expected Answer: informal\n",
      "Model Answer: The language style used in the statement \"Hey, wanna grab a bite to eat later?\" is informal.\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.058823529411764705, 'p': 1.0, 'f': 0.11111111006172841}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.058823529411764705, 'p': 1.0, 'f': 0.11111111006172841}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =       9.27 ms /    27 runs   (    0.34 ms per token,  2913.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3427.92 ms /    21 tokens (  163.23 ms per token,     6.13 tokens per second)\n",
      "llama_print_timings:        eval time =    5540.72 ms /    26 runs   (  213.10 ms per token,     4.69 tokens per second)\n",
      "llama_print_timings:       total time =    9083.54 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      13.55 ms /    48 runs   (    0.28 ms per token,  3541.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4929.67 ms /    30 tokens (  164.32 ms per token,     6.09 tokens per second)\n",
      "llama_print_timings:        eval time =    9767.30 ms /    47 runs   (  207.81 ms per token,     4.81 tokens per second)\n",
      "llama_print_timings:       total time =   14900.40 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: What's up with that new movie everyone's talking about? Have you seen the trailer?\n",
      "Expected Answer: informal\n",
      "Model Answer: The language style used in the statement is informal, casual conversation. It is a common expression used among friends or peers to ask about something that is currently trending or popular. The use of \"everyone's talking\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n",
      "Question: What language style is used in the following Statement: Do you know if Sarah's coming to the party tonight? I heard she might not make it.\n",
      "Expected Answer: informal\n",
      "Model Answer: The language style used in the statement is Informal English.\n",
      "The sentence is written in a casual, conversational tone, with colloquial expressions such as \"I heard\" and \"might not make it.\" This\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      12.96 ms /    48 runs   (    0.27 ms per token,  3704.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4808.68 ms /    31 tokens (  155.12 ms per token,     6.45 tokens per second)\n",
      "llama_print_timings:        eval time =    9132.01 ms /    47 runs   (  194.30 ms per token,     5.15 tokens per second)\n",
      "llama_print_timings:       total time =   14128.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13720.35 ms\n",
      "llama_print_timings:      sample time =      15.40 ms /    48 runs   (    0.32 ms per token,  3116.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3852.52 ms /    26 tokens (  148.17 ms per token,     6.75 tokens per second)\n",
      "llama_print_timings:        eval time =   11101.54 ms /    47 runs   (  236.20 ms per token,     4.23 tokens per second)\n",
      "llama_print_timings:       total time =   15170.54 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: So, what's the deal with that new song everyone's listing to?\n",
      "Expected Answer: informal\n",
      "Model Answer: The language style used in the statement \"So, what's the deal with that new song everyone's listening to?\" is informal, colloquial, and conversational. It is a common expression used in everyday\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m#run method\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m model_answers, rouge_scores \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_rouge_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestions_rouge\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m, in \u001b[0;36mevaluate_model_rouge_\u001b[0;34m(model, questions)\u001b[0m\n\u001b[1;32m     12\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer Options:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m prompt \u001b[38;5;241m=\u001b[39m get_prompt(prompt)\n\u001b[0;32m---> 14\u001b[0m model_answer \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#remove leading & trailing spaces, extract language style from model_answer if answer is unnecessarily long\u001b[39;00m\n\u001b[1;32m     17\u001b[0m model_answer \u001b[38;5;241m=\u001b[39m model_answer\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/langchain/llms/base.py:870\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    868\u001b[0m     )\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 870\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    880\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/langchain/llms/base.py:650\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    636\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    637\u001b[0m         )\n\u001b[1;32m    638\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    639\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    640\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    648\u001b[0m         )\n\u001b[1;32m    649\u001b[0m     ]\n\u001b[0;32m--> 650\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/langchain/llms/base.py:538\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    537\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    539\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/langchain/llms/base.py:525\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    517\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    522\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 525\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    529\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    533\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    534\u001b[0m         )\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/langchain/llms/base.py:1047\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1046\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1049\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1050\u001b[0m     )\n\u001b[1;32m   1051\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/langchain/llms/llamacpp.py:291\u001b[0m, in \u001b[0;36mLlamaCpp._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# If streaming is enabled, we use the stream\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# method that yields as they are generated\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# and return the combined strings from the first choices's text:\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     combined_text_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[1;32m    292\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m    293\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    294\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    296\u001b[0m     ):\n\u001b[1;32m    297\u001b[0m         combined_text_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_text_output\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/langchain/llms/llamacpp.py:344\u001b[0m, in \u001b[0;36mLlamaCpp._stream\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_parameters(stop), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    343\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(prompt\u001b[38;5;241m=\u001b[39mprompt, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m    345\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m part[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    346\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m GenerationChunk(\n\u001b[1;32m    347\u001b[0m         text\u001b[38;5;241m=\u001b[39mpart[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    348\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs},\n\u001b[1;32m    349\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/llama_cpp/llama.py:1395\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001b[0m\n\u001b[1;32m   1393\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1394\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   1396\u001b[0m     prompt_tokens,\n\u001b[1;32m   1397\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1398\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   1399\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m   1400\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m   1401\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1402\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1403\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1404\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1405\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1406\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1407\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1408\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m   1409\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1410\u001b[0m ):\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_eos:\n\u001b[1;32m   1412\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/llama_cpp/llama.py:1199\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     grammar\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1200\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m   1201\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1202\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1212\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1213\u001b[0m     )\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stopping_criteria \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m stopping_criteria(\n\u001b[1;32m   1215\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m   1216\u001b[0m     ):\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/llama_cpp/llama.py:1030\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m   1026\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m   1028\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m   1029\u001b[0m )\n\u001b[0;32m-> 1030\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/llama_cpp/llama.py:466\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/llama_cpp/llama_cpp.py:1136\u001b[0m, in \u001b[0;36mllama_decode\u001b[0;34m(ctx, batch)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllama_decode\u001b[39m(ctx: llama_context_p, batch: llama_batch) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m-> 1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#ROUGE WITHOUT extraction of language style\n",
    "def evaluate_model_rouge_(model, questions):\n",
    "    try:\n",
    "        total_questions = len(questions)\n",
    "        model_answers = []\n",
    "        rouge_scores_list_r1 = []\n",
    "        rouge_scores_list_r2 = []\n",
    "        rouge_scores_list_rl= []\n",
    "        rouge_scores = {\"rouge\": {\"recall\": 0, \"precision\": 0, \"f1-score\": 0}} \n",
    "\n",
    "        for question, expected_answer in questions_rouge.items():\n",
    "            prompt = f\"{question}\\nAnswer Options:\\n\"\n",
    "            prompt = get_prompt(prompt)\n",
    "            model_answer = model(prompt)\n",
    "\n",
    "            #remove leading & trailing spaces, extract language style from model_answer if answer is unnecessarily long\n",
    "            model_answer = model_answer.strip()\n",
    "            expected_answer = expected_answer.lower()\n",
    "\n",
    "            #model_answer = extract_language_style(model_answer)\n",
    "\n",
    "            # Get Rouge Score - Rouge() hat rouge-1, rouge-2 and rouge-l as option \n",
    "            rouge = Rouge()\n",
    "            scores = rouge.get_scores(expected_answer, model_answer) \n",
    "\n",
    "            #save rouge-1, rouge-2 and rouge-l values for each question in list\n",
    "            scores_list_r1 = rouge.get_scores(expected_answer, model_answer)[0]\n",
    "            rouge_scores_list_r1.append({\n",
    "                \"r\": scores_list_r1.get('rouge-1', {}).get('r', 0),\n",
    "                \"p\": scores_list_r1.get('rouge-1', {}).get('p', 0),\n",
    "                \"f\": scores_list_r1.get('rouge-1', {}).get('f', 0),\n",
    "            })\n",
    "            scores_list_r2 = rouge.get_scores(expected_answer, model_answer)[0]\n",
    "            rouge_scores_list_r2.append({\n",
    "                \"r\": scores_list_r2.get('rouge-2', {}).get('r', 0),\n",
    "                \"p\": scores_list_r2.get('rouge-2', {}).get('p', 0),\n",
    "                \"f\": scores_list_r2.get('rouge-2', {}).get('f', 0),\n",
    "            })\n",
    "            scores_list_rl = rouge.get_scores(expected_answer, model_answer)[0]\n",
    "            rouge_scores_list_rl.append({\n",
    "                \"r\": scores_list_rl.get('rouge-l', {}).get('r', 0),\n",
    "                \"p\": scores_list_rl.get('rouge-l', {}).get('p', 0),\n",
    "                \"f\": scores_list_rl.get('rouge-l', {}).get('f', 0),\n",
    "            })\n",
    "            \n",
    "            # Print the results for each question\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Expected Answer: {expected_answer}\")\n",
    "            print(f\"Model Answer: {model_answer}\")\n",
    "            print(f\"ROUGE Scores: {scores}\")\n",
    "            print(\"------------\")\n",
    "\n",
    "            # Store the model's answer and the corresponding question in the array\n",
    "            model_answers.append({\n",
    "                \"question\": question,\n",
    "                \"expected_answer\": expected_answer,\n",
    "                \"model_answer\": model_answer,\n",
    "            })\n",
    "\n",
    "        # Calculate the average 'r', 'p', and 'f' values for rouge-1, rouge-2 and rouge-l\n",
    "        average_r_r1 = round(sum(entry[\"r\"] for entry in rouge_scores_list_r1) / total_questions,3) \n",
    "        average_p_r1 = round(sum(entry[\"p\"] for entry in rouge_scores_list_r1) / total_questions,3)\n",
    "        average_f_r1 = round(sum(entry[\"f\"] for entry in rouge_scores_list_r1) / total_questions,3)\n",
    "\n",
    "        average_r_r2 = round(sum(entry[\"r\"] for entry in rouge_scores_list_r2) / total_questions,3) \n",
    "        average_p_r2 = round(sum(entry[\"p\"] for entry in rouge_scores_list_r2) / total_questions,3)\n",
    "        average_f_r2 = round(sum(entry[\"f\"] for entry in rouge_scores_list_r2) / total_questions,3)\n",
    "        \n",
    "        average_r_rl = round(sum(entry[\"r\"] for entry in rouge_scores_list_rl) / total_questions,3) \n",
    "        average_p_rl = round(sum(entry[\"p\"] for entry in rouge_scores_list_rl) / total_questions,3)\n",
    "        average_f_rl = round(sum(entry[\"f\"] for entry in rouge_scores_list_rl) / total_questions,3)\n",
    "\n",
    "        print(f\"Questions total: {total_questions}\")\n",
    "        print(f\"Average Recall (ROUGE-1): {average_r_r1}\")\n",
    "        print(f\"Average Precision (ROUGE-1): {average_p_r1}\")\n",
    "        print(f\"Average F1-score (ROUGE-1): {average_f_r1}\")\n",
    "        print(f\"Average Recall (ROUGE-2): {average_r_r2}\")\n",
    "        print(f\"Average Precision (ROUGE-2): {average_p_r2}\")\n",
    "        print(f\"Average F1-score (ROUGE-2): {average_f_r2}\")\n",
    "        print(f\"Average Recall (ROUGE-L): {average_r_rl}\")\n",
    "        print(f\"Average Precision (ROUGE-L): {average_p_rl}\")\n",
    "        print(f\"Average F1-score (ROUGE-L): {average_f_rl}\")\n",
    "\n",
    "        return model_answers, rouge_scores\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "#run method\n",
    "model_answers, rouge_scores = evaluate_model_rouge_(llm, questions_rouge)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##multiple sentences\n",
    "import json\n",
    "\n",
    "# Load some sentences\n",
    "with open('./tests/data.json') as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "hyps, refs = map(list, zip(*[[d['hyp'], d['ref']] for d in data]))\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hyps, refs)\n",
    "# or\n",
    "scores = rouge.get_scores(hyps, refs, avg=True)\n",
    "\n",
    "\n",
    "##two files\n",
    "from rouge import FilesRouge\n",
    "\n",
    "files_rouge = FilesRouge()\n",
    "scores = files_rouge.get_scores(hyp_path, ref_path)\n",
    "# or\n",
    "scores = files_rouge.get_scores(hyp_path, ref_path, avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install bitsandbytes\n",
    "#%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation - Perplexity: measure that quantifies how well the model predicts a sample of text. Lower perplexity values indicate better performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, LlamaTokenizer\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "model_name = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n",
    "max_memory = f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  device_map='auto',\n",
    "  load_in_4bit=True,\n",
    "  max_memory=max_memory,\n",
    "  do_sample=True,\n",
    "  torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.max_length\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "device = \"cuda\"\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check Langchains QAEvalChain\n",
    "#https://github.com/langchain-ai/langchain/blob/a6b39afe0e34621fafac885af05bbbb445ea5ac0/langchain/evaluation/qa/eval_chain.py#L42C1-L42C1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
