{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain llama-cpp-python \n",
    "#CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define System Prompt and load Llama2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFAULT PROMPT\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + DEFAULT_SYSTEM_PROMPT + E_SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chatbot prompt for later use case\n",
    "SYSTEM_PROMPT_USECASE = \"\"\"\n",
    "<<SYS>>\n",
    "You are a chatbot that helps people learn a new language on a platform. You always give the answer to the question in an academic language style.\n",
    "If you don't know the answer to a question, you tell them truthfully that you don't know and don't give false information. You are a helpful, respectful and honest assistant.\n",
    "Your answers should not contain harmful, unethical, racist, sexist, toxic, dangerous or illegal content. Please make sure that your answers are socially unbiased and positive if nature#.\n",
    "If a question does not make sense or is not factually coherent, please explain why, rather than answering something incorrectly.<</SYS>>\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + SYSTEM_PROMPT_USECASE +E_SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general prompt to test llm for testing questions\n",
    "SYSTEM_PROMPT_EVAL_ACCURACY = \"\"\"\n",
    "<<SYS>>\n",
    "You will be presented with a question and several answer options where only one is correct. You will respond with the letter that represents the correct answer option.\n",
    "Do not add any answer options, only choose the correct one from the for options presented because one of them is correct.\n",
    "If you don't know the answer, be honest and state that you don't know.\n",
    "<</SYS>>\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + SYSTEM_PROMPT_EVAL_ACCURACY +E_SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'llama-2-7b.Q4_K_M.gguf', 'llama-2-7b-chat.Q4_K_M.gguf']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prompt(instruction):\n",
    "    return B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "\n",
    "os.listdir(\"/Users/josi/Llama2_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/josi/Llama2_weights/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.15.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:        blk.15.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:             blk.15.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:             blk.15.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.16.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:        blk.16.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.16.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.16.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.17.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.17.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.17.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.17.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.18.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:        blk.18.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.18.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.18.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.19.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.19.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.19.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.19.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:              blk.2.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.2.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:         blk.2.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.2.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:              blk.2.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.20.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.20.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.20.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.20.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.20.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.20.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.21.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.21.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.21.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.21.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.21.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.21.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.22.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.22.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.22.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.22.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.22.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.22.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.23.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.23.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.23.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.23.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.23.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.23.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:            blk.3.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:              blk.3.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:              blk.3.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:         blk.3.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:              blk.3.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:              blk.3.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:            blk.4.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:              blk.4.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:              blk.4.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:         blk.4.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:              blk.4.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:              blk.4.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:            blk.5.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:              blk.5.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:              blk.5.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:         blk.5.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:              blk.5.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:              blk.5.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:            blk.6.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:              blk.6.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:              blk.6.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:         blk.6.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:              blk.6.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:              blk.6.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.7.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:              blk.7.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:              blk.7.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:         blk.7.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:              blk.7.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:              blk.7.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:            blk.8.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:              blk.8.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:              blk.8.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:         blk.8.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:              blk.8.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.8.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:            blk.9.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:              blk.9.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:              blk.9.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:         blk.9.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:              blk.9.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:              blk.9.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:        blk.24.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:        blk.25.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.26.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:        blk.27.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:        blk.28.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:        blk.29.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:        blk.30.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  18:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MB\n",
      "llm_load_tensors: mem required  = 3891.35 MB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "llama_build_graph: non-view tensors processed: 740/740\n",
      "llama_new_context_with_model: compute buffer total size = 2.66 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path= \"/Users/josi/Llama2_weights/llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    temperature=0,\n",
    "    max_tokens=48,\n",
    "    top_p=1,\n",
    "    #callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionaries with test questions (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Choose the correct option that represents an informal greeting.\n",
      "Answer Options:\n",
      "a: Hi\n",
      "b: Good Day\n",
      "c: Greetings\n",
      "d: Dear Mr./Mrs.\n",
      "Correct Answer: a: Hi\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: Choose the correct option that represents an informal response to the question: 'How's it going?'\n",
      "Answer Options:\n",
      "a: Hi, good hbu?\n",
      "b: Greetings, everything is fine, and you?\n",
      "c: Good day, I'm well, thank you. How are you?\n",
      "d: Hello, everything's fine\n",
      "Correct Answer: a: Hi, good hbu?\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: Choose the correct option that represents an informal email closing.\n",
      "Answer Options:\n",
      "a: Cheers\n",
      "b: Sincerely\n",
      "c: Best regards\n",
      "d: I am awaiting your response\n",
      "Correct Answer: a: Cheers\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: Choose the correct option that represents a formal greeting.\n",
      "Answer Options:\n",
      "a: Good Day\n",
      "b: Hi\n",
      "c: Hey\n",
      "d: Yo\n",
      "Correct Answer: a: Good Day\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: Choose the correct option that represents a formal response to the question: 'How are you doing?\n",
      "Answer Options:\n",
      "a: Hello, I am doing well. How about you?\n",
      "b: Hi, good you?\n",
      "c: Not too bad, you?\n",
      "d: good, you?\n",
      "Correct Answer: a: Hello, I am doing well. How about you?\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: Choose the correct option that represents a professional email closing.\n",
      "Answer Options:\n",
      "a: Sincerely\n",
      "b: Cheers\n",
      "c: See you!\n",
      "d: Until next time.\n",
      "Correct Answer: a: Sincerely\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: Choose the correct option that represents an academic paper introduction.\n",
      "Answer Options:\n",
      "a: Today we present our topic XYZ and we will also focus on ABC.\n",
      "b: In this paper XYZ is presented with a focus on ABC.\n",
      "c: Hi! In our paper we write about XYZ and ABC.\n",
      "d: I show you something about XYZ in this paper.\n",
      "Correct Answer: b: In this paper XYZ is presented with a focus on ABC.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: The improvements can ºt be introduced due to funding restrictions.\n",
      "Answer Options:\n",
      "a: Formal Language Style\n",
      "b: Informal Language Style\n",
      "c: Academic Language Style\n",
      "Correct Answer: b: Informal Language Style\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question:  It was raining cats and dogs.\n",
      "Answer Options:\n",
      "a: Formal Language Style\n",
      "b: Informal Language Style\n",
      "c: Academic Language Style\n",
      "Correct Answer: b: Informal Language Style\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: Improvements cannot be introduced due to funding restrictions.\n",
      "Answer Options:\n",
      "a: Formal Language Style\n",
      "b: Informal Language Style\n",
      "c: Academic Language Style\n",
      "Correct Answer: a: Formal Language Style\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: During the interview, students were asked about their experiences.\n",
      "Answer Options:\n",
      "a: Formal Language Style\n",
      "b: Informal Language Style\n",
      "c: Academic Language Style\n",
      "Correct Answer: a: Formal Language Style\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: New results in this area are produced by the research group.\n",
      "Answer Options:\n",
      "a: Formal Language Style\n",
      "b: Informal Language Style\n",
      "c: Academic Language Style\n",
      "Correct Answer: c: Academic Language Style\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: Choose the correct translation for 'hello' in Spanish.\n",
      "Answer Options:\n",
      "a: Hola\n",
      "b: Hello\n",
      "c: Hallo\n",
      "d: Bonjour\n",
      "Correct Answer: a: Hola\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: Choose the correct translation for 'Thank you' in German.\n",
      "Answer Options:\n",
      "a: Danke\n",
      "b: Thanks\n",
      "c: Gracias\n",
      "d: Auf Wiedersehen\n",
      "Correct Answer: a: Danke\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: Choose the correct translation for 'book' in italian.\n",
      "Answer Options:\n",
      "a: Libro\n",
      "b: Buch\n",
      "c: Plant\n",
      "d: Libre\n",
      "Correct Answer: a: Libro\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: Choose the correct answer that is the capital of France?\n",
      "Answer Options:\n",
      "a: Paris\n",
      "b: Berlin\n",
      "c: Madrid\n",
      "d: New York\n",
      "Correct Answer: a: Paris\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: Choose the correct answer for the following math problem: 3x + 5 = 20.\n",
      "Answer Options:\n",
      "a: x = 5\n",
      "b: x = 15\n",
      "c: x = 10\n",
      "d: x = 2\n",
      "Correct Answer: a: x = 5\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: Choose the correct answer for the following statement: If 'cat' is to 'kitten,' what is 'dog' to?\n",
      "Answer Options:\n",
      "a: 'Dog' is to 'puppy'\n",
      "b: 'Dog' is to 'kitten'\n",
      "c: 'Dog' is to 'cub'\n",
      "d: 'Dog' is to 'kitty'\n",
      "Correct Answer: a: 'Dog' is to 'puppy'\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: Choose the correct answer for the following statement: Identify the named Person in the sentence 'Elon Musk is the CEO of a company that produces cars.'\n",
      "Answer Options:\n",
      "a: Elon Musk\n",
      "b: CEO\n",
      "c: company\n",
      "d: cars\n",
      "Correct Answer: a: Elon Musk\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: Choose the correct answer for the following statement: Extract the date mentioned in the text 'The conference is scheduled for January 25, 2023.'\n",
      "Answer Options:\n",
      "a: January 25, 2023\n",
      "b: May 25-27, 2023.\n",
      "c: January 27, 2023.\n",
      "d: January 2, 2025\n",
      "Correct Answer: a: January 25, 2023\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Questions with Answer options for ACCURACY\n",
    "questions_accuracy = [\n",
    "\n",
    "    #Informal Style\n",
    "    {\n",
    "        \"question\": \"Choose the correct option that represents an informal greeting.\",\n",
    "        \"choices\": {\"a\": \"Hi\", \"b\": \"Good Day\", \"c\": \"Greetings\", \"d\": \"Dear Mr./Mrs.\"},\n",
    "        \"correct\": \"a: Hi\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct option that represents an informal response to the question: 'How's it going?'\",\n",
    "        \"choices\": {\"a\": \"Hi, good hbu?\", \"b\": \"Greetings, everything is fine, and you?\", \"c\": \"Good day, I'm well, thank you. How are you?\", \"d\": \"Hello, everything's fine\"},\n",
    "        \"correct\": \"a: Hi, good hbu?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct option that represents an informal email closing.\",\n",
    "        \"choices\": {\"a\": \"Cheers\", \"b\": \"Sincerely\", \"c\": \"Best regards\", \"d\": \"I am awaiting your response\"},\n",
    "        \"correct\": \"a: Cheers\"\n",
    "    },\n",
    "\n",
    "    #Formal Style\n",
    "    {\n",
    "        \"question\": \"Choose the correct option that represents a formal greeting.\",\n",
    "        \"choices\": {\"a\": \"Good Day\", \"b\": \"Hi\", \"c\": \"Hey\", \"d\": \"Yo\"},\n",
    "        \"correct\": \"a: Good Day\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct option that represents a formal response to the question: 'How are you doing?\",\n",
    "        \"choices\": {\"a\": \"Hello, I am doing well. How about you?\", \"b\": \"Hi, good you?\", \"c\": \"Not too bad, you?\", \"d\": \"good, you?\"},\n",
    "        \"correct\": \"a: Hello, I am doing well. How about you?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct option that represents a professional email closing.\",\n",
    "        \"choices\": {\"a\": \"Sincerely\", \"b\": \"Cheers\", \"c\": \"See you!\", \"d\": \"Until next time.\"},\n",
    "        \"correct\": \"a: Sincerely\"\n",
    "    },\n",
    "\n",
    "    #Academic Style\n",
    "      {\n",
    "        \"question\": \"Choose the correct option that represents an academic paper introduction.\",\n",
    "        \"choices\": {\"a\": \"Today we present our topic XYZ and we will also focus on ABC.\", \"b\": \"In this paper XYZ is presented with a focus on ABC.\", \"c\": \"Hi! In our paper we write about XYZ and ABC.\", \"d\": \"I show you something about XYZ in this paper.\"},\n",
    "        \"correct\": \"b: In this paper XYZ is presented with a focus on ABC.\"\n",
    "    },\n",
    "\n",
    "\n",
    "    #Detect correct language style\n",
    "    {\n",
    "        \"question\": \"The improvements can ºt be introduced due to funding restrictions.\",\n",
    "        \"choices\": {\"a\": \"Formal Language Style\", \"b\": \"Informal Language Style\", \"c\": \"Academic Language Style\"},\n",
    "        \"correct\": \"b: Informal Language Style\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \" It was raining cats and dogs.\",\n",
    "        \"choices\": {\"a\": \"Formal Language Style\", \"b\": \"Informal Language Style\", \"c\": \"Academic Language Style\"},\n",
    "        \"correct\": \"b: Informal Language Style\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Improvements cannot be introduced due to funding restrictions.\",\n",
    "        \"choices\": {\"a\": \"Formal Language Style\", \"b\": \"Informal Language Style\", \"c\": \"Academic Language Style\"},\n",
    "        \"correct\": \"a: Formal Language Style\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"During the interview, students were asked about their experiences.\",\n",
    "        \"choices\": {\"a\": \"Formal Language Style\", \"b\": \"Informal Language Style\", \"c\": \"Academic Language Style\"},\n",
    "        \"correct\": \"a: Formal Language Style\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"New results in this area are produced by the research group.\",\n",
    "        \"choices\": {\"a\": \"Formal Language Style\", \"b\": \"Informal Language Style\", \"c\": \"Academic Language Style\"},\n",
    "        \"correct\": \"c: Academic Language Style\"\n",
    "    },\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    #Translation\n",
    "    {\n",
    "        \"question\": \"Choose the correct translation for 'hello' in Spanish.\",\n",
    "        \"choices\": {\"a\":\"Hola\", \"b\": \"Hello\", \"c\":\"Hallo\", \"d\": \"Bonjour\"},\n",
    "        \"correct\": \"a: Hola\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct translation for 'Thank you' in German.\",\n",
    "        \"choices\": {\"a\": \"Danke\", \"b\": \"Thanks\", \"c\": \"Gracias\", \"d\": \"Auf Wiedersehen\"},\n",
    "        \"correct\": \"a: Danke\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct translation for 'book' in italian.\",\n",
    "        \"choices\": {\"a\": \"Libro\", \"b\": \"Buch\", \"c\": \"Plant\", \"d\": \"Libre\"},\n",
    "        \"correct\": \"a: Libro\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct answer that is the capital of France?\",\n",
    "        \"choices\": {\"a\": \"Paris\", \"b\": \"Berlin\", \"c\": \"Madrid\", \"d\": \"New York\"},\n",
    "        \"correct\": \"a: Paris\"\n",
    "    },\n",
    "\n",
    "    #General\n",
    "    {\n",
    "        \"question\": \"Choose the correct answer for the following math problem: 3x + 5 = 20.\",\n",
    "        \"choices\": {\"a\": \"x = 5\", \"b\": \"x = 15\", \"c\": \"x = 10\", \"d\": \"x = 2\"},\n",
    "        \"correct\": \"a: x = 5\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct answer for the following statement: If 'cat' is to 'kitten,' what is 'dog' to?\",\n",
    "        \"choices\": {\"a\": \"'Dog' is to 'puppy'\", \"b\": \"'Dog' is to 'kitten'\", \"c\": \"'Dog' is to 'cub'\", \"d\": \"'Dog' is to 'kitty'\"},\n",
    "        \"correct\": \"a: 'Dog' is to 'puppy'\"\n",
    "    },\n",
    "\n",
    "    #NER\n",
    "    {\n",
    "        \"question\": \"Choose the correct answer for the following statement: Identify the named Person in the sentence 'Elon Musk is the CEO of a company that produces cars.'\",\n",
    "        \"choices\": {\"a\": \"Elon Musk\", \"b\": \"CEO\", \"c\": \"company\", \"d\": \"cars\"},\n",
    "        \"correct\": \"a: Elon Musk\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Choose the correct answer for the following statement: Extract the date mentioned in the text 'The conference is scheduled for January 25, 2023.'\",\n",
    "        \"choices\": {\"a\": \"January 25, 2023\", \"b\": \"May 25-27, 2023.\", \"c\": \"January 27, 2023.\", \"d\": \"January 2, 2025\"},\n",
    "        \"correct\": \"a: January 25, 2023\"\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "for question in questions_accuracy:\n",
    "    print(\"Question:\", question[\"question\"])\n",
    "    print(\"Answer Options:\")\n",
    "    for option, answer in question[\"choices\"].items():\n",
    "        print(f\"{option}: {answer}\")\n",
    "    print(\"Correct Answer:\", f\"{question['correct']}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model based on Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Choose the correct option that represents an informal greeting.\n",
      "Answer Options:\n",
      "a: Hi\n",
      "b: Good Day\n",
      "c: Greetings\n",
      "d: Dear Mr./Mrs.\n",
      "Correct Answer: a: Hi\n",
      "Model Answer:   a: Hi\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       1.47 ms /     5 runs   (    0.29 ms per token,  3399.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5280.95 ms /    43 tokens (  122.81 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:        eval time =     565.37 ms /     4 runs   (  141.34 ms per token,     7.08 tokens per second)\n",
      "llama_print_timings:       total time =    5877.17 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Choose the correct option that represents an informal response to the question: 'How's it going?'\n",
      "Answer Options:\n",
      "a: Hi, good hbu?\n",
      "b: Greetings, everything is fine, and you?\n",
      "c: Good day, I'm well, thank you. How are you?\n",
      "d: Hello, everything's fine\n",
      "Correct Answer: a: Hi, good hbu?\n",
      "Model Answer:   c: Good day, I'm well, thank you. How are you?\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       5.15 ms /    19 runs   (    0.27 ms per token,  3690.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8207.84 ms /    71 tokens (  115.60 ms per token,     8.65 tokens per second)\n",
      "llama_print_timings:        eval time =    2706.46 ms /    18 runs   (  150.36 ms per token,     6.65 tokens per second)\n",
      "llama_print_timings:       total time =   10997.63 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Choose the correct option that represents an informal email closing.\n",
      "Answer Options:\n",
      "a: Cheers\n",
      "b: Sincerely\n",
      "c: Best regards\n",
      "d: I am awaiting your response\n",
      "Correct Answer: a: Cheers\n",
      "Model Answer:   b: Sincerely\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       2.05 ms /     7 runs   (    0.29 ms per token,  3419.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4180.72 ms /    36 tokens (  116.13 ms per token,     8.61 tokens per second)\n",
      "llama_print_timings:        eval time =     901.72 ms /     6 runs   (  150.29 ms per token,     6.65 tokens per second)\n",
      "llama_print_timings:       total time =    5116.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Choose the correct option that represents a formal greeting.\n",
      "Answer Options:\n",
      "a: Good Day\n",
      "b: Hi\n",
      "c: Hey\n",
      "d: Yo\n",
      "Correct Answer: a: Good Day\n",
      "Model Answer:   b: Hi\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       1.42 ms /     5 runs   (    0.28 ms per token,  3518.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3733.87 ms /    32 tokens (  116.68 ms per token,     8.57 tokens per second)\n",
      "llama_print_timings:        eval time =     587.34 ms /     4 runs   (  146.84 ms per token,     6.81 tokens per second)\n",
      "llama_print_timings:       total time =    4345.02 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Choose the correct option that represents a formal response to the question: 'How are you doing?\n",
      "Answer Options:\n",
      "a: Hello, I am doing well. How about you?\n",
      "b: Hi, good you?\n",
      "c: Not too bad, you?\n",
      "d: good, you?\n",
      "Correct Answer: a: Hello, I am doing well. How about you?\n",
      "Model Answer:   The correct answer is (a): \"Hello, I am doing well. How about you?\"\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       6.57 ms /    21 runs   (    0.31 ms per token,  3197.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7084.40 ms /    58 tokens (  122.14 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:        eval time =    4325.87 ms /    20 runs   (  216.29 ms per token,     4.62 tokens per second)\n",
      "llama_print_timings:       total time =   11511.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Choose the correct option that represents a professional email closing.\n",
      "Answer Options:\n",
      "a: Sincerely\n",
      "b: Cheers\n",
      "c: See you!\n",
      "d: Until next time.\n",
      "Correct Answer: a: Sincerely\n",
      "Model Answer:   d: Until next time\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       2.02 ms /     7 runs   (    0.29 ms per token,  3472.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4871.71 ms /    36 tokens (  135.33 ms per token,     7.39 tokens per second)\n",
      "llama_print_timings:        eval time =    1141.10 ms /     6 runs   (  190.18 ms per token,     5.26 tokens per second)\n",
      "llama_print_timings:       total time =    6047.54 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Choose the correct option that represents an academic paper introduction.\n",
      "Answer Options:\n",
      "a: Today we present our topic XYZ and we will also focus on ABC.\n",
      "b: In this paper XYZ is presented with a focus on ABC.\n",
      "c: Hi! In our paper we write about XYZ and ABC.\n",
      "d: I show you something about XYZ in this paper.\n",
      "Correct Answer: b: In this paper XYZ is presented with a focus on ABC.\n",
      "Model Answer:   The correct answer is (b): \"In this paper XYZ is presented with a focus on ABC.\"\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       7.28 ms /    24 runs   (    0.30 ms per token,  3297.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11474.04 ms /    82 tokens (  139.93 ms per token,     7.15 tokens per second)\n",
      "llama_print_timings:        eval time =    4022.55 ms /    23 runs   (  174.89 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:       total time =   15605.92 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: The improvements can ºt be introduced due to funding restrictions.\n",
      "Answer Options:\n",
      "a: Formal Language Style\n",
      "b: Informal Language Style\n",
      "c: Academic Language Style\n",
      "Correct Answer: b: Informal Language Style\n",
      "Model Answer:   c: Academic Language Style\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       2.20 ms /     8 runs   (    0.27 ms per token,  3641.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5895.32 ms /    43 tokens (  137.10 ms per token,     7.29 tokens per second)\n",
      "llama_print_timings:        eval time =    1178.55 ms /     7 runs   (  168.36 ms per token,     5.94 tokens per second)\n",
      "llama_print_timings:       total time =    7112.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       2.35 ms /     8 runs   (    0.29 ms per token,  3398.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5382.18 ms /    39 tokens (  138.00 ms per token,     7.25 tokens per second)\n",
      "llama_print_timings:        eval time =    1186.43 ms /     7 runs   (  169.49 ms per token,     5.90 tokens per second)\n",
      "llama_print_timings:       total time =    6608.68 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  It was raining cats and dogs.\n",
      "Answer Options:\n",
      "a: Formal Language Style\n",
      "b: Informal Language Style\n",
      "c: Academic Language Style\n",
      "Correct Answer: b: Informal Language Style\n",
      "Model Answer:   b: Informal Language Style\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       2.33 ms /     8 runs   (    0.29 ms per token,  3427.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5683.43 ms /    42 tokens (  135.32 ms per token,     7.39 tokens per second)\n",
      "llama_print_timings:        eval time =    1155.16 ms /     7 runs   (  165.02 ms per token,     6.06 tokens per second)\n",
      "llama_print_timings:       total time =    6879.42 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Improvements cannot be introduced due to funding restrictions.\n",
      "Answer Options:\n",
      "a: Formal Language Style\n",
      "b: Informal Language Style\n",
      "c: Academic Language Style\n",
      "Correct Answer: a: Formal Language Style\n",
      "Model Answer:   c: Academic Language Style\n",
      "------------\n",
      "Question: During the interview, students were asked about their experiences.\n",
      "Answer Options:\n",
      "a: Formal Language Style\n",
      "b: Informal Language Style\n",
      "c: Academic Language Style\n",
      "Correct Answer: a: Formal Language Style\n",
      "Model Answer:   c: Academic Language Style\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       2.24 ms /     8 runs   (    0.28 ms per token,  3565.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5765.39 ms /    42 tokens (  137.27 ms per token,     7.28 tokens per second)\n",
      "llama_print_timings:        eval time =    1210.68 ms /     7 runs   (  172.95 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =    7014.49 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: New results in this area are produced by the research group.\n",
      "Answer Options:\n",
      "a: Formal Language Style\n",
      "b: Informal Language Style\n",
      "c: Academic Language Style\n",
      "Correct Answer: c: Academic Language Style\n",
      "Model Answer:   c: Academic Language Style\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       2.21 ms /     8 runs   (    0.28 ms per token,  3616.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5248.69 ms /    42 tokens (  124.97 ms per token,     8.00 tokens per second)\n",
      "llama_print_timings:        eval time =    1188.12 ms /     7 runs   (  169.73 ms per token,     5.89 tokens per second)\n",
      "llama_print_timings:       total time =    6474.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Choose the correct translation for 'hello' in Spanish.\n",
      "Answer Options:\n",
      "a: Hola\n",
      "b: Hello\n",
      "c: Hallo\n",
      "d: Bonjour\n",
      "Correct Answer: a: Hola\n",
      "Model Answer:   a: Hola\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       1.76 ms /     6 runs   (    0.29 ms per token,  3416.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4876.03 ms /    40 tokens (  121.90 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:        eval time =     902.57 ms /     6 runs   (  150.43 ms per token,     6.65 tokens per second)\n",
      "llama_print_timings:       total time =    5812.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Choose the correct translation for 'Thank you' in German.\n",
      "Answer Options:\n",
      "a: Danke\n",
      "b: Thanks\n",
      "c: Gracias\n",
      "d: Auf Wiedersehen\n",
      "Correct Answer: a: Danke\n",
      "Model Answer:   a: Danke\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       1.78 ms /     6 runs   (    0.30 ms per token,  3365.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4372.16 ms /    36 tokens (  121.45 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:        eval time =     852.37 ms /     5 runs   (  170.47 ms per token,     5.87 tokens per second)\n",
      "llama_print_timings:       total time =    5255.12 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       1.75 ms /     6 runs   (    0.29 ms per token,  3430.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4757.94 ms /    32 tokens (  148.69 ms per token,     6.73 tokens per second)\n",
      "llama_print_timings:        eval time =    1023.54 ms /     6 runs   (  170.59 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:       total time =    5815.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Choose the correct translation for 'book' in italian.\n",
      "Answer Options:\n",
      "a: Libro\n",
      "b: Buch\n",
      "c: Plant\n",
      "d: Libre\n",
      "Correct Answer: a: Libro\n",
      "Model Answer:   a: Libro\n",
      "------------\n",
      "Question: Choose the correct answer that is the capital of France?\n",
      "Answer Options:\n",
      "a: Paris\n",
      "b: Berlin\n",
      "c: Madrid\n",
      "d: New York\n",
      "Correct Answer: a: Paris\n",
      "Model Answer:   a: Paris\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       1.49 ms /     5 runs   (    0.30 ms per token,  3355.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5011.81 ms /    34 tokens (  147.41 ms per token,     6.78 tokens per second)\n",
      "llama_print_timings:        eval time =     702.68 ms /     4 runs   (  175.67 ms per token,     5.69 tokens per second)\n",
      "llama_print_timings:       total time =    5744.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Choose the correct answer for the following math problem: 3x + 5 = 20.\n",
      "Answer Options:\n",
      "a: x = 5\n",
      "b: x = 15\n",
      "c: x = 10\n",
      "d: x = 2\n",
      "Correct Answer: a: x = 5\n",
      "Model Answer:   Sure, I'm ready to help! The correct answer for the math problem 3x + 5 = 20 is:\n",
      "d: x = 2\n",
      "So, the answer is (d) x = \n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =      13.54 ms /    48 runs   (    0.28 ms per token,  3546.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9543.95 ms /    56 tokens (  170.43 ms per token,     5.87 tokens per second)\n",
      "llama_print_timings:        eval time =    9029.46 ms /    47 runs   (  192.12 ms per token,     5.21 tokens per second)\n",
      "llama_print_timings:       total time =   18772.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Choose the correct answer for the following statement: If 'cat' is to 'kitten,' what is 'dog' to?\n",
      "Answer Options:\n",
      "a: 'Dog' is to 'puppy'\n",
      "b: 'Dog' is to 'kitten'\n",
      "c: 'Dog' is to 'cub'\n",
      "d: 'Dog' is to 'kitty'\n",
      "Correct Answer: a: 'Dog' is to 'puppy'\n",
      "Model Answer:   b: 'Dog' is to 'kitten'.\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       3.04 ms /    14 runs   (    0.22 ms per token,  4611.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10734.35 ms /    80 tokens (  134.18 ms per token,     7.45 tokens per second)\n",
      "llama_print_timings:        eval time =    2337.18 ms /    14 runs   (  166.94 ms per token,     5.99 tokens per second)\n",
      "llama_print_timings:       total time =   13143.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Choose the correct answer for the following statement: Identify the named Person in the sentence 'Elon Musk is the CEO of a company that produces cars.'\n",
      "Answer Options:\n",
      "a: Elon Musk\n",
      "b: CEO\n",
      "c: company\n",
      "d: cars\n",
      "Correct Answer: a: Elon Musk\n",
      "Model Answer:   b: CEO\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       1.96 ms /     6 runs   (    0.33 ms per token,  3069.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7572.69 ms /    53 tokens (  142.88 ms per token,     7.00 tokens per second)\n",
      "llama_print_timings:        eval time =     881.87 ms /     5 runs   (  176.37 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:       total time =    8497.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Choose the correct answer for the following statement: Extract the date mentioned in the text 'The conference is scheduled for January 25, 2023.'\n",
      "Answer Options:\n",
      "a: January 25, 2023\n",
      "b: May 25-27, 2023.\n",
      "c: January 27, 2023.\n",
      "d: January 2, 2025\n",
      "Correct Answer: a: January 25, 2023\n",
      "Model Answer:   a: January 25, 2023\n",
      "------------\n",
      "Total Questions: 20\n",
      "Correct Answers: 8\n",
      "Accuracy: 40.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       4.33 ms /    14 runs   (    0.31 ms per token,  3230.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16650.84 ms /    90 tokens (  185.01 ms per token,     5.41 tokens per second)\n",
      "llama_print_timings:        eval time =    4555.21 ms /    13 runs   (  350.40 ms per token,     2.85 tokens per second)\n",
      "llama_print_timings:       total time =   21289.25 ms\n"
     ]
    }
   ],
   "source": [
    "model_answers = []\n",
    "\n",
    "def evaluate_model_accuracy(model, questions_accuracy):\n",
    "    try:\n",
    "        correct_answers = 0\n",
    "        total_questions = len(questions_accuracy)\n",
    "\n",
    "        for question_data in questions_accuracy:\n",
    "            question = question_data[\"question\"]\n",
    "            choices = question_data[\"choices\"]\n",
    "            correct_option = question_data[\"correct\"]\n",
    "\n",
    "            # Get model's answer for current question\n",
    "            prompt = f\"{question}\\nAnswer Options:\\n\"\n",
    "            for option, answer in choices.items():\n",
    "                prompt += f\"{option}: {answer}\\n\"\n",
    "            prompt = get_prompt(prompt)\n",
    "            #print(prompt)\n",
    "            model_answer = model(prompt)\n",
    "\n",
    "            # Check if selected option in model's answer matches the correct option\n",
    "            if model_answer.strip() == correct_option.strip():\n",
    "                correct_answers += 1\n",
    "\n",
    "            # Print results for each question\n",
    "            print(f\"Question: {question}\")\n",
    "            print(\"Answer Options:\")\n",
    "            for option, answer in choices.items():\n",
    "                print(f\"{option}: {answer}\")\n",
    "            print(f\"Correct Answer: {correct_option}\")\n",
    "            print(f\"Model Answer: {model_answer}\")\n",
    "            print(\"------------\")\n",
    "\n",
    "            # Store the model's answer and the corresponding question in the array\n",
    "            model_answers.append({\n",
    "                \"question\": question,\n",
    "                \"choices\": choices,\n",
    "                \"correct_option\": correct_option,\n",
    "                \"selected_option\": model_answers,\n",
    "            })\n",
    "\n",
    "        # print overall evaluation results\n",
    "        print(f\"Total Questions: {total_questions}\")\n",
    "        print(f\"Correct Answers: {correct_answers}\")\n",
    "        print(f\"Accuracy: {correct_answers / total_questions * 100:.2f}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "#run method to get accuracy score\n",
    "evaluate_model_accuracy(llm, questions_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model based on Rouge (Recall-Oriented Understudy for Gissing Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of metrics used for evaluating the quality of summaries. It compares the generated summary with one or more reference summaries and calculates precision, recall, and F1-score \n",
    "\n",
    "- ROUGE-N quantifies the overlap of N-grams, contiguous sequences of N items (typically words or characters), between the system-generated summary and the reference summary) \n",
    "    - ROUGE-1 (unigram overlap): This metric measures the overlap of unigrams (single words) between the generated summary and the reference summary. It focuses on the recall of unigrams.\n",
    "    - ROUGE-2 (bigram overlap): Similar to ROUGE-1, but it measures the overlap of bigrams (pairs of consecutive words) instead of unigrams. It evaluates the recall of bigrams.\n",
    "- ROUGE-L (longest common subsequence): Instead of measuring word overlap, ROUGE-L focuses on the longest common subsequence (LCS) between the generated and reference summaries. It evaluates the recall of the longest common subsequence\n",
    "\n",
    "- high precision value suggests that the words or phrases churned out by the machine translation or submodel are primarily accurate\n",
    "- high recall value, ideally close to 1, implies that the content of the machine-generated output aligns closely with the human-made reference. It signifies the model's proficiency in capturing relevant information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general prompt to test llm for testing questions\n",
    "SYSTEM_PROMPT_EVAL_ROUGE = \"\"\"\n",
    "<<SYS>>\n",
    "You will be presented with a question where you are expected to respond only with the correct answer in a short and simple manner. Use one sentence maximum to answer the question. \n",
    "Only respond with the correct answer itself and leave out fill words like \"sure\" or \"certainly\".\n",
    "If you don't know the answer, be honest and state that you don't know. Don't give any false information.\n",
    "<</SYS>>\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + SYSTEM_PROMPT_EVAL_ROUGE + E_SYS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt:\"\"\"\n",
    "<<SYS>>\n",
    "You will be presented with a question where you are expected to respond only with the correct answer in a short and simple manner. Use one sentence maximum to answer the question. \n",
    "Only respond with the correct answer itself and leave out fill words like \"sure\" or \"certainly\".\n",
    "If you don't know the answer, be honest and state that you don't know. Don't give any false information.\n",
    "<</SYS>>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "results: Average Recall: 0.376\n",
    "Average Precision: 0.582\n",
    "Average F1-score: 0.416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test questions with different categories for ROUGE\n",
    "textual_entailment = {\n",
    "    \"If 'A implies B' and 'B is false,' what can you conclude about A?\":\"Nothing definitive can be concluded about A.\",\n",
    "}\n",
    "\n",
    "reasoning_problem_solving = {\n",
    "    \"If a car travels at 60 miles per hour, how long will it take to cover 120 miles?\": \"2 hours\",\n",
    "}\n",
    "\n",
    "analogical_reasoning = {\n",
    "    \"If 'cat' is to 'kitten,' what is 'dog' to?\":\"'Dog' is to 'puppy'\",\n",
    "}\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "ner = {\n",
    "    \"Identify the named person in the sentence 'Elon Musk is the CEO of SpaceX.'\": \"Elon Musk\",\n",
    "    \"Extract the date mentioned in the text 'The conference is scheduled for January 25, 2023.'\": \"January 25, 2023.\",\n",
    "}\n",
    "\n",
    "identification ={\n",
    "    \"Given the following user review, answer the question. User review: The CO-1T is a great pair of headphones! The sound quality is the best out there, and I can hear every detail of my music. Question: Why is the CO-1T a great wireless headphone?\": \"Because the audio experience is unrivaled\",\n",
    "    \"How many objects are named in the following sequence: The dog was running towards the tree with a ball in his mouth.\": \"One object is named, a dog.\",\n",
    "    \"Given the statement, answer the following question. Statement: The sky is blue and the building is grey. Question: What color does the sky have?\": \"Blue\"\n",
    "}\n",
    "\n",
    "languageStyles = {\n",
    "  \"What language style is used in the following Statement: Improvements cannot be introduced due to funding restrictions.\" : \"Formal.\",\n",
    "  \"What language style is used in the following Statement: New results in this area are produced by the research group.\" : \"Academic.\",\n",
    "  \"What language style is used in the following Statement: I don ºt believe that the results are accurate.\" : \"Informal\",\n",
    "  \"What language style is used in the following Statement: The results are not believed to be accurate.\" : \"Formal\",\n",
    "  \"What language style is described by the following statement: The style is more casual and spontaneous and is used to communicate with friends and family either in writing or conversations.\" : \"Informal\",\n",
    "  \"What language style is described by the following statement: The style is used when writing or speaking for professional purpose and does not use colloquialism, contractions or first-person pronouns.\" : \"Formal\",\n",
    "  \"What language style is described by the following statement: The style is used for higher school purposes like thesis or research papers and does not use first-person pronouns and is not personal but factual\" : \"Academic\",\n",
    "}\n",
    "\n",
    "# Combine all test questions into a single dictionary\n",
    "questions_rouge = {**textual_entailment,**reasoning_problem_solving,**analogical_reasoning,**ner,**identification,**languageStyles}\n",
    "#print(questions_rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-1': {'r': 0.42857142857142855, 'p': 0.5833333333333334, 'f': 0.49411764217577864}, 'rouge-2': {'r': 0.18571428571428572, 'p': 0.3170731707317073, 'f': 0.23423422957552154}, 'rouge-l': {'r': 0.3877551020408163, 'p': 0.5277777777777778, 'f': 0.44705881864636676}}]\n"
     ]
    }
   ],
   "source": [
    "#%pip install rouge - ##test rouge library - https://pypi.org/project/rouge/ \n",
    "from rouge import Rouge\n",
    "\n",
    "##example calc for rouge\n",
    "hypothesis = \"the #### transcript is a written version of each day 's cnn student news program use this transcript to help students with reading comprehension and vocabulary use the weekly newsquiz to test your knowledge of storie s you     saw on cnn student news\"\n",
    "\n",
    "reference = \"this page includes the show transcript use the transcript to help students with reading comprehension and vocabulary at the bottom of the page , comment for a chance to be mentioned on cnn student news . you must be a teac    her or a student age # # or older to request a mention on the cnn student news roll call . the weekly newsquiz tests     students ' knowledge of even ts in the news\"\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: If 'A implies B' and 'B is false,' what can you conclude about A?\n",
      "Expected Answer: Nothing definitive can be concluded about A.\n",
      "Model Answer: A cannot be concluded about.\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.8, 'p': 0.5714285714285714, 'f': 0.6666666618055556}, 'rouge-2': {'r': 0.5, 'p': 0.3333333333333333, 'f': 0.39999999520000007}, 'rouge-l': {'r': 0.6, 'p': 0.42857142857142855, 'f': 0.499999995138889}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       2.68 ms /     8 runs   (    0.33 ms per token,  2988.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    2057.70 ms /     8 runs   (  257.21 ms per token,     3.89 tokens per second)\n",
      "llama_print_timings:       total time =    2095.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: If a car travels at 60 miles per hour, how long will it take to cover 120 miles?\n",
      "Expected Answer: 2 hours\n",
      "Model Answer: The correct answer is: 2 hours\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.3333333333333333, 'p': 1.0, 'f': 0.4999999962500001}, 'rouge-2': {'r': 0.2, 'p': 1.0, 'f': 0.33333333055555564}, 'rouge-l': {'r': 0.3333333333333333, 'p': 1.0, 'f': 0.4999999962500001}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       2.85 ms /    10 runs   (    0.28 ms per token,  3510.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5532.32 ms /    34 tokens (  162.72 ms per token,     6.15 tokens per second)\n",
      "llama_print_timings:        eval time =    1442.08 ms /     9 runs   (  160.23 ms per token,     6.24 tokens per second)\n",
      "llama_print_timings:       total time =    7022.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: If 'cat' is to 'kitten,' what is 'dog' to?\n",
      "Expected Answer: 'Dog' is to 'puppy'\n",
      "Model Answer: Puppy\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       1.16 ms /     4 runs   (    0.29 ms per token,  3460.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3193.78 ms /    24 tokens (  133.07 ms per token,     7.51 tokens per second)\n",
      "llama_print_timings:        eval time =     816.22 ms /     4 runs   (  204.06 ms per token,     4.90 tokens per second)\n",
      "llama_print_timings:       total time =    4032.60 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Identify the named person in the sentence 'Elon Musk is the CEO of SpaceX.'\n",
      "Expected Answer: Elon Musk\n",
      "Model Answer: Elon Musk\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       1.63 ms /     6 runs   (    0.27 ms per token,  3678.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3785.21 ms /    30 tokens (  126.17 ms per token,     7.93 tokens per second)\n",
      "llama_print_timings:        eval time =     790.84 ms /     5 runs   (  158.17 ms per token,     6.32 tokens per second)\n",
      "llama_print_timings:       total time =    4602.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       7.29 ms /    23 runs   (    0.32 ms per token,  3155.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3996.18 ms /    34 tokens (  117.53 ms per token,     8.51 tokens per second)\n",
      "llama_print_timings:        eval time =    3480.77 ms /    22 runs   (  158.22 ms per token,     6.32 tokens per second)\n",
      "llama_print_timings:       total time =    7566.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Extract the date mentioned in the text 'The conference is scheduled for January 25, 2023.'\n",
      "Expected Answer: January 25, 2023.\n",
      "Model Answer: Sure! The correct answer is:\n",
      "January 25, 2023.\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.375, 'p': 1.0, 'f': 0.5454545414876033}, 'rouge-2': {'r': 0.2857142857142857, 'p': 1.0, 'f': 0.4444444409876544}, 'rouge-l': {'r': 0.375, 'p': 1.0, 'f': 0.5454545414876033}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =      11.99 ms /    48 runs   (    0.25 ms per token,  4003.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8171.73 ms /    71 tokens (  115.09 ms per token,     8.69 tokens per second)\n",
      "llama_print_timings:        eval time =    7595.61 ms /    47 runs   (  161.61 ms per token,     6.19 tokens per second)\n",
      "llama_print_timings:       total time =   15949.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Given the following user review, answer the question. User review: The CO-1T is a great pair of headphones! The sound quality is the best out there, and I can hear every detail of my music. Question: Why is the CO-1T a great wireless headphone?\n",
      "Expected Answer: Because the audio experience is unrivaled\n",
      "Model Answer: Great answer! Here's the next question:\n",
      "User review: The new iPhone XS Max has an amazing camera! It takes incredible photos and videos. Question: What makes the iPhone XS Max camera so good?\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.034482758620689655, 'p': 0.16666666666666666, 'f': 0.05714285430204096}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.034482758620689655, 'p': 0.16666666666666666, 'f': 0.05714285430204096}}]\n",
      "------------\n",
      "Question: How many objects are named in the following sequence: The dog was running towards the tree with a ball in his mouth.\n",
      "Expected Answer: One object is named, a dog.\n",
      "Model Answer: One object is named in the sequence: the dog.\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.5, 'p': 0.6666666666666666, 'f': 0.5714285665306124}, 'rouge-2': {'r': 0.25, 'p': 0.4, 'f': 0.3076923029585799}, 'rouge-l': {'r': 0.5, 'p': 0.6666666666666666, 'f': 0.5714285665306124}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       4.11 ms /    13 runs   (    0.32 ms per token,  3163.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3858.95 ms /    32 tokens (  120.59 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:        eval time =    2314.17 ms /    13 runs   (  178.01 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:       total time =    6230.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       0.93 ms /     3 runs   (    0.31 ms per token,  3232.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5074.27 ms /    40 tokens (  126.86 ms per token,     7.88 tokens per second)\n",
      "llama_print_timings:        eval time =     459.65 ms /     3 runs   (  153.22 ms per token,     6.53 tokens per second)\n",
      "llama_print_timings:       total time =    5556.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Given the statement, answer the following question. Statement: The sky is blue and the building is grey. Question: What color does the sky have?\n",
      "Expected Answer: Blue\n",
      "Model Answer: Blue\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n",
      "Question: What language style is used in the following Statement: Improvements cannot be introduced due to funding restrictions.\n",
      "Expected Answer: Formal.\n",
      "Model Answer: The language style used in the statement is Formal.\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.1111111111111111, 'p': 1.0, 'f': 0.1999999982}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.1111111111111111, 'p': 1.0, 'f': 0.1999999982}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       3.89 ms /    13 runs   (    0.30 ms per token,  3345.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3902.61 ms /    32 tokens (  121.96 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:        eval time =    1920.55 ms /    12 runs   (  160.05 ms per token,     6.25 tokens per second)\n",
      "llama_print_timings:       total time =    5882.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: New results in this area are produced by the research group.\n",
      "Expected Answer: Academic.\n",
      "Model Answer: The language style of the statement is: Technical or Academic.\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.1, 'p': 1.0, 'f': 0.18181818016528928}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.1, 'p': 1.0, 'f': 0.18181818016528928}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       4.83 ms /    16 runs   (    0.30 ms per token,  3315.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2641.67 ms /    21 tokens (  125.79 ms per token,     7.95 tokens per second)\n",
      "llama_print_timings:        eval time =    2565.18 ms /    15 runs   (  171.01 ms per token,     5.85 tokens per second)\n",
      "llama_print_timings:       total time =    5268.38 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: I don ºt believe that the results are accurate.\n",
      "Expected Answer: Informal\n",
      "Model Answer: The language style of the statement is Informal.\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.125, 'p': 1.0, 'f': 0.2222222202469136}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.125, 'p': 1.0, 'f': 0.2222222202469136}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       3.42 ms /    12 runs   (    0.28 ms per token,  3508.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2256.16 ms /    20 tokens (  112.81 ms per token,     8.86 tokens per second)\n",
      "llama_print_timings:        eval time =    1699.29 ms /    11 runs   (  154.48 ms per token,     6.47 tokens per second)\n",
      "llama_print_timings:       total time =    4000.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is used in the following Statement: The results are not believed to be accurate.\n",
      "Expected Answer: Formal\n",
      "Model Answer: The language style of the statement is Formal.\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.125, 'p': 1.0, 'f': 0.2222222202469136}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.125, 'p': 1.0, 'f': 0.2222222202469136}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       3.45 ms /    12 runs   (    0.29 ms per token,  3475.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2041.35 ms /    18 tokens (  113.41 ms per token,     8.82 tokens per second)\n",
      "llama_print_timings:        eval time =    1584.85 ms /    11 runs   (  144.08 ms per token,     6.94 tokens per second)\n",
      "llama_print_timings:       total time =    3670.45 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is described by the following statement: The style is more casual and spontaneous and is used to communicate with friends and family either in writing or conversations.\n",
      "Expected Answer: Informal\n",
      "Model Answer: Informal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       1.16 ms /     4 runs   (    0.29 ms per token,  3451.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4415.41 ms /    40 tokens (  110.39 ms per token,     9.06 tokens per second)\n",
      "llama_print_timings:        eval time =     670.69 ms /     4 runs   (  167.67 ms per token,     5.96 tokens per second)\n",
      "llama_print_timings:       total time =    5111.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is described by the following statement: The style is used when writing or speaking for professional purpose and does not use colloquialism, contractions or first-person pronouns.\n",
      "Expected Answer: Formal\n",
      "Model Answer: The language style described in the statement is: Formal\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.1111111111111111, 'p': 1.0, 'f': 0.1999999982}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.1111111111111111, 'p': 1.0, 'f': 0.1999999982}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       3.39 ms /    13 runs   (    0.26 ms per token,  3840.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4741.20 ms /    37 tokens (  128.14 ms per token,     7.80 tokens per second)\n",
      "llama_print_timings:        eval time =    2085.69 ms /    12 runs   (  173.81 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =    6884.63 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What language style is described by the following statement: The style is used for higher school purposes like thesis or research papers and does not use first-person pronouns and is not personal but factual\n",
      "Expected Answer: Academic\n",
      "Model Answer: A. Academic\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.5, 'p': 1.0, 'f': 0.6666666622222223}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.5, 'p': 1.0, 'f': 0.6666666622222223}}]\n",
      "------------\n",
      "Questions total: 15\n",
      "Average Recall (ROUGE-1): 0.408\n",
      "Average Precision (ROUGE-1): 0.827\n",
      "Average F1-score (ROUGE-1): 0.469\n",
      "Average Recall (ROUGE-2): 0.149\n",
      "Average Precision (ROUGE-2): 0.249\n",
      "Average F1-score (ROUGE-2): 0.166\n",
      "Average Recall (ROUGE-L): 0.394\n",
      "Average Precision (ROUGE-L): 0.817\n",
      "Average F1-score (ROUGE-L): 0.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13434.66 ms\n",
      "llama_print_timings:      sample time =       1.40 ms /     6 runs   (    0.23 ms per token,  4301.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4233.26 ms /    36 tokens (  117.59 ms per token,     8.50 tokens per second)\n",
      "llama_print_timings:        eval time =     729.23 ms /     5 runs   (  145.85 ms per token,     6.86 tokens per second)\n",
      "llama_print_timings:       total time =    4991.00 ms\n"
     ]
    }
   ],
   "source": [
    "#maybe check if rouge-score library better\n",
    "\n",
    "def evaluate_model_rouge(model, questions):\n",
    "    try:\n",
    "        total_questions = len(questions)\n",
    "        model_answers = []\n",
    "        rouge_scores_list_r1 = []\n",
    "        rouge_scores_list_r2 = []\n",
    "        rouge_scores_list_rl= []\n",
    "        rouge_scores = {\"rouge\": {\"recall\": 0, \"precision\": 0, \"f1-score\": 0}} \n",
    "\n",
    "        for question, expected_answer in questions_rouge.items():\n",
    "            prompt = f\"{question}\\nAnswer Options:\\n\"\n",
    "            prompt = get_prompt(prompt)\n",
    "            #print(prompt)\n",
    "            model_answer = model(prompt)\n",
    "\n",
    "            model_answer = model_answer.strip()\n",
    "            expected_answer = expected_answer.strip()\n",
    "\n",
    "            # Get Rouge Score - Rouge() hat rouge-1, rouge-2 and rouge-l as option \n",
    "            rouge = Rouge()\n",
    "            scores = rouge.get_scores(expected_answer, model_answer) \n",
    "\n",
    "            #save rouge-1, rouge-2 and rouge-l values for each question in list\n",
    "            scores_list_r1 = rouge.get_scores(expected_answer, model_answer)[0]\n",
    "            rouge_scores_list_r1.append({\n",
    "                \"r\": scores_list_r1.get('rouge-1', {}).get('r', 0),\n",
    "                \"p\": scores_list_r1.get('rouge-1', {}).get('p', 0),\n",
    "                \"f\": scores_list_r1.get('rouge-1', {}).get('f', 0),\n",
    "            })\n",
    "            scores_list_r2 = rouge.get_scores(expected_answer, model_answer)[0]\n",
    "            rouge_scores_list_r2.append({\n",
    "                \"r\": scores_list_r2.get('rouge-2', {}).get('r', 0),\n",
    "                \"p\": scores_list_r2.get('rouge-2', {}).get('p', 0),\n",
    "                \"f\": scores_list_r2.get('rouge-2', {}).get('f', 0),\n",
    "            })\n",
    "            scores_list_rl = rouge.get_scores(expected_answer, model_answer)[0]\n",
    "            rouge_scores_list_rl.append({\n",
    "                \"r\": scores_list_rl.get('rouge-l', {}).get('r', 0),\n",
    "                \"p\": scores_list_rl.get('rouge-l', {}).get('p', 0),\n",
    "                \"f\": scores_list_rl.get('rouge-l', {}).get('f', 0),\n",
    "            })\n",
    "            \n",
    "            \n",
    "            # Print the results for each question\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Expected Answer: {expected_answer}\")\n",
    "            print(f\"Model Answer: {model_answer}\")\n",
    "            print(f\"ROUGE Scores: {scores}\")\n",
    "            print(\"------------\")\n",
    "\n",
    "            # Store the model's answer and the corresponding question in the array\n",
    "            model_answers.append({\n",
    "                \"question\": question,\n",
    "                \"expected_answer\": expected_answer,\n",
    "                \"model_answer\": model_answer,\n",
    "            })\n",
    "\n",
    "        # Calculate the average 'r', 'p', and 'f' values for rouge-1, rouge-2 and rouge-l\n",
    "        average_r_r1 = round(sum(entry[\"r\"] for entry in rouge_scores_list_r1) / total_questions,3) \n",
    "        average_p_r1 = round(sum(entry[\"p\"] for entry in rouge_scores_list_r1) / total_questions,3)\n",
    "        average_f_r1 = round(sum(entry[\"f\"] for entry in rouge_scores_list_r1) / total_questions,3)\n",
    "\n",
    "        average_r_r2 = round(sum(entry[\"r\"] for entry in rouge_scores_list_r2) / total_questions,3) \n",
    "        average_p_r2 = round(sum(entry[\"p\"] for entry in rouge_scores_list_r2) / total_questions,3)\n",
    "        average_f_r2 = round(sum(entry[\"f\"] for entry in rouge_scores_list_r2) / total_questions,3)\n",
    "        \n",
    "        average_r_rl = round(sum(entry[\"r\"] for entry in rouge_scores_list_rl) / total_questions,3) \n",
    "        average_p_rl = round(sum(entry[\"p\"] for entry in rouge_scores_list_rl) / total_questions,3)\n",
    "        average_f_rl = round(sum(entry[\"f\"] for entry in rouge_scores_list_rl) / total_questions,3)\n",
    "\n",
    "        print(f\"Questions total: {total_questions}\")\n",
    "        print(f\"Average Recall (ROUGE-1): {average_r_r1}\")\n",
    "        print(f\"Average Precision (ROUGE-1): {average_p_r1}\")\n",
    "        print(f\"Average F1-score (ROUGE-1): {average_f_r1}\")\n",
    "        print(f\"Average Recall (ROUGE-2): {average_r_r2}\")\n",
    "        print(f\"Average Precision (ROUGE-2): {average_p_r2}\")\n",
    "        print(f\"Average F1-score (ROUGE-2): {average_f_r2}\")\n",
    "        print(f\"Average Recall (ROUGE-L): {average_r_rl}\")\n",
    "        print(f\"Average Precision (ROUGE-L): {average_p_rl}\")\n",
    "        print(f\"Average F1-score (ROUGE-L): {average_f_rl}\")\n",
    "\n",
    "        return model_answers, rouge_scores\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "#run method\n",
    "model_answers, rouge_scores = evaluate_model_rouge(llm, questions_rouge)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tests/data.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load some sentences\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./tests/data.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m   data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      8\u001b[0m hyps, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[[d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyp\u001b[39m\u001b[38;5;124m'\u001b[39m], d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]))\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tests/data.json'"
     ]
    }
   ],
   "source": [
    "##multiple sentences\n",
    "import json\n",
    "\n",
    "# Load some sentences\n",
    "with open('./tests/data.json') as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "hyps, refs = map(list, zip(*[[d['hyp'], d['ref']] for d in data]))\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hyps, refs)\n",
    "# or\n",
    "scores = rouge.get_scores(hyps, refs, avg=True)\n",
    "\n",
    "\n",
    "##two files\n",
    "from rouge import FilesRouge\n",
    "\n",
    "files_rouge = FilesRouge()\n",
    "scores = files_rouge.get_scores(hyp_path, ref_path)\n",
    "# or\n",
    "scores = files_rouge.get_scores(hyp_path, ref_path, avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install bitsandbytes\n",
    "#%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation - Perplexity: measure that quantifies how well the model predicts a sample of text. Lower perplexity values indicate better performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, LlamaTokenizer\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "model_name = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n",
    "max_memory = f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  device_map='auto',\n",
    "  load_in_4bit=True,\n",
    "  max_memory=max_memory,\n",
    "  do_sample=True,\n",
    "  torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.max_length\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "device = \"cuda\"\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check Langchains QAEvalChain\n",
    "#https://github.com/langchain-ai/langchain/blob/a6b39afe0e34621fafac885af05bbbb445ea5ac0/langchain/evaluation/qa/eval_chain.py#L42C1-L42C1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
