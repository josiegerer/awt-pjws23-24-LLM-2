{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain llama-cpp-python \n",
    "#CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define System Prompt and load Llama2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "# DEFAULT_SYSTEM_PROMPT = \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chatbot prompt for later use case\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "<<SYS>>\n",
    "You are a chatbot that helps people learn a new language. You always give the answer to the question in a formal manner.\n",
    "If you don't know the answer to a question, you tell them truthfully that you don't know and don't give false information. You are a helpful, respectful and honest assistant.\n",
    "Your answers should not contain harmful, unethical, racist, sexist, toxic, dangerous or illegal content. Please make sure that your answers are socially unbiased and positive.\n",
    "If a question does not make sense or is not factually coherent, please explain why, rather than answering something incorrectly.<</SYS>>\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + DEFAULT_SYSTEM_PROMPT +E_SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general prompt to test llm for testing questions\n",
    "SYSTEM_PROMPT_2 = \"\"\"\n",
    "<<SYS>>\n",
    "You are alwys giving the answer to the question in a short and very simple manner and you answer in the language style that is asked from you. You leave out unnecessary words and if you have several options you would like to present only name one. If the answer is only one word you don't put a dot and the end. If you are asked to translate something you translate it.\n",
    "If you don't know the answer to a question, you tell them truthfully that you don't know and don't give false information. You are a helpful, respectful and honest assistant.\n",
    "Your answers should not contain harmful, unethical, racist, sexist, toxic, dangerous or illegal content. Please make sure that your answers are socially unbiased and positive.\n",
    "If a question does not make sense or is not factually coherent, please explain why, rather than answering something incorrectly.<</SYS>>\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + SYSTEM_PROMPT_2 +E_SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['llama-7b.ggmlv3.q4_K_M.bin',\n",
       " '.DS_Store',\n",
       " 'em_german_leo_mistral.Q5_K_M.gguf',\n",
       " 'llama-2-7b.Q4_K_M.gguf',\n",
       " 'llama-2-7b-chat.Q5_K_M.gguf']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prompt(instruction):\n",
    "    return B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "\n",
    "os.listdir(\"/Users/josi/Llama2_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/josi/Llama2_weights/llama-2-7b.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.15.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:        blk.15.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:             blk.15.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:             blk.15.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.16.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:        blk.16.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.16.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.16.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.17.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.17.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.17.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.17.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.18.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:        blk.18.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.18.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.18.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.19.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.19.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.19.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.19.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:              blk.2.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.2.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:         blk.2.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.2.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:              blk.2.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.20.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.20.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.20.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.20.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.20.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.20.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.21.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.21.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.21.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.21.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.21.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.21.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.22.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.22.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.22.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.22.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.22.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.22.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.23.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.23.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.23.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.23.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.23.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.23.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:            blk.3.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:              blk.3.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:              blk.3.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:         blk.3.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:              blk.3.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:              blk.3.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:            blk.4.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:              blk.4.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:              blk.4.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:         blk.4.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:              blk.4.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:              blk.4.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:            blk.5.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:              blk.5.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:              blk.5.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:         blk.5.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:              blk.5.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:              blk.5.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:            blk.6.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:              blk.6.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:              blk.6.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:         blk.6.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:              blk.6.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:              blk.6.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.7.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:              blk.7.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:              blk.7.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:         blk.7.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:              blk.7.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:              blk.7.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:            blk.8.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:              blk.8.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:              blk.8.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:         blk.8.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:              blk.8.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.8.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:            blk.9.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:              blk.9.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:              blk.9.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:         blk.9.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:              blk.9.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:              blk.9.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:        blk.24.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:        blk.25.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.26.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:        blk.27.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:        blk.28.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:        blk.29.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:        blk.30.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  18:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MB\n",
      "llm_load_tensors: mem required  = 3891.35 MB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "llama_build_graph: non-view tensors processed: 740/740\n",
      "llama_new_context_with_model: compute buffer total size = 2.66 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path= \"/Users/josi/Llama2_weights/llama-2-7b.Q4_K_M.gguf\",\n",
    "    #model_path = \"/Users/josi/Llama2_weights/llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2048,\n",
    "    top_p=1,\n",
    "    # callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionaries with test questions (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test questions for different categories\n",
    "informal_questions = {\n",
    "    \"Write an informal greeting.\": \"Hey\",\n",
    "    \"Write an informal response to the question: 'How's it going?'\": \"Hi, going good, hbu?\",\n",
    "    \"Write an informal message inviting someone to dinner tonight\": \"Hey, wanna join us for dinner tonight?\",\n",
    "    \"Compose an informal email closing.\": \"Cheers, [Your Name]\",\n",
    "}\n",
    "\n",
    "formal_questions = {\n",
    "    \"Write a formal greeting.\": \"Hello\",\n",
    "    \"Write a formal response to the question: 'How is it going?'\": \"Hello, I am doing well. How about you?\",\n",
    "    \"Write a formal message inviting someone to dinner tonight\": \"Hello, would you like to join us for dinner tonight?\",\n",
    "    \"Compose a professional email closing.\": \"Sincerely, [Your Name]\",\n",
    "}\n",
    "\n",
    "academic_questions = {\n",
    "    \"Describe the french revolution in two sentences using academic language.\": \"The historical event of the French Revolution was marked by widespread social upheaval...\",\n",
    "    \"Define gravity in two sentences using academic language.\": \"Gravity is a force that attracts two objects with mass\",\n",
    "}\n",
    "\n",
    "translation_questions = {\n",
    "    \"Translate 'hello' to Spanish.\": \"Hola\",\n",
    "    \"Translate 'good morning' to French.\": \"Bonjour\",\n",
    "    \"Translate 'Thank you' to German.\": \"Danke\",\n",
    "    \"How do you say 'hello' in Japanese?\": \"Konnichiwa (こんにちは)\",\n",
    "    \"Provide the Italian translation for 'book'.\": \"Libro\",\n",
    "}\n",
    "\n",
    "general_knowledge_questions = {\n",
    "    \"What is the capital of France?\": \"The capital of France is Paris\",\n",
    "    \"What is the square root of 25?\": \"The square root of 25 is 5\",\n",
    "    \"What is the circumference of the earth?\":\"The circumference of the earth is approximately 40,075 kilometers\",\n",
    "    \"question\": \"Explain the concept of photosynthesis.\", \"answer\": \"Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods with the help of chlorophyll.\",\n",
    "}\n",
    "\n",
    "textual_entailment = {\n",
    "    \"question\": \"Given the statement 'The sun rises in the east,' is the statement 'The earth is flat' likely true or false?\", \"answer\": \"False\",\n",
    "    \"question\": \"If 'A implies B' and 'B is false,' what can you conclude about A?\", \"answer\": \"Nothing definitive can be concluded about A.\",\n",
    "}\n",
    "\n",
    "ambiguity_handling = {\n",
    "    \"question\": \"Provide two different interpretations of the phrase 'bank account.'\", \"answer\": \"1. An account with a financial institution. 2. The land alongside a river.\",\n",
    "    \"question\": \"What could the word 'bat' mean in the context of baseball and in the context of an animal?\", \"answer\": \"In baseball, a 'bat' is a piece of equipment used to hit the ball. In the context of an animal, a 'bat' is a flying mammal.\",\n",
    "}\n",
    "\n",
    "reasoning_problem_solving = {\n",
    "    \"question\": \"If a car travels at 60 miles per hour, how long will it take to cover 120 miles?\", \"answer\": \"2 hours\",\n",
    "    \"question\": \"Solve the following math problem: 3x + 5 = 20.\", \"answer\": \"x = 5\",\n",
    "}\n",
    "\n",
    "analogical_reasoning = {\n",
    "    \"question\": \"If 'cat' is to 'kitten,' what is 'dog' to?\", \"answer\": \"'Dog' is to 'puppy'\",\n",
    "    \"question\": \"Identify the relationship between 'pen' and 'ink' and apply the same relationship to 'keyboard.'\", \"answer\": \"'Keyboard' is to 'electricity'\",\n",
    "}\n",
    "\n",
    "commonsense_reasoning = {\n",
    "    \"question\": \"What might be a common reaction to receiving a gift?\", \"answer\": \"A common reaction to receiving a gift is expressing gratitude and appreciation.\",\n",
    "    \"question\": \"Predict a possible consequence of leaving food out in the sun for too long.\", \"answer\": \"Leaving food out in the sun for too long may lead to spoilage and bacterial growth.\",\n",
    "}\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "ner = {\n",
    "    \"question\": \"Identify the named entities in the sentence 'Elon Musk is the CEO of SpaceX.'\", \"answer\": \"Named entities: Elon Musk, SpaceX.\",\n",
    "    \"question\": \"Extract the dates mentioned in the text 'The conference is scheduled for January 25-27, 2023.'\", \"answer\": \"Dates: January 25-27, 2023.\",\n",
    "}\n",
    "\n",
    "# Combine all test questions into a single dictionary\n",
    "test_questions = {**informal_questions, **formal_questions, **academic_questions, **translation_questions, **general_knowledge_questions,**textual_entailment,**ambiguity_handling,**reasoning_problem_solving,**analogical_reasoning,**commonsense_reasoning,**ner}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model based on Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answers = []\n",
    "def evaluate_model(model, questions):\n",
    "    correct_answers = 0\n",
    "    total_questions = len(questions)\n",
    "\n",
    "    for question, expected_answer in questions.items():\n",
    "        # Get the model's answer for the current question\n",
    "        prompt = get_prompt(question)\n",
    "        model_answer = model(prompt)\n",
    "\n",
    "        # Remove trailing dots from the model's answer and Compare the model's answer to the expected answer (case-insensitive, whitespace-insensitive)\n",
    "        model_answer = model_answer.rstrip('.')\n",
    "        if model_answer.strip().lower() == expected_answer.strip().lower():\n",
    "            correct_answers += 1\n",
    "\n",
    "        # Check if any of the expected words are present in the model's answer\n",
    "        #if any(word.lower() in model_answer.lower() for word in expected_answer.split()):\n",
    "        #    correct_answers += 1\n",
    "\n",
    "        # Print the results for each question\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected Answer: {expected_answer}\")\n",
    "        print(f\"Model Answer: {model_answer}\")\n",
    "        print(\"------------\")\n",
    "\n",
    "        # Store the model's answer and the corresponding question in the array\n",
    "        model_answers.append({\n",
    "            \"question\": question,\n",
    "            \"expected_answer\": expected_answer,\n",
    "            \"model_answer\": model_answer,\n",
    "        })\n",
    "\n",
    "    # Print overall evaluation results\n",
    "    print(f\"Total Questions: {total_questions}\")\n",
    "    print(f\"Correct Answers: {correct_answers}\")\n",
    "    print(f\"Accuracy: {correct_answers / total_questions * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(llm, test_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model based on Rouge (Recall-Oriented Understudy for Gissing Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of metrics used for evaluating the quality of summaries. It compares the generated summary with one or more reference summaries and calculates precision, recall, and F1-score \n",
    "\n",
    "- ROUGE-N (quantifies the overlap of N-grams, contiguous sequences of N items (typically words or characters), between the system-generated summary and the reference summary) \n",
    "- ROUGE-1 (unigram overlap): This metric measures the overlap of unigrams (single words) between the generated summary and the reference summary. It focuses on the recall of unigrams.\n",
    "- ROUGE-2 (bigram overlap): Similar to ROUGE-1, but it measures the overlap of bigrams (pairs of consecutive words) instead of unigrams. It evaluates the recall of bigrams.\n",
    "- ROUGE-L (longest common subsequence): Instead of measuring word overlap, ROUGE-L focuses on the longest common subsequence (LCS) between the generated and reference summaries. It evaluates the recall of the longest common subsequence\n",
    "\n",
    "- high precision value suggests that the words or phrases churned out by the machine translation or submodel are primarily accurate\n",
    "- high recall value, ideally close to 1, implies that the content of the machine-generated output aligns closely with the human-made reference. It signifies the model's proficiency in capturing relevant information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also check if rouge-score library better\n",
    "\n",
    "#%pip install rouge\n",
    "from rouge import Rouge\n",
    "\n",
    "def evaluate_model(model, questions):\n",
    "    try:\n",
    "        correct_answers = 0\n",
    "        total_questions = len(questions)\n",
    "        model_answers = []\n",
    "        rouge_scores = {\"rouge-1\": {\"precision\": 0, \"recall\": 0, \"f1-score\": 0}} \n",
    "\n",
    "        for question, expected_answer in questions.items():\n",
    "            # Get the model's answer for the current question\n",
    "            prompt = get_prompt(question)\n",
    "            model_answer = model(prompt)\n",
    "\n",
    "            # format answer to match properly \n",
    "            model_answer = model_answer.rstrip('.')\n",
    "            #and calculate accuracy (which is currently defined as approximate match where only any word has to match the correct answer.)\n",
    "            if any(word.lower() in model_answer.lower() for word in expected_answer.split()):\n",
    "                correct_answers += 1\n",
    "\n",
    "            # Concatenate model's answer and expected answer into strings for Rouge evaluation\n",
    "            model_summary = \" \".join(model_answer.split())\n",
    "            reference_summary = \" \".join(expected_answer.split())\n",
    "\n",
    "            # Get Rouge Score\n",
    "            rouge = Rouge()\n",
    "            scores = rouge.get_scores(reference_summary,model_summary) ####CHECK IF REF OR MODEL FIRST\n",
    "\n",
    "            # Check if Rouge calculation was successful - WIP -> see test below \n",
    "            if scores:\n",
    "                scores = scores[0]\n",
    "                for metric in [\"precision\", \"recall\", \"f1-score\"]:\n",
    "                    rouge_scores[f\"rouge-1\"][metric] += scores.get(f\"rouge-1\", {}).get(metric, 0)\n",
    "\n",
    "            rouge_score = rouge.get_scores(reference_summary, model_summary)\n",
    "\n",
    "\n",
    "            # store model answers in array\n",
    "            model_answers.append({\n",
    "                \"question\": question,\n",
    "                \"expected_answer\": expected_answer,\n",
    "                \"model_answer\": model_answer,\n",
    "            })\n",
    "\n",
    "            #print each model answer with score\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Expected Answer: {expected_answer}\")\n",
    "            print(f\"Model Answer: {model_answer}\")\n",
    "            print(f\"Rouge Score: {rouge_score}\")\n",
    "            print(\"------------\")\n",
    "\n",
    "\n",
    "        # Calculate average Rouge scores\n",
    "        for metric in [\"precision\", \"recall\", \"f1-score\"]:\n",
    "            rouge_scores[f\"rouge-1\"][metric] /= total_questions\n",
    "\n",
    "        # Print overall evaluation results\n",
    "        print(f\"Total Questions: {total_questions}\")\n",
    "        print(f\"Correct Answers: {correct_answers}\")\n",
    "        print(f\"Accuracy: {correct_answers / total_questions * 100:.2f}%\")\n",
    "        #print(\"Rouge Scores:\")\n",
    "        #print(rouge_scores)\n",
    "\n",
    "        return model_answers, rouge_scores\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =       4.79 ms /     4 runs   (    1.20 ms per token,   834.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   50879.52 ms /   230 tokens (  221.22 ms per token,     4.52 tokens per second)\n",
      "llama_print_timings:        eval time =     744.41 ms /     3 runs   (  248.14 ms per token,     4.03 tokens per second)\n",
      "llama_print_timings:       total time =   51748.80 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Write an informal greeting.\n",
      "Expected Answer: Hey\n",
      "Model Answer:   Hey!\n",
      "Rouge Score: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n",
      "Question: Write an informal response to the question: 'How's it going?'\n",
      "Expected Answer: Hi, going good, hbu?\n",
      "Model Answer:   Going great!\n",
      "Rouge Score: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =       8.03 ms /     6 runs   (    1.34 ms per token,   747.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2553.77 ms /    16 tokens (  159.61 ms per token,     6.27 tokens per second)\n",
      "llama_print_timings:        eval time =    1248.55 ms /     5 runs   (  249.71 ms per token,     4.00 tokens per second)\n",
      "llama_print_timings:       total time =    3846.45 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Write an informal message inviting someone to dinner tonight\n",
      "Expected Answer: Hey, wanna join us for dinner tonight?\n",
      "Model Answer:   \"Hey! Want to grab dinner tonight? I'm buying :D\"\n",
      "Rouge Score: [{'rouge-1': {'r': 0.2222222222222222, 'p': 0.2857142857142857, 'f': 0.24999999507812506}, 'rouge-2': {'r': 0.125, 'p': 0.16666666666666666, 'f': 0.14285713795918387}, 'rouge-l': {'r': 0.2222222222222222, 'p': 0.2857142857142857, 'f': 0.24999999507812506}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =       7.86 ms /    22 runs   (    0.36 ms per token,  2798.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1978.83 ms /    12 tokens (  164.90 ms per token,     6.06 tokens per second)\n",
      "llama_print_timings:        eval time =    4542.77 ms /    21 runs   (  216.32 ms per token,     4.62 tokens per second)\n",
      "llama_print_timings:       total time =    6613.51 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =      11.11 ms /    38 runs   (    0.29 ms per token,  3420.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1841.26 ms /    11 tokens (  167.39 ms per token,     5.97 tokens per second)\n",
      "llama_print_timings:        eval time =    8067.97 ms /    37 runs   (  218.05 ms per token,     4.59 tokens per second)\n",
      "llama_print_timings:       total time =   10060.87 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Compose an informal email closing.\n",
      "Expected Answer: Cheers, [Your Name]\n",
      "Model Answer:   Of course! Here's an example of an informal email closing:\n",
      "Warmly (or Sincerely, or Best regards, etc.),\n",
      "[Your Name]\n",
      "Rouge Score: [{'rouge-1': {'r': 0.10526315789473684, 'p': 0.6666666666666666, 'f': 0.18181817946280993}, 'rouge-2': {'r': 0.05263157894736842, 'p': 0.5, 'f': 0.09523809351473926}, 'rouge-l': {'r': 0.10526315789473684, 'p': 0.6666666666666666, 'f': 0.18181817946280993}}]\n",
      "------------\n",
      "Question: Write a formal greeting.\n",
      "Expected Answer: Hello\n",
      "Model Answer:   Good morning!\n",
      "Rouge Score: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =       1.61 ms /     5 runs   (    0.32 ms per token,  3107.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1118.46 ms /     8 tokens (  139.81 ms per token,     7.15 tokens per second)\n",
      "llama_print_timings:        eval time =    1220.16 ms /     5 runs   (  244.03 ms per token,     4.10 tokens per second)\n",
      "llama_print_timings:       total time =    2362.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Write a formal response to the question: 'How is it going?'\n",
      "Expected Answer: Hello, I am doing well. How about you?\n",
      "Model Answer:   Good day! I'm doing well, thank you for asking\n",
      "Rouge Score: [{'rouge-1': {'r': 0.1111111111111111, 'p': 0.125, 'f': 0.11764705384083066}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.125, 'f': 0.11764705384083066}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =       4.62 ms /    16 runs   (    0.29 ms per token,  3459.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2212.37 ms /    15 tokens (  147.49 ms per token,     6.78 tokens per second)\n",
      "llama_print_timings:        eval time =    3116.02 ms /    15 runs   (  207.73 ms per token,     4.81 tokens per second)\n",
      "llama_print_timings:       total time =    5392.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =      10.39 ms /    36 runs   (    0.29 ms per token,  3463.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1619.13 ms /    12 tokens (  134.93 ms per token,     7.41 tokens per second)\n",
      "llama_print_timings:        eval time =    7507.53 ms /    35 runs   (  214.50 ms per token,     4.66 tokens per second)\n",
      "llama_print_timings:       total time =    9266.98 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Write a formal message inviting someone to dinner tonight\n",
      "Expected Answer: Hello, would you like to join us for dinner tonight?\n",
      "Model Answer:   \"Dinner Invitation Tonight? Yes, I'd love to join you for dinner tonight! Let me know the details and I'll be there.\"\n",
      "Rouge Score: [{'rouge-1': {'r': 0.22727272727272727, 'p': 0.5, 'f': 0.31249999570312503}, 'rouge-2': {'r': 0.09523809523809523, 'p': 0.2222222222222222, 'f': 0.13333332913333346}, 'rouge-l': {'r': 0.18181818181818182, 'p': 0.4, 'f': 0.2499999957031251}}]\n",
      "------------\n",
      "Question: Compose a professional email closing.\n",
      "Expected Answer: Sincerely, [Your Name]\n",
      "Model Answer:   Best regards,\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "Thank you for your time,\n",
      "\n",
      "All the best,\n",
      "\n",
      "Warm regards,\n",
      "\n",
      "Kind regards,\n",
      "\n",
      "\n",
      "Rgds,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Regards,\n",
      "Rouge Score: [{'rouge-1': {'r': 0.06666666666666667, 'p': 0.3333333333333333, 'f': 0.1111111083333334}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.06666666666666667, 'p': 0.3333333333333333, 'f': 0.1111111083333334}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =      17.55 ms /    52 runs   (    0.34 ms per token,  2962.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1630.39 ms /    10 tokens (  163.04 ms per token,     6.13 tokens per second)\n",
      "llama_print_timings:        eval time =   10132.34 ms /    51 runs   (  198.67 ms per token,     5.03 tokens per second)\n",
      "llama_print_timings:       total time =   11959.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Describe the french revolution in two sentences using academic language.\n",
      "Expected Answer: The historical event of the French Revolution was marked by widespread social upheaval...\n",
      "Model Answer:   The French Revolution (1789-1799) was a pivotal historical event marked by radical political, social, and cultural transformations. This period saw the overthrow of the Bourbon monarchy, the establishment of the First French Republic, and the implementation of the principles of liberty, equality, and fraternity through violent means, ultimately leading to the Reign of Terror and the rise of Napoleon Bonaparte\n",
      "Rouge Score: [{'rouge-1': {'r': 0.22727272727272727, 'p': 0.7692307692307693, 'f': 0.3508771894613727}, 'rouge-2': {'r': 0.07142857142857142, 'p': 0.3333333333333333, 'f': 0.11764705591695508}, 'rouge-l': {'r': 0.18181818181818182, 'p': 0.6153846153846154, 'f': 0.2807017508648815}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =      28.39 ms /    92 runs   (    0.31 ms per token,  3240.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2375.03 ms /    16 tokens (  148.44 ms per token,     6.74 tokens per second)\n",
      "llama_print_timings:        eval time =   17875.77 ms /    91 runs   (  196.44 ms per token,     5.09 tokens per second)\n",
      "llama_print_timings:       total time =   20609.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Define gravity in two sentences using academic language.\n",
      "Expected Answer: Gravity is a force that attracts two objects with mass\n",
      "Model Answer:   Gravity is a fundamental force that governs the behavior of objects with mass in the universe, attracting them towards each other with a force proportional to their masses and the distance between them. The concept of gravity has been extensively studied and formalized within the context of classical mechanics and general relativity, providing a framework for understanding its underlying principles and predicting its effects on various phenomena\n",
      "Rouge Score: [{'rouge-1': {'r': 0.1509433962264151, 'p': 0.8, 'f': 0.25396825129755607}, 'rouge-2': {'r': 0.09090909090909091, 'p': 0.6666666666666666, 'f': 0.159999997888}, 'rouge-l': {'r': 0.1509433962264151, 'p': 0.8, 'f': 0.25396825129755607}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =      23.89 ms /    82 runs   (    0.29 ms per token,  3432.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1824.38 ms /    13 tokens (  140.34 ms per token,     7.13 tokens per second)\n",
      "llama_print_timings:        eval time =   17194.85 ms /    81 runs   (  212.28 ms per token,     4.71 tokens per second)\n",
      "llama_print_timings:       total time =   19336.93 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Translate 'hello' to Spanish.\n",
      "Expected Answer: Hola\n",
      "Model Answer:   Hola! (Hello in Spanish)\n",
      "Rouge Score: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =       3.44 ms /    10 runs   (    0.34 ms per token,  2906.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1835.25 ms /    11 tokens (  166.84 ms per token,     5.99 tokens per second)\n",
      "llama_print_timings:        eval time =    3084.62 ms /     9 runs   (  342.74 ms per token,     2.92 tokens per second)\n",
      "llama_print_timings:       total time =    4967.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =       2.30 ms /     6 runs   (    0.38 ms per token,  2606.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1673.77 ms /     8 tokens (  209.22 ms per token,     4.78 tokens per second)\n",
      "llama_print_timings:        eval time =    1464.50 ms /     6 runs   (  244.08 ms per token,     4.10 tokens per second)\n",
      "llama_print_timings:       total time =    3167.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Translate 'good morning' to French.\n",
      "Expected Answer: Bonjour\n",
      "Model Answer:   Bonjour\n",
      "Rouge Score: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n",
      "Question: Translate 'Thank you' to German.\n",
      "Expected Answer: Danke\n",
      "Model Answer:   Danke\n",
      "Rouge Score: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =       1.32 ms /     5 runs   (    0.26 ms per token,  3790.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1349.79 ms /     8 tokens (  168.72 ms per token,     5.93 tokens per second)\n",
      "llama_print_timings:        eval time =    1369.12 ms /     5 runs   (  273.82 ms per token,     3.65 tokens per second)\n",
      "llama_print_timings:       total time =    2743.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How do you say 'hello' in Japanese?\n",
      "Expected Answer: Konnichiwa (こんにちは)\n",
      "Model Answer:   Konnichiwa (こんにちは)\n",
      "Rouge Score: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =       3.87 ms /    13 runs   (    0.30 ms per token,  3362.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2041.48 ms /    14 tokens (  145.82 ms per token,     6.86 tokens per second)\n",
      "llama_print_timings:        eval time =    2569.13 ms /    12 runs   (  214.09 ms per token,     4.67 tokens per second)\n",
      "llama_print_timings:       total time =    4662.92 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Provide the Italian translation for 'book'.\n",
      "Expected Answer: Libro\n",
      "Model Answer:   Libro\n",
      "Rouge Score: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =       1.39 ms /     5 runs   (    0.28 ms per token,  3591.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2075.23 ms /    14 tokens (  148.23 ms per token,     6.75 tokens per second)\n",
      "llama_print_timings:        eval time =     798.73 ms /     4 runs   (  199.68 ms per token,     5.01 tokens per second)\n",
      "llama_print_timings:       total time =    2895.83 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of France?\n",
      "Expected Answer: The capital of France is Paris\n",
      "Model Answer:   Paris\n",
      "Rouge Score: [{'rouge-1': {'r': 1.0, 'p': 0.16666666666666666, 'f': 0.2857142832653061}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 0.16666666666666666, 'f': 0.2857142832653061}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =       0.80 ms /     3 runs   (    0.27 ms per token,  3745.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1717.57 ms /    11 tokens (  156.14 ms per token,     6.40 tokens per second)\n",
      "llama_print_timings:        eval time =     356.75 ms /     2 runs   (  178.38 ms per token,     5.61 tokens per second)\n",
      "llama_print_timings:       total time =    2090.40 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the square root of 25?\n",
      "Expected Answer: The square root of 25 is 5\n",
      "Model Answer:   The square root of 25 is 5\n",
      "Rouge Score: [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =       4.39 ms /    13 runs   (    0.34 ms per token,  2958.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1804.95 ms /    11 tokens (  164.09 ms per token,     6.09 tokens per second)\n",
      "llama_print_timings:        eval time =    2420.24 ms /    12 runs   (  201.69 ms per token,     4.96 tokens per second)\n",
      "llama_print_timings:       total time =    4278.57 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the circumference of the earth?\n",
      "Expected Answer: The circumference of the earth is approximately 40,075 kilometers\n",
      "Model Answer:   The circumference of the Earth is approximately 40,075 kilometers (24,901 miles)\n",
      "Rouge Score: [{'rouge-1': {'r': 0.7272727272727273, 'p': 0.8888888888888888, 'f': 0.79999999505}, 'rouge-2': {'r': 0.6, 'p': 0.75, 'f': 0.6666666617283951}, 'rouge-l': {'r': 0.7272727272727273, 'p': 0.8888888888888888, 'f': 0.79999999505}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =       9.18 ms /    28 runs   (    0.33 ms per token,  3049.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1683.27 ms /    10 tokens (  168.33 ms per token,     5.94 tokens per second)\n",
      "llama_print_timings:        eval time =    6073.41 ms /    27 runs   (  224.94 ms per token,     4.45 tokens per second)\n",
      "llama_print_timings:       total time =    7870.14 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: question\n",
      "Expected Answer: Extract the dates mentioned in the text 'The conference is scheduled for January 25-27, 2023.'\n",
      "Model Answer:   Of course! I'm here to help. Please ask your question, and I will do my best to provide a short and simple answer\n",
      "Rouge Score: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =       8.39 ms /    31 runs   (    0.27 ms per token,  3695.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     786.80 ms /     5 tokens (  157.36 ms per token,     6.35 tokens per second)\n",
      "llama_print_timings:        eval time =    6017.23 ms /    30 runs   (  200.57 ms per token,     4.99 tokens per second)\n",
      "llama_print_timings:       total time =    6920.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: answer\n",
      "Expected Answer: Dates: January 25-27, 2023.\n",
      "Model Answer:   Understood! I'll do my best to provide short and simple answers in the language style requested, while being honest, respectful, and socially unbiased. If I don't know the answer to a question, I will say so truthfully without giving false information. Please feel free to ask me anything!\n",
      "Rouge Score: [{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
      "------------\n",
      "Total Questions: 20\n",
      "Correct Answers: 17\n",
      "Accuracy: 85.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14553.27 ms\n",
      "llama_print_timings:      sample time =      20.30 ms /    68 runs   (    0.30 ms per token,  3348.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     787.52 ms /     5 tokens (  157.50 ms per token,     6.35 tokens per second)\n",
      "llama_print_timings:        eval time =   14106.35 ms /    67 runs   (  210.54 ms per token,     4.75 tokens per second)\n",
      "llama_print_timings:       total time =   15164.04 ms\n"
     ]
    }
   ],
   "source": [
    "model_answers, rouge_scores = evaluate_model(llm, test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##test rouge library - https://pypi.org/project/rouge/ \n",
    "## -> not clear which rouge metric meant: theres rouge-1/2/l - only referencing paper\n",
    "##Note: \"f\" stands for f1_score, \"p\" stands for precision, \"r\" stands for recall.\n",
    "\n",
    "\n",
    "##1 sentence\n",
    "hypothesis = \"I am doing well. How about you?\"\n",
    "reference = \"Hello, I am doing well. How about you?\"\n",
    "#reference = \"Good day! I'm doing well, thank you for asking. How about you?\"\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##multiple sentences\n",
    "import json\n",
    "\n",
    "# Load some sentences\n",
    "with open('./tests/data.json') as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "hyps, refs = map(list, zip(*[[d['hyp'], d['ref']] for d in data]))\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hyps, refs)\n",
    "# or\n",
    "scores = rouge.get_scores(hyps, refs, avg=True)\n",
    "\n",
    "\n",
    "##two files\n",
    "from rouge import FilesRouge\n",
    "\n",
    "files_rouge = FilesRouge()\n",
    "scores = files_rouge.get_scores(hyp_path, ref_path)\n",
    "# or\n",
    "scores = files_rouge.get_scores(hyp_path, ref_path, avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Correctness Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bitsandbytes\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Performance Evaluation - Perplexity: measure that quantifies how well the model predicts a sample of text. Lower perplexity values indicate better performance \n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, LlamaTokenizer\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "model_name = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n",
    "max_memory = f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  device_map='auto',\n",
    "  load_in_4bit=True,\n",
    "  max_memory=max_memory,\n",
    "  do_sample=True,\n",
    "  torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.max_length\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "device = \"cuda\"\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
